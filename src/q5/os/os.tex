\input{../../lib.tex}


\hypertitle{Systèmes informatiques 2}{5}{INGI}{1131}
{Benoît Legat\and Yannick Tivisse\and Antoine Walsdorff}
{Marc Lobelle}

%------------------------------------------------------------------------------
%				Chapitre 1
%------------------------------------------------------------------------------


\part{Part One: Background}
\section{Chap 1 : Computer System Overview}

\subsection{Basic Elements}
Explication des 4  éléments structuraux principaux d'un ordinateur: Processor (CPU), main memory, I/O modules, System bus.
Explication des memory address register (MAR), memory buffer register (MBR), I/OAR et I/OBR.


\subsection{Evolution of the Microprocessor}
Petit brin d'histoire sur le passage de microprocesseur sur un seul chip à multiprocesseur: Chaque chip (appellée socket) contient plusieurs processeurs (appelés cores).
Explication des Graphical Processing Units (GPUs), qui calculent sur un tableau de données respectant le Single-Instruction Multiple Data (SIMD).
Explication des Digital SIgnal Procesors (DSPs), qui gère les flux de signaux (audio, video, modems, ..).
Le multiprocesseur classique est en train de laisser place à un System on a Chip (SoC),
où non seulement les CPUs et les caches sont sur le même chip,
mais aussi beaucoup d'autres composants du systeme (DSPs, GPUs, I/O devices (codecs, radios) et main memory).


\subsection{Instruction Execution}
Rien de nouveau, explication d'une Instruction cycle.
Consiste en 2  étapes: ``Fetch stage'' et ``Execute stage''.
Le Program Counter (PC) garde la prochaine instruction à lire en mémoire.
L'instruction fetchée est chargée dans le Instruction Register (IR).
Les instructions peuvent êtres de 4 types : Processor-Memory, Processor-I/O,
Data processing (arithmetic or logical operations) ou Control
(par ex: la prochaine instruction à exécuter n'est pas à l'adresse suivante mais autre part).
Explication d'un format d'instruction sur une machine hypothétique + explication de l'accumulateur (AC) et exemple de 3 cycles d'instruction.

\subsection{Interrupts}
Pendant une interruption, le processeur peut exécuter d'autres instructions pendant que le dispositif I/O est occupé.
Quand le dispositif externe est prêt à recevoir plus de données du processeur,
son module I/O envoie un Interrupt Request Signal au proccesseur.
Le processeur suspend l'exécution du programme courant (si il le peut),
lance une routine appellée Interrupt Handler,
et reviens à l'exécution du programme courant quand le dispositif I/O est servi.
NB: l'interrupt handler fait partie de l'OS,
et son bon fonctionement n'est pas de la responsabilité de l'utilisateur.
Si le tempsd'attente I/O est court,
l'utilisation d'interruption est efficace.
Si le temps d'attente est long, il ne va pas y avoir d'interruption mais c'est quand même efficace magré le temps d'attente,
puisque le processeur aura eu le temps d'exécueter d'autres instruction?


\subsubsection{Interrupt Processing}
Lorsque un appareil I/O termine une opération, il y a neuf étapes: 1-5 : HARDWARE et 6-9 : SOFTWARE
\begin{enumerate}
  \item Le dispositif I/O lance un Interrupt signal au processeur.
  \item Le processeur achève l'éxécution de l'instruction courante avant de répondre à l'interruption.
  \item Le processeur regarde si il y a un Interrupt Request en attente,
    et envoie un acknowledgment signal au dispositif qui a lancé l'interruption,
    cela l'autorise à retirer son Interrupt Signal.
  \item Le processeur push le Program Status Word (PSW) et PC sur la control stack.
  \item Le processeur charge le nouveau PC basé sur l'interruption.
  \item Sauvegarde du reste des Process State Information.
  \item Process Interrupt.
  \item Restore process state information.
  \item Restore old PSW et PC.
\end{enumerate}

\subsubsection{Multiple Interrupts}
Il faut considérer le cas où plusieurs interruptions peuvent arriver en même temps,
un programme peut lire les données dans une ligne de communication et imprimer les résultats en même temps.
Il existe deux approches:

\begin{itemize}
  \item Désactiver les interruptions quand une interruption est en cours.
    Problème: Ne tiens pas compte des priorités d'exécutions et des besoin en temps critiques.
    Ex: Il se peut que des données doivent êtres traitées rapidement pour laisser place à d'autres données.
    Imaginons que le prmeier paquet de données n'ai pas été absorbé quand le deuxième arrive,
    alors il se peut que l'on perde des données barce que le buffer est en overflow.
    Sequential Interrupt Processing.
  \item Définir les priorités d'interruption et autoriser une interruption de plus haute priorité
    à interrompre le Interruption Handler d'une interruption à plus faible priorité.
\end{itemize}

\subsection{The Memory Hierarchy}
On a une pyramide qui représente la hierarchie de la mémoire.
Plus on descend dans la pyramide:
\begin{itemize}
  \item Plus le cout par bit décroit
  \item Plus la capacité augmente
  \item Plus le temps d'accès augmente
  \item Plus la fréquence d'accès du processeur à la mémoire diminue
\end{itemize}
Un ordi contient en moyenne une douzaine de registre, puis deux rang plus bas,
il y a la main memory et entre les deux la cache qui gère les mouvements de données entre registres
et main memory pour améliorer les performances.
Ces 3 types de mémoires sont volatiles.
Il existe aussi les mémoires auxiliaires ou on va stocker les programmes et les fichiers,
et ou on va aussi exploiter la mémoire virtuelle.

\subsection{Cache Memory}
\subsubsection{Motivation}
Un gros problème d'antan était que la vitesse du processeur augmentait plus rapidement que la vitesse d'accès à la mémoire.
La solution est la cache: Une mémoire petite et rapide d'accès entre le processeur et la main memory.

\subsubsection{Cache Principles}
Si le processeur essaye de lire un mot en mémoire,
il check d'abord si il est pas dans la cache.
Si oui, il le charge (rapidement).
Si non, il le charge de la mémoire dans la cache.
Il peut y avoir plusieurs niveau de cache.
De plus en plus grands et de moins en moins rapides.
Voir figure~1.17 pour structure de la cache.

\subsubsection{Cache Design}
Plusieurs facteurs sont développés:
\begin{itemize}
	\item Cache Size
	\item Block Size
	\item Mapping function
	\item Replacement algorithm
	\item Writ policy
	\item Number of cache levels.
\end{itemize}

\subsection{Direct Memory Access}
Trois techniques sont possibles pour les opérations I/O:
\begin{itemize}
	\item Programmed I/O: Le dispositif I/O prend les mesures nécessaires pour se préparer à recevoir des données mais n'interrompt pas le processeur.
      C'est donc au processeur de vérifier régulièrement si le module I/O est prêt.
      Les performances sont largement dégradées
	\item Interrupter-driver I/O: Le module I/O interrompt le processeur quand il est prêt,
      avant le processeur fait autre chose.
      Ça demande quand même une intervention active du processeur et donc c'est pas top.
      \begin{enumerate}
        \item Le transfert I/O est limité par la vitesse à laquelle le processeur peut tester et servir le matériel et
        \item Le processeur est mainetnu par la gestion du transfert I/O.
      \end{enumerate}
	\item DIRECT MEMORY ACCESS: réalisé dans un module séparé sur le bus système ou incorporé dans le dispositif I/O.
      Le processeur délègue tout le travail au module DMA et poursuit son boulot.
      Quand le transfert est fini, le module DMA envoie une interruption au processeur.
\end{itemize}

\subsection{Multiprocessor and Multicore Organization}
\label{sec:smp}
Il y a trois types de parralélisme: SMP, Multicores computes, et clusters.
Cluster est vu plus loin dans le livre.

\subsubsection{Symmetric Multiprocessors}
\begin{enumerate}
	\item Il y a deux ou plus processeurs semblables de capacités comparables.
	\item Ces processeurs partagent la même mémoire et les I/O et sont interconnectés par un bus => Accès mémoire identique.
	\item Tous les deux ont accès aux I/O.
	\item Ils savent faire la même chose.
	\item L'OS fait interagir les deux procs.
\end{enumerate}
Voir \cite[p.~53-54]{stallings} pour la définition et les avantages.

\subsubsection{Mutlicore Computers}
Plusieurs processeurs (cores) sur un seul die.
Voir exemple \cite[p.~56]{stallings}.

\subsection{Appendix 1A}
Discussions des performances.

\newpage


%------------------------------------------------------------------------------
%				Chapitre 2
%------------------------------------------------------------------------------
\section{Chap 2 : Operating System Overview}

\subsection{OS Objectives and Functions}
Définition d'un OS avec ses 3 objectifs: Convenience, Efficiency, Abillity to evolve.

\subsubsection{The OS as a User/Computer Interface}

Explique en quoi l'OS constitue une interface entre l'utilisateur et l'ordinateur dans les différentes régions:
\begin{itemize}
\item Program development
\item Program execution
\item Access to I/O devices
\item Controlled access to files
\item System access
\item Error detection and response
\item Accounting
\item Instruction Set Architecture (ISA)
\item Application Binary Interface (ABI)
\item Application Programming Interface (API).
\end{itemize}

\subsubsection{The OS as Resource Manager}
Explique en quoi est ce que l'OS est responsable de gérer les ressources,
tout en faisant partie de celle-ci (Contrairement à un thermomètre qui controle un système de chauffage,
tout en ne faisant pas partie du système de chauffage).

\subsubsection{Ease of Evolution of an OS}
L'OS évolue à cause de plusieurs raisons:
\begin{itemize}
  \item Hardwares upgrades plus new types of hardware
  \item New services
  \item Fixes.
\end{itemize}

\subsection{The Evolution of OS}

\subsubsection{Serial Processing}
Dans le temps, les gens venaient avec leur progrmamme sur cartes perforées et venaient executer leur programme sur une grosse machine commune.
Cela présentait 2 gros problèmes:
\begin{itemize}
  \item Scheduling: Temps perdu pour rien car plage d'exécution fixée.
  \item Setup time: Pas de linking en début d'exécution,...
Prend bcp de temps rien que
    pour charger le programme sur l'ordi.
\end{itemize}

\subsubsection{Simple Batch Systems}
L'idée centrale est d'utiliser un moniteur.
Avec ce type d'OS, l'utilisateur n'a plus accès au processeur.
Les jobs sont placés en files, disponibles pour le monitor.
En gros, le monitor est un programme qui tourne et dont le but est de faire tourner les jobs en attente,
dans le but de minimiser le temps entre les exécutions (Scheduling function).
Voir la \cite[p.~74]{stallings} pour plus d'info sur la structure d'un job.
Pour que ça marche, il faut certaines caractéristiques hardware (sinon ben caca):
\begin{itemize}
  \item Memory Protection
  \item Timer
  \item Priviledged instructions
  \item Interrupts.
\end{itemize}

On voit arriver le concept de p-mode d'opération: User Mode et Kernel Mode


\subsubsection{Multiprogrammed Batch Systems}
Quand un job attent pour I/O, le processeur peut passer à un autre job qui préférentiellement n'attend pas d'I/O.
Cela s'appelle le multiprogramming ou multitasking.
Voir exemple \cite[p.~77]{stallings}.

\subsubsection{Time-Sharing Systems}
But: Minimiser le temps de réponse, avec des commandes entrées dans le terminal.
Voir ex \cite[p.~79]{stallings}.
NB le CTSS est primitif par rapport au time-sharing actuel, mais à l'époque ils n'avaient pas les même facilités d'interface que nous.

\subsection{Major Achievements}

\subsubsection{The Process}
Définition de Processus.
Trois lignes de développement ont poussé à la création du concept de processus:
\begin{enumerate}
  \item Multiprogramming batch operation
  \item Time-sharing : Le but est d'être capable de supporter plusieurs utilisateurs à la fois
  \item Real-time transaction System
\end{enumerate}

Un des premiers outils disponibles fut les interruptions.
Mais il s'avère que l'implémentation est incroyabelement sophistiquée dans ce contexte là.
Si il arrive une erreur d'exécution, on ne sais pas dire d'ou elle viens car il n'y a pas de structure (Il faut faire la distinction entre erreur matérielle, erreur logicielle, ou erreur d'exécution due à un ordre bien particulier non compatible d'exécution).
En général, ces erreurs viennent de 4 causes:
\begin{itemize}
  \item Improper synchronization
  \item Failed mutual exclusion
  \item Nondeterminate program operation
  \item Deadlocks
\end{itemize}

Pour éviter ces erreurs, il faut une manière systématique de monitorer et de controler les différents programmes s'exécutant sur le processeur, et la définition de processus fournit la base de cette solution.
Processus : 3 composants:
\begin{enumerate}
  \item Un programme exécutable
  \item Les données associées nécessaires au programme (variables, espace de traval, buffer,...)
  \item Le execution context (ou process state) du programme: SUPER IMPORTANT.
    Ce sont les données internes avec lesquelles l'OS est capable de controler le processus.
\end{enumerate}


\subsubsection{Memory Management}
L'OS a 5 principales responsabilités par rappot à la gestion de la mémoire:
\begin{enumerate}
  \item Process isolation
  \item Automatic allocation and management
  \item Support of modular programming
  \item Protection and access control
  \item Long-term storage.
\end{enumerate}
Ces 5 responsabilités sont assurées grace à la mémoire virtuelle et aux système de fichiers.
Explication du système de pages, et d'adresse virtuelle/réelle \cite[p.~87]{stallings} +
explication de l'adressage en mémoire virtuelle \cite[p.~88]{stallings}.

\subsubsection{Information Protection and Security}
Regroupe 4 catégories:
\begin{itemize}
  \item Availability
  \item Confidentiality
  \item Data Integrity
  \item Authenticity.
\end{itemize}

\subsubsection{Scheduling and Resource Management}
Toutes les allocations de ressources et la scheduling policy doit considérer 3 facteurs:
\begin{enumerate}
  \item Fairness
  \item Differential responsiveness
  \item Efficiency.
\end{enumerate}
Explication de la technique de Round-Robin (queues circulaires) + différentes queues d'attentes (long, short, I/O).

\subsection{Developments Leading to Modern OS}
A cause de différents facteurs comme l'arrivée d'internet, un certain nombre de catégories ont été fort travaillées ces dernières années: \\
\begin{itemize}
  \item Microkernel Architecture
  \item Multithreading
  \item Symmetric multiprocessing
  \item Distributed OS
  \item Object-Oriented design.
\end{itemize}

\subsection{Virtual Machines}

\subsubsection{VM and Virtualizing}
Virtualization technology autorise un PC ou un serveur à pouvoir lancer simultanément plusieurs OS ou plusieurs sessions sur le même OS.
Un OS hôte peut supporter un certain nombre de Virtual Machines (VM)
Le Virtual Machine Monitor (VMM) gère toutes les communications des OS avec le processeur, le storage medium et le réseau.

\subsubsection{VM Architecture}
L'ABI définit la machine comme vue par le processus.
L'API définit la machine comme vue par l'application.
Pour l'OS, c'est l'ISA qui définit l'interface entre le système et la machine.

\paragraph{Process VM}
Un process VM est une plateforme virtuelle qui execute un seul programme.
Le process VM est créé quand le processus est créé et est terminée quand le processus est terminé.
(ex: JAVA, .NET FRAMEWORK)

\paragraph{System VM}
Lire \cite[p.~96]{stallings}

\subsection{OS Design Considerations for Multiprocessor and Multicore}

\subsubsection{Symmetric Multiprocessor OS Considerations}
En gros, le kernel peut s'exécuter sur le processeur qu'il veut,
et il peut même exécuter différentes parties indépendantes de lui-même sur plusieurs processeurs.
Le problème est que toutes ces ressources sont partagées et donc il faut pas que ça couille.
Donc un OS multiprocesseurs doit fournir toutes les fonctionnalités d'un multiprogramming
system avec des caractéristiques en plus pour permettre l'utilisation de plusieurs processeurs.
Ces caractéristiques sont:
\begin{itemize}
  \item Simultaneous concurrent processes or threads
  \item Scheduling
  \item Synchronization
  \item Memory management
  \item Reliability and fault tolerance.
\end{itemize}

\subsubsection{Multicore OS Considerations}
Les considérations sont les même que pour les systèmes SMP (voir section~\ref{sec:smp}).
Mais il vient se rajouter plusieurs choses.
Le problème est de bien exploiter les ressources potentielles offertes par le multicores.
La question est de savoir comment bien exploiter le parralélisme qui existe à 3 niveau dans les systèmes multicoeurs actuels:
\begin{enumerate}
  \item
    Hardware parallelism within each core processor.
    (Instruction Level Parallelism)
  \item Multiprogramming and multithreaded execution within each processor
  \item A single application executing in concurrent processes or threads accross multiple cores.
\end{enumerate}

\subsection{Microsoft Windows Overview}
Il faut retenir que windows a presque pas changé depuis NT :)

\subsection{Traditional Unix Systems}
Au début le kernel était pas modulaire, c'est pour cela qu'ils ont ajouté plein de code pour connaitre ce qu'on a aujourd'hui.

\subsection{Modern Unix Systems}
UNIX c'est les meilleurs.

\subsection{Linux}
La plupart des kernel UNIX sont monolithiques, et ben linux a fait pareil.
Il a l'avantage des monolithiques qu'il y a pas d'overhead à passer d'un serveur à l'autre mais
il n'a pas les inconvénients grâce à ses loadable modules.
Si un modules doit être modifié, on est pas obligé de recharger tout le kernel et de relinker tout le schmilblick.
Les modules peuvent avoir des dépendances entre eux.
Tout est géré par une structure chainée, les dépendances sont automatiquement loaded en cas de besoin et
unloaded si elles ne sont plus nécessaires pour gagner de la place en RAM.
Le kernel est composés des composantes suivantes, voir \cite[p.~117]{stallings} pour les relations entre eux.
\begin{itemize}
  \item System calls;
  \item Virtual memory;
  \item Signals;
  \item Traps \& faults;
  \item Physical memory;
  \item Processes \& scheduler;
  \item Char device drivers;
  \item File systems;
  \item Block device drivers;
  \item Interrupts;
  \item Network protocols;
  \item Network device drivers;
\end{itemize}
Un processus en userspace peut accéder directement au System calls et Virtual memory et est
accédé directement par Signals.

\subsection{Linux Vserver VM Architecture}
Si on veut $n$ machines virtuelle qui sont toutes Linux
et que le host est aussi Linux.
Linux Vserver permet de ne pas avoir $n+1$ kernels.
Il y en a qu'un seul et les machines virtuelles n'ont pas leur propre kernel.
Ils utililisent tous le même.
C'est par exemple ce que fait OpenVZ.

\newpage


%------------------------------------------------------------------------------
%				Chapitre 3
%------------------------------------------------------------------------------

\part{Processes}

\section{Chapter 3 : Process Description and Control}

\subsection{What is a Process?}
\subsubsection{Background}
D'abord il rappelle les concepts vus au 2 premiers chapitres par rapport aux applications, system software et ressources.
\subsubsection{Processes and Process Control Blocks}
Il définit un processus.
Deux éléments important: Le code du programme et un ensemble de données associées au programme.
TRES IMPORTANT: Le process control block, il contient à n'importe quel moment, un nombre de caractéristiques qui permettent de définir uniquement le processus:
\begin{enumerate}
  \item Identifier
  \item State
  \item Priority
  \item Program Counter
  \item Memory Pointers
  \item Context Data
  \item I/O Status Information
  \item Accounting information.
\end{enumerate}
Toutes ces données sont suffisante pour que le processeur puisse gérér tous les processus.

\subsection{Process State}
\begin{description}
  \item[Trace d'un processus] séquence d'instructions à executer dans le processus
  \item[Dispatcher] permet de passer d'un processus à l'autre pendant l'exécution.
    Change de processus toutes les 6 instructions par exemple (version simplifiée de la réalité)
\end{description}

\subsubsection{A Two State Process Model}
Un processus peut avoir 2 états: Running ou Not Running.
L'OS doit pouvoir garder trace de chaque processus, cela se fait avec le PCB.
Les processus qui sont Not Running sont placés dans une sorte de queue (Ou plutot leur PCB est dans une queue), attendant patiemment leur tour.
Ce modèle est trop simple.

\subsubsection{The Creation and Termination of Processes}
Process Creation
Un processus peut être créé par 4 types d'évènements:
\begin{itemize}
  \item New Batch Job
  \item Interactive log-on
  \item Created by OS to provide a service
  \item Spawned by existing process
\end{itemize}
Process Termination
Un processus peut s'arrêter pour plusieurs raisons:
\begin{itemize}
  \item Normal completion
  \item Time limit exceeded
  \item Memory unavailable
  \item Bounds violation
  \item Protection error
  \item Arithmetic error
  \item Time overrun
  \item I/O failre
  \item Invalid instruction
  \item Priviledged instruction
  \item Data misuse
  \item Operator or OS intervention
  \item Parent termination
  \item Parent request
\end{itemize}

\subsubsection{A Five-State Model}
L'idée, c'est que tous les processus dans la file d'attente ne sont pas forcément prêts, il peuvent par exemple attendre un dispositif I/O.
Le dispatcher devrait alors chercher dans le processus celui qui est prêt et qui attend depuis le plus longtemps.
Voila pourquoi on va séparer l'état Not running en 2 états: Ready et Blocked.
On ajoute deux états et on a notre Five-State Model:
\begin{enumerate}
  \item Running
  \item Ready
  \item Blocked/Waiting
  \item New
  \item Exit
\end{enumerate}
Les différents états ainsi que les différentes transitions entre états sont très bien expliquées dans le livre.
La structure des files d'attentes est expliquée page 140.

\subsubsection{Suspended Processes}
The Need for Swapping
Si aucun processus dans la main memory n'est dans l'état ready, on va en déplacer un sur le disque(swap) et charger un processus ready du disque vers la main memory, ou lancer un nouveau processus.
Néanmoins, ceci est une opération I/O, mais comme les disk I/O sont les opérations I/O les plus rapide, ceci va améliorer les performances.
On ajoute alors un nouvel état : SUSPENDED.
Il faut faire attention: Si on a juste un état suspended, on ne fait pas la différence entre un processus qui était ready ou blocked avant d'être suspendu.
Donc on besoin de 4 états:
\begin{itemize}
  \item Ready
  \item Blocked
  \item Blocked/Suspended
  \item Ready/Suspended
\end{itemize}
Il faut aussi considérer que l'on a supposé qu'il n'y avait pas de virtual memory.
On pourrait penser que la virtual memory peut eliminer le besoin de swap explicite, mais en fait ca peut pas faire de tort, voir \cite[p.~142]{stallings}.
Toutes les nouvelles transitions sont aussi très bien expliquées :)
Other Uses of Suspension
\begin{itemize}
  \item Swapping
  \item Other OS reason
  \item Interactive user request
  \item Timing
  \item Parent process request
\end{itemize}

\subsection{Process Description}
On peut voir l'OS comme une entité qui gère l'utilisation des ressources du système par les processus.

\subsubsection{OS Control Structures}
L'OS doit garder des informations sur l'état actuel de toutes les entités qu'il gère.
Il construit et maintient des tables d'information:
\begin{enumerate}
  \item
    Memory Tables: Garde la trace de la main et virtual memory.
    (Certaines parties de la main memory sont réservées à l'OS).
    Les informations retenues sont:
    \begin{itemize}
      \item L'allocation de la main memory aux processus
      \item L'allocation de la virtual memory aux processus
      \item Des attributs de protection de certains block dans la main ou virtual memory, comme par exemple si une partie de la mémoire est partagée entre plusieurs processus
      \item Toute information nécessaire pour gérer la VM
    \end{itemize}
  \item I/O tables: Si une opération I/O est en cours, l'OS doit savoir le statut de l'opération I/O et la partie dans la main memory utilisée comme source ou destination du transfert I/O
  \item File tables: Garde des infos sur l'existence de fichiers, leur endroit dans la virtual memory; leur statut actuel et d'autres attributs.
  \item Process tables: Sera expliqué dans la prochaine section.
    D'abord deux considérations:
    \begin{enumerate}
      \item Les tables doivent être liées entre-elles.
      \item L'OS doit avant toutes choses connaitre son environnement basique (ex: qu'y a-t-il comme dispositif I/O), cela se fait par assistance humuaine ou autoconfiguration d'un programme.
    \end{enumerate}
\end{enumerate}

\subsubsection{Process Control Structures}
L'OS doit savoir 2 choses pour gérer les processus:
Process Location
Un processus inclus un programme ou un ensemble de programme à executer, associés à un ensemble de données pour les variables locales et globales, ajoutons à ca une stack, et un ensemble d'attributs pour que l'OS puisse gérer le processus.
Cette collection d'information est appellée Process Image.
N'oublions pas qu'a n'importe quel moment, la portion de la process image peut être dans la main memmory, le reste étant dans la secondary memory.
Donc les process tables doivent garder les positions de chaque pages de chaque process images.
À cet effet, il y a une primary process table avec une entrée pour chaque processus qui contienne autant de pointeurs qu'il y a de block à la process image.

\paragraph{Process Attributes}
Toutes les informations requises rentrent dans 3 catégories. Voir table page 150 pour plus de détails.
\begin{itemize}
  \item Process identification: Définit d'une manière unique le processus.
  \item Processor state information: Comprend tous les contenus des registres du processeur. (Ex: EFLAGS register in Intel x86)
  \item Process control information: Tout ce qu'il faut à l'OS pour controler les processus.
\end{itemize}
Voir figures 3.13 et 3.14 p. 153-154 pour les strucures des process images et des files d'attentes de processus.
The Role of Process Cotrol Block
C'est la structure la plus important dans un OS. Mais il y a néanmoins un soucis de protection.

\subsection{Process Control}
\subsubsection{Modes of Execution}
Un bug dans une simple routine, comme un interrupt handler, peut endommager un process control block.
Un changement dans la structure du process control block va affecter nombre de modules de l'OS.
Process Control Modes of Execution
Tous les OS ont au moins 2 modes d'exécution: Usermode et Kernelmode. La raison est simple: protéger l'OS et les tables clés de l'OS, comme le process control block. Le processeur sait dans quel mode on est grace à un bit dans le PSW. Voir la table 3.7 pour les fonctions typiques d'un OS \cite[p.~135]{stallings}.

\subsubsection{Process Creation}
On a déjà parlé des raisons qui pouvaient pousser à la création d'un processus. Cela se fait dans cet ordre.
\begin{enumerate}
  \item Assign a unique process identifier to the new process
  \item Allocate space for the process
  \item Initialize the process control block
  \item Set the appropriate linkages
  \item Create or expand other data structures.
\end{enumerate}

\subsubsection{Process Switching}
Ca l'air simple, mais ca amène quelques difficultés d'implémentations.
When to Switch Processes
Ca peut arriver chaque fois que l'OS obtient le controle des processus. Cela peut provenir de 3 facons:
\begin{itemize}
  \item Interrupt : Evènement externe et indépendant du processus courant. Le controle est alors donné à une routine (interrupt handler). Cela peut provenir de plusieurs sources (Clock interrupt, I/O interrupt, Memory fault,...)
  \item Trap : Du a une erreur ou une exception du processus courant. Soit l'erreur est fatale, le processus courant passe à l'état exit => Process switch. Soit elle ne l'est pas, et en fonction de l'erreur =>process switch ou resume process.
  \item Supervisor call: ex: un user process demande une I/O operation (file open). L'appel est transmis à une routine de l'OS.
\end{itemize}
\paragraph{Mode Switching}
Lors d'une instruction cycle, si une interruption est en attente, le processeur fait 2 choses:
\begin{enumerate}
  \item Place le PC à l'adresse de départ de l'interrupt handler.
  \item Switch de usermode à kernelmode pour que l'interruption puisse bénéficier des instructions privilégiées.
\end{enumerate}
\paragraph{Change of Pocess State}
Les différentes étapes d'un process switch sont les suivantes:
\begin{enumerate}
  \item Sauver le contexte du processeur (PC, et autre registres)
  \item Update le Process control block du processus qui est dans le running state.
  \item Déplacer le process control block dans une file d'attente appropriée
  \item Sélectioner un nouveau processus à exécuter.
  \item Update du process control block du nouveau processus à exécuter.
  \item Update des structures de gestion des données en mémoire (memory management data structures)
  \item Restaurer le contexte du processeur à celui qui existait au dernier moment ou le processus fraichement rappelé s'était exécuté.
\end{enumerate}

\subsection{Execution of the OS}
Nous avons vu 2 faits intérressants à propos de l'OS
\begin{itemize}
  \item L'OS fonctionne comme un simple programme étant donné que c'est un ensemble de programmes exécutés par le processeur
  \item L'OS donne fréquemment le contrôle au processeur et dépend du processeur pour retrouver le controle.
\end{itemize}
On peut alors se demander si l'OS est un processus, et si oui, comment est-il controllé. On va voir 3 facons de faire das les OS contemporains.

\subsubsection{Nonprocess Kernel}
Approche traditionnelle commune à de nombreux OS plus anciens. Le kernel s'exécute en dehors de tout processus. L'OS a sa propre région de mémoire et sa propre stack. Le concept de processus ne s'applique alors qu'au programmes utilisateurs. L'OS s'exécute comme une entité séparée dans un mode privilégié.

\subsubsection{Execution withing User Processes}
\label{sec:uproc}
Alternative très commune avec les OS sur de petits ordinateurs (PC,..), on execute virtuellement toutes les fonctionnalités de l'OS dans le contexte d'un processus utilisateur. Chaque process image contient également les programmes, données et stack pour les kernel programs. C'est-à-dire qu'une kernel stack séparée est utilisée pour gérer les appels-retours de fonctions quand le processus est en kernelmode. Toutes les données et codes de l'OS sont en partage avec tous les user processes.

\subsubsection{Process-Based Operating System}
Dernière alternative, implémenter l'OS comme une collection de processus système.
Avantages:
\begin{itemize}
  \item Impose une discipline de program design qui encourage l'utilisation d'un OS modulaire avec un minimum d'interfaces propres entre modules.
  \item Des parties non critiques de l'OS sont implémentées dans les processus séparés.
  \item C'est très utilse pour un environnement multiprocesseur ou multicomputer, car on eut dédier certains processeurs à l'exécution de différents morceaux de l'OS, augmenttation des performances.
\end{itemize}

\subsection{Security Issues}
Le clé de sécurité dans le design de tout OS qui se repecte est de détecter, voire prévenir, les essais d'un utilisateur ou d'un malware d'accéder au privilèges nonautorisés du système, plus particulièrement aux droits de superutilisateur (root access).

\subsubsection{System Access Threats}
Il y a deux types de System Access Threats:
\begin{itemize}
  \item Intruders (hackers, crackers)
    \begin{itemize}
      \item Masquerader : outsider
      \item Misfeasor : insider
      \item Clandestine user : (in/out)sider
    \end{itemize}
  \item Malicious Softwares (Malwares)
    \begin{itemize}
      \item Il y a ceux qui ont besoin d'un programme hôte = parasitic (bomb, virus,backdoors) et ceux qui sont indépendant (worms, bot)
      \item Il y a ceux qui ne se réplique pas (logic bomb, backdoors et bot) et ceux qui se dispersent (virus, worms)
    \end{itemize}
\end{itemize}

\subsubsection{Countermeasures}
Il y a l'intrusion detection. Voir \cite[p.~165]{stallings} les différent type de détections.
Il y a l'authentification. Première ligne de défense, voir livre Il y a le controle d'acces. voir livre Il y a le pare-feu. spécialisé dans le reseau, surveille les fichiers suspects.

\subsection{UNIX SVR4 Process Management}
UNIX SVR4 utilise ce qui est présenté à la section~\ref{sec:uproc}:
``Execution within User Processes''.
Il y a deux mode, usermode et kernelmode

\subsubsection{Process States}
Un total de 9 états:
\begin{itemize}
  \item user running
  \item kernel running
  \item ready to run, in memory
  \item asleep in memory
  \item ready to run, swapped
  \item sleeping, swapped
  \item preempted
  \item created
  \item zombie
\end{itemize}

\subsubsection{Process Description}
Voir pages 169-171. explication d'une process image, de la process table entry et U area.

\subsubsection{Process Control}
Explication des étapes de création d'un processus avec fork(), et des différentes décisions d'exécution qui peuvent êtres prises à la fin de la création.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 4
%------------------------------------------------------------------------------

\section{Chapter 4 : Threads}

\subsection{Processes and Threads }
\subsubsection{Multithreading}
Multithreading: abilité d'un OS à supporter de multiples chemins d'exécution concurrents à l'intérieur d'un processus. Dans un environnement multithreadé, un processus est une unité d'allocation de ressources et une unité de protection. Cela comprend:\\
• Une espace d'adresse virtuelle qui contient la process image\\
• Un accès protégé au processeurs, autres processus, fichiers et dispositifs I/O.\\
Dans un processus, il peut y avoir plusieurs thread, chacun possédant:\\
• Un état d'exécution (Running,...)\\
• Un thread context sauvé si not running (Un thread est un Program Counter\\
indépendant dans un processus.) \\
• Une stack\\
• Un espace de stockage de variable local par thread\\
• Un accès à la mémoire et aux ressources du processus.\\
Dans le processus, il y a toujours un seul process control block et un user adress space. Le gros bénéfice des thread provient des performances:\\
• Faut moins de temps pour créer un thread qu'un nouveau processus (10 fois moins)\\
• Faut moins de temps pour terminer un thread qu'un processus\\
• Faut moins de temps pour switcher entre deux thread dans le même processus que de switcher entre 2 processus.\\
• 2 thread peuvent communiquer entre eux sans passer par le kernel, contrairement aux processus.\\
Les thread sont super intéressants sur un pc multiprocesseur. Mais également sur un mono processeur. En voici quelques exemples sur un multiprocesseur (\cite[p.~181]{stallings})\\
• Foreground and background work \\
• Asynchronous processing\\
• Speed of execution\\
• Modular program structure\\
\subsubsection{Thread Functionality}
Thread States
Les états clés sont running, ready et blocked. Suspended n'a pas beaucoup de sens car il est propre aux processus. Il y a 4 opérations qui peuvent changer l'état d'un thread: Spawn, Block, Unblock et Finish.\\
Thread Synchronization\\
Comme les thread partagent les mêmes ressources du processus, il est nécessaire de synchroniser les activités des différents threads pour qu'ils n'interfèrent pas en eux ou ne corrompent les structures de données. C'est en général le même problème que la synchronisation des processus, voir chapitre 5 et 6.
\subsection{Types of Threads}
\subsubsection{User-Level and Kernel-Level Threads}
User-Level Threads
Tout le travail de gestion des threads est fait par l'application et le kernel n'est pas au courant de l'existence de threads. Cela se fait en utilisant des librairies de threads (genre POSIX), qui sont des packages pour la gestion ULT. Ces librairies contiennent les codes, pour créer, détruire, scheduler,... les threads. Le kernel continue de scheduler le processus comme une unité et lui assigne un seul état d'exécution. Voir ex \cite[p.~186]{stallings} Avantage de ULT \\
1. Le thread switching ne demande pas de privilèges en mode kernel \\
2. Le scheduler peut être spécifique à l'application (round robin, priority-based,...) \\
3. Les ULTs peuvent tourner sur tous les OS. \\
Désavantages de ULT \\
1. Les appels systèmes sont souvent bloquants, si un ULT fait un appel système bloquant, tous les autres threads sont également bloqués \\
2. Dans une stratégie purement ULT, les application multithreadées ne savent pas profiter des avantages du multiprocessing. En effet, un seul processus peut s'exécuter à la fois, donc un seul thread aussi. \\
Kernel-Level Threads \\
Tout le travail de gestion des threads est fait par le kernel. Il y a un API dans les installations thread du kernel. (Ex:Windows)
Les KTL n'ont pas les deux désavantages des UTL, de plus les routines de gestion des threads du kernel peuvent être multithreadées. Le principal désavantage est que le transfert de controle d'un thread à l'autre dans le même processus demande un mode switch du kernel, ce qui prend plus de temps.\\
Combined Approach \\
Mélange les deux pour profiter des avantages des 2 si c'est bien réalisé. Solaris fait ça, en forçant la relation UTL/KTL à être one-to-one.
\subsubsection{Other Arrangements}
On a vu le One thread: One process (Traditional Unix implementation)
Le Many threads: One process (Window NT, Solaris, Linux,...)
Le One thread:Many Process (Le thread saute de processus en processus, et donc peut migrer de système) Voir \cite[p.~189]{stallings} Le Many to Many a été exploré dans un OS expérimental, TRIX. voir \cite[p.~188]{stallings}
\subsection{Multicores and Multithreading}
\subsubsection{Performance of Software on Multicore}
En gros, on nous explique fonctionner avec plusieurs processeurs augmente l'éfficacité d'exécution si on sait bien paralléliser sont code, maintenant à partir d'un certain niveau, les performances se dégradent. Mais pour certaines applications il est possible d'exploiter le multiprocessing de manière optimale (voir exemples \cite[p.~193]{stallings})
\subsubsection{Application Example: Valve Game Software}
Gordon Freeman serait fier de lire cette section. Cette section explique en quoi le moteur physique de Valve utilise le multithreading à bon escient.
\subsection{Windows 7 Threads ans SMP Management}
Explication de la structure d'un thread ou d'un processus. L'un comme l'autre, ce sont des objets.
\subsubsection{Process and Thread Objects}
Explication du process object et thread object et leurs attributs.
\subsubsection{Multithreading}
Windows supporte la concurrence des processus car des threads dans différens processus peuvent s'exécuter concurrentiellement. Un processus orienté-objet multithreadé est une idée efficace pour implémenter une application de serveurs, car elle peut gérer un certain nombre de clients concurrentiellement.
\subsubsection{Thread States}
Il y a 6 états: \\
• Ready \\
• Standby \\
• Running \\
• Waiting \\
• Transition \\
• Terminated \\
\subsubsection{Support for OS Subsystems}
Explique en gros comment un sous-système doit faire pour créer un nouveau processus ou thread... Pas vraiment développé.
\subsubsection{Symmetric Multiprocessing Support}
Windows supporte le SMP. En effet, un thread peut s'exécuter sur n'importe quel processeur (généralement le premier disponible). Le dispatcher peut aussi essayer de réassigner un processus avec le même processeur que sa dernière exécution pour pouvoir réutiliser la mémoire cache de sa dernière exécution. Il est ausi possible q'une application force l'exécution d'un thread sur certains processeurs.
\subsection{Solaris Thread and SMP Management }
\subsubsection{Multithreaded Architecture}
Explication des 4 types de thread-related concepts: Process, User-level Threads, Lightweight processes, Kernel Threads. Voir \cite[p.~202]{stallings}.
\subsubsection{Motivation}
Le but d'une implémentation des thread à 3 niveau (ULT, KLT, LWP) est de faciliter la gestion des threas par l'OS.
\subsubsection{Process Structure}
Explication de la structure d'un processus dans Solaris par rapport à celle du Unix traditionnel.
\subsubsection{Thread execution}
Voir le diagramme d'état. il y a:  \\
• RUN \\
• ONPROC  \\
• SLEEP \\
• STOP \\
• ZOMBIE \\
• FREE\\
\subsubsection{Interrupts as Threads}
Solaris a implémenté les interruptions comme des kernel Threads. Voir \cite[p.~206]{stallings}
\subsection{Linux Tasks}
Un processus ets une structure de donnée . Voir \cite[p.~207]{stallings}
\subsubsection{Linux Threads}
Il n'y a pas de distinction entre processus et thread.
\subsubsection{Mac OS X Grand Central Dispatch (GCD)}
La flemme ...

\newpage

%------------------------------------------------------------------------------
%				Chapitre 5
%------------------------------------------------------------------------------


\section{Chapter 5 : Mutual Exclusion and Synchronization}

Explique les situations où on peut trouver de la concurrence et définit les mots clés (deadlock, critical section,...)
\subsection{Principles of Concurrency}
Interleavinget overlapping apporte plusieurs problèmes: Le partages des ressources globales est mis en péril ; l'allocation des ressources ne peut plus être facilement gérée par l'OS, on peut avoir des deadlock ; les erreurs sont difficiles à trouver car elles sont non-déterministes et reproductibles.
\subsubsection{A Simple Example}
Un exemple simple avec la fonction echo montre plusieurs scénarios d'erreurs. Conclusion, il faut controller l'acces aux ressources partagées.
\subsubsection{Race Condition}
Cela arrive quand plusieurs processus ou thread lisent et écrivent des données et que le résultat final dépend de l'ordre d'exécution des instructions. Exemple à la clé.
\subsubsection{OS Concerns}
Explique les problèmes de gestion et de design qu'apporte la concurrence du point de vue de l'OS Voir \cite[p.~224]{stallings}. Le dernier point est important dans ce chapitre, il dit que le fonctionnement d'un processus doit être indépendant de la vitesse à laquelle son exécution est menée par rapport à la vitesse des autre sprocessus concurrents.
\subsubsection{Process Interaction}
L'interaction d'un processus avec les autres dépend de son degré de conscience des autres. Il y en a 3: \\
• Les processus sont inconscients des autres : Les processus ne sont pas censés travailler ensemble. Il y a donc une relation de compétition pour les ressources. Cela apporte le problème de mutual exclusion, de deadlock et de starvation. \\
• Les processus sont indirectement conscient des autres : Ne connaissent pas les autres processus, mais partage le même objet (I/O buffer,...). C'est donc une relation de coopération par partage. Même problème qu'au dessus + problème de data coherence \\
• Les processus sont directement conscients des autres: Il peuvent communiquer entre-eux. C'est une relation de coopération par communication. \\
. Starvation et deadlock sont présents, Ces différents degrés de conscience ont leurs conséquences et leurs problèmes. Voir table 5.2 \cite[p.~225]{stallings}
\subsubsection{Requirements for Mutual Exclusion}
Toute ce qui veut apporter un support à l'exclusion mutuelle doit respecter les critères suivants: \\
1. L'exclusion mutuelle doit être renforcée. Seulement un processus à la fois est autorisé dans la section critique, parmis les processus qui ont une section critique pour la même ressource partagée. \\
2. Un processus qui s'arrête dans sa section non critique doit le faire sans interférer avec les autres processus. \\
3. Il ne peut pas être possible pour un processus demandant de rentrer dans une section critique d'être mis en attente indéfiniment: Pas de starvation, ni de deadlock. \\
4. Quand aucun processus n'est dans la section critique, n'importe quel processus demandant pour rentrer dans cette section critique doit avoir la permission sans attente. \\
5. Aucune considération n'est prise par rapport aux vitesses relatives des processus et au nombre de processeurs. \\
6. A processus reste dans une section critique pendant un temps fini. \\

\subsection{Mutual Exclusion: Hardware Support}
\subsubsection{Interrupt Disabling}
Dans un systeme uniprocesseur, il n'y a pas d'overlapped execution. Donc il est suffisant de désactiver les interruption d'un processus pendant qu'il est dans sa section critique. Mais les performances sont nettement dégradées et ca ne marche que pour les machines uniprocesseurs.
\subsubsection{Special Machine Instructions}
Les concepteurs de processeurs ont proposés des instructions machines, sorte de formule magique qui exécute plusieurs actions AUTOMIQUEMENT, et bloque les ressources pendant le temps d'exécution de l'instruction machine. \\
• Compare et Swap Instruction: On fait de l'exclusion mutuelle en mettant une variable bolt à 0, et le seul processus qui peut accéder à la section critique est celui qui trouve bolt égal à 0. \\
• Exchange Instruction: Une variable bolt est initialisée à 0, chaque processus à une variable initialisée à 1. Le seul processus qui peut entrer dans la section critique est celui qui trouve bolt égal à 0 et le met à 1 pour bloquer les autres. \\
Avantages: \\
• Applicable à n'importe que nombre de processus et n'importe que nombre de processeurs \\
• Simple et facile à vérifier \\
• Peut définir plusieurs sections critiques, à chaque section sa variable Désavantages: \\
• Busy waiting: Dépense du temps de processeur pour rien  \\
• Starvation possible\\
• DeadLock possible \\
\subsection{Semaphores}
Regardons plutot du coté des mécanismes de l'OS et du langage de programmation pour la concurrence. Les différents mécanismes fréquemment utilisés sont les suivants: \\
• Semaphore \\
• Binary Semaphore \\
• Mutex \\
• Condition Variable \\
• Monitor \\
• Event Flags \\
• Mailboxes/Messages \\
• SpinLocks \\
Définition et explication du fonctionnement d'un sémaphore page 235. \\
Définition et explication du fonctionnement d'un sémaphore binaire page 236. Définition et explication du fonctionnement d'un mutex page 236. Un mutex est comme un sémaphore sauf que le processus qui a bloqué le mutex DOIT être celui qui va le débloquer.
Si la queue d'attente est une FIFO → Strong Semaphore, sinon weak semaphore.
\subsubsection{Mutual Exclusion}
Un solution utilisant les sémaphores est donnée page 239, avec un schéma d'exécution.
\subsubsection{The Producer/Consummer Problem}
Fonctionne par essais et trouve une solution avec un buffer infini et des sémaphores page 241.
Une solution avec un buffer de taille finie est donnée \cite[p.~243]{stallings}
\subsubsection{Implementation of Semaphores}
Les opérations SemWait et SemSignal doivent être implémentées de manière atomique primitive. On peut le faire avec l'algorithme de Peterson ou avec une instruction machine. Voir \cite[p.~245]{stallings} ou Appendix A.
\subsection{Monitors}
Le moniteur est une construction en langage de programmation qui fournit les même fonctionalités que les sémaphoses, le tout en étant plus facile à controler. Il autoriste le programmeur à mettre un lock sur n'importe quel objet.
\subsubsection{Monitors with Signals}
Définition d'un moniteur, plus les condition variable, et ensuite explication de la structure d'un moniteur \cite[p.~248]{stallings}
Solution du producer/consumer avec buffer de taille fixée utilisant des moniteurs \cite[p.~249]{stallings}.
L'avantage avec les sémaphores est que toutes les fonctions de synchronisation sont confinées dans le moniteur.
Moniteur bien programmé → l'accès aux ressources protégées est correct pour tous les processus
Tous les processus qui ont accès aux ressources protégées sont bien programmés avec des sémaphores → L'accès aux ressources protégées est correct.
\subsubsection{Alternate Model of Monitors with Notify and Broadcast}
La définition de moniteur comme vu au point précédent à deux désavantages: \\
• Si le processus qui envoie le csignal n'a pas fini avec le moniteur, alors deux process switch supplémentaires sont requis: un pour bloquer ce processus et un pour le résumer quand le moniteur redevient accessible \\
• Le process scheduling associé avec un signal doit être complement fiable. \\
Voilà pourquoi on voit un autre type de moniteur fait à la base en langage Mesa. csignal est remplacé par cnotify, qui a un fonctionnement différent, réglant le problème de process switch additionnels et réglant également le problème éventuel de starvation.
\subsection{Message Passing}
L'échange d'informations et l'exclusion mutuelle peuvent être fournis avec le message passing.
\subsubsection{Synchronization}
• Send \\
– blocking: Le processus bloque tant que le message n'a pas été recu \\
– nonblocking : Le processus continue son exécution, peut importe la réception du signal \\
• Receive \\
– blocking Si aucun message n'est arrivé, il bloque \\
– nonblocking : Si aucun message n'est arrivé, il s'en fout – test for arrival \\
Blocking send, blocking receive → rendez-vous (synchronisation très étroite) Nonblocking send, blocking receive → Combinaison la plus utile (ex:server) Nonblocking send, nonblocking receive \\
Le danger avec les nonblocking send, c'est qu'une erreur peut pousser un processus à envoyer sans arrêt des messages, et consumer les ressources du système inutilement. \\
Les blocking receive peuvent bloquer infiniment si le processus qui devait envoyer se plante avant de le faire. A côté de ca, des messages peuvent être définitivement perdu si un utilise des nonblocking receive et que le processus qui recoit fait sa demande avant le processus qui envoie.
\subsubsection{Addressing}
• Direct \\
– send: Le processus de destination est spécifiquement identifié  \\
– receive : \\
explicit : Le processus qui envoie est spécifiquement désigné. Le processus sait à l'avance de qui va venir le message \\
implicit : ex imprimante, on accepte un message de n'importe qui mais le paramètre ”source” possède une valeur retournée après que le message aie été transmis. \\
• Indirect : Les messages sont envoyé à une structure de donnée partagées qui sont des queues qui gardes les messages (mailboxes). Il peut y avoir différents types de relation entre sender et receiver: one-to-one (lien de communication privé), manyto-one (client/server application, on appelle ca un port), one-to-many (Un message est traité par plein de processus), many-to-many (plusieurs processus server fournissent un service concurrent à plusieurs clients) \\
– static: Les ports sont souvent statiques \\
– dynamic: Si il ya beaucoup de clients et de va et viens, on peut faire ca
dynamique avec les primitifs connect et disconnect. \\
– ownership: Si le processus qui recoit dans un port est détruit, le port est détruit. \\
\subsubsection{Message Format}
\noindent • Content \\
• Length \\
– fixed: La taille des messages est petite et fixée, si un grand nombre de donnée doit être transféré, le tout est mis dans un fichier et une référence vers le fichier est envoyée \\
– variable: Programmation un peu plus flexible. Contient un header, et un body \\
\subsubsection{Queuing Discipline}
Le plus facile est de faire une FIFO mais cela n'est pas suffisant pour des messages urgents, on peut donc entrevoir la priorité des messages , ou autoriser le receiver d'inspecter la queue et de choisir le prochain message à traiter.
\subsubsection{Mutual Exclusion}
Exemple avec blocking receive et non blocking send. \cite[p.~257]{stallings}
Solution au producer/consumer problem avec du message passing \cite[p.~258]{stallings}
\subsection{Readers/Writers Problem}
Il y a une zone de données partagée en plusieurs processus. A tout moment, il faut que: \\
1. N'importe quel nombre de readers peuvent lire le fichier simultanément \\
2. Seulement un writer peut écrire dans le fichier au même moment \\
3. Si un writer écrit dans un fichier, aucun lecteur ne peut le lire. \\
On va apercevoir deux solutions.
\subsubsection{Readers have Priority}
Solution avec sémaphores \cite[p.~260]{stallings}.
\subsubsection{Writers have Priority}
Solution avec sémaphore \cite[p.~261]{stallings}. Cette solution est plus compliquée que lorsque les reader ont priorité, je me conseille de la relire attentivement. Une autre solution utilisant le messag passing est donnée \cite[p.~263]{stallings}.
\newpage

%------------------------------------------------------------------------------
%				Chapitre 6
%------------------------------------------------------------------------------

\section{Chapter 6 : Deadlock and Starvation}

\subsection{Principles of DeadLock}
Deadlock : Blocage permanent d'un ensemble de processus qui combattent pour des ressources système ou pour communiquer entre eux. Exemple avec le carrefour. Exemple de joint progress diagram.
\subsubsection{Reusable Resources}
Ce sont des ressources qu'on peut réutiliser, qui ne sont pas dégradées par un processus (processeur, I/O channels, main et secondary memory,...) . Un deadlock arrive si chaque processus tient une ressources et demande l'autre. Une autre sorte de deadlock est par rapport à la main memory, et l'allocation de mémoire: Si deux processus durant leur exécution concurrente demandent plus d'allocation de mémoire que peut fournir la main memory, alors il y a deadlock. Ce problème est réglé avec l'utilisation de la VM.
\subsubsection{Consumable Resources}
Ressource qui peut être créée et détruite (interrupts, signals, messages,...). Un deadlock peut arriver si chaque processus attent un message de l'autre pour en renvoyer un à l'autre. Les deux processus sont en attente et ne recevront jamais rien. (difficile à trouver). Il y a un tableau \cite[p.~265]{stallings} qui explique les différentes techniques mises en oeuvre pour combattre les deadlocks.
\subsubsection{Resource Allocation Graphs}
Explicatin de graphes + exemple avec le carrefour.
\subsubsection{The Conditions for Deadlock}
Il y a 3 conditions: \\
1. Mutual Exclusion \\
2. Hold and Wait\\
3. No preemption\\
4. Circular wait → La plus importante\\
Les 3 premières implique l'existence d'une région fatale. La quatrième implique qu'un région fatale a été franchie.
\subsection{Deadlock Prevention}
C'est a dire mettre en place un système qui exclus la possibilité de deadlock. Deux facon de faire : Indirecte (Prévenir l'occurence d'une des 3 premières conditions) ou indirecte (Prévenir la 4ème condition (circular wait)).
\subsubsection{Mutual Exclusion}
On peut pas s'en passer c'est comme ca.
\subsubsection{Hold and Wait}
On peut demander que le processus se bloque tant que toutes les ressources qu'il demande ne sont pas libre. C'est inéfficace pour 2 raisons: Le processus peut rien foutre pendant longtemps, alors qu'il pourrait peut être déjà faire quelque chose avec certaines des ressources qu'il demande et les ressources prises par le processus peuvent être prises pendant longtemps, ce qui emmerde tous les autres
\subsubsection{No Preemption}
Si un processus qui tiens plusieurs ressources est bloqué sur une future requête, il peut tout lacher et recommencer plus tard. Ou si il demande une ressource qui est tenue par quelqu'un d'autre, l'autre peut lacher cette ressource. Ca prévient les deadlock si les deux processus ont la même priorité. Mais c'est partique seulement si l'état de la ressource peut être sauvé et restauré facilement, comme le processeur.
\subsubsection{Circular Wait}
Joli, mais inéfficace car ralentit les processus et refuse des ressources inutilement.
\subsection{Deadlock Avoidance}
Autorise plus de concurrence que le deadlock prevention, car autorise les 3 premières condition, mais empêche un deadlock d'arriver.
\subsubsection{Process Initiation Denial}
On ne démarre pas un processus si il peut mener à un deadlock. Une formule est donnée \cite[p.~271]{stallings}. Un processus est démarré si la demande maximale de tous les processus courants plus celle des nouveaux processus peut ête rencontrée. Ce n'est pas super optimal car on imagine le pire scénario à chaque fois.
\subsubsection{Resource Allocation Denial}
On ne donne pas de ressources à un processus si cette allocation peut mener à un deadlock. (Algorithme du banquier)
Un état sûr est un état dans lequel il existe au moins une séquence d'allocation de moire aux processus qui ne mène pas à un deadlock. Ce truc est vraiment cool, mais il y a quelques restrictions:\\
• Le maximum de ressources demandées par chaque processus doit être connu à l'avance.\\
• Les processus considérés doivent être indépendants.\\
• il doit y avoir un nombre fixé de ressources à allouer\\
• Aucun processus ne peut se barrer (exit) en maintenant des ressources.\\
\subsection{Deadlock Detection}
Les processus obtiennent les ressources dès que possible. Le but est d'empêcher la 4ème condition.
\subsubsection{Deadlock Detection Algorithm}
L'algorithme marque tous les processus qui ne mènent pas à un deadlock. Il ne préviens pas un deadlock, mais il détermine si un deadlock existe en ce moment.
\subsubsection{Recovery}
Quand un deadlock est détecté, il y a plusieurs stratégies pour retomber sur ses pattes,
on a, 1.
2. 3. 4.
par ordre croissant de complexité:
On abandonne tous les processus deadlocké (Incroyable mais c'est la technique la plus courante dans les OS)
Retour vers un checkpoint des processus deadlockés. Il faut un mécanisme de redémarrage et de roolback dans le sytème
Successivement, on abandonne un par un les processus bloqués jusqu'à ce qu'il n'y aie plus de deadlock
Successivement, preempter une par une les ressources litigieuses.
\subsection{An Integrated Deadlock Strategy}
Vu que chaque technique a ses avantages et inconvénients, on nous donne ici une manière de raisonner pour choisir la meilleure technique en fonction des cas, exemples à l'appui.
\subsection{Dining Philosophers Problem}
C'est le bazar avec les gens qui doivent choisir leur fourchette. Ca a l'air con, mais c'est pas facile à résoudre et ca illustre la coordination de ressources partagées.
\subsubsection{Solution Using Semaphores}
Solution avec sémaphore, en forcant qu'il y aie au plus n-1 philosophes à table. (\cite[p.~281]{stallings})
\subsubsection{Solution Using a Monitor}
On a un vecteur de conditions, une condition par fourchette. \cite[p.~282]{stallings}
\subsection{UNIX Concurrency Mechanisms}
Unix fourni une variété de mécanismes pour la communication interprocesseurs et la synchronisation.\\
• Pipes: C'est une FIFO, écrite par un processus et lue par un autre.\\
• Messages: associés à une queue qui fonctionne comme une mailbox\\
• Shared Memory: Bloc de mémoire virtuelle partagé par de multiples processeurs\\
• Semaphores : Ce sont ceux vu dans le livre qui sont généralisés.\\
• Signals: Similaire à une interruption hardware, mais n'utilise pas le principe de priorité.
\subsection{Linux Kernel Concurrency Mechanisms}
Contient les même trucs que UNIX.
Fournit en plus des opréations atomiques qui permettent d'éviter les simple race conditions par exemple. La liste des opérations et leurs avantages principaux sont repris \cite[p.~286]{stallings},287
La technique la plus utilisée dans Linux pour protéger une section critique est le spinlock. Voir le fonctionnement. C'est pratique et efficace seulement si le temps d'attente est censé être très court, car les processus bloqués font de l'attente active.
Voir pour semaphore, j'ai la flemme.
Barrier: Pour forcer l'ordre d'xécution des instructions.
\subsection{Solaris Thread Synchrnoization Primitives}
Possède 4 types de primitifs de synchronisation des threads: \\
• Mutual exclusion, mutex et locks\\
• Semaphores\\
• Multiple readers, single writers locks\\
• Condition variables\\
\subsection{Windows 7 Concurrency Mechanisms}
Les méthodes de synchronisation les plus importantes sont: \\
• Executive Dispatcher Objects \\
• User-mode critical sections \\
• Slim reader-writer locks\\
• Condition variables \\
• Lock-free opérations

\newpage
\part{Memory}

%------------------------------------------------------------------------------
%				Chapitre 7
%------------------------------------------------------------------------------

\section{Chapter 7 : Memory Management}
• Frame: Un bloc à longueur fixée dans la main memory \\
• Page: Un bloc à longueur fixée dans la secondary memory \\
• Segment: Un bloc de données à longueur variable dans la secondary memory. \\
\subsection{Memory Management Requirements}
\subsubsection{Relocation}
Vu qu'un programme peut être swappé et reswappé, ce serait limitant de vouloir le replacer chaque fois au même endroit, donc il faut relocaliser le processus dans différentes zones de mémoire. On ne sait pas prévoir à l'avance où va se trouverun programme dans le mémoire. En gros, le processeur et l'OS doivent être capable de traduie les références à la mémoire trouvées dans le code en véritables adresses physiques.
\subsubsection{Protection}
Chaque processus devrait être protégé desn interférences non désitrée des autres processus. La protection de la mémoire doit être satisfaite par le processeur plutot que par l'OS. Les mécanismes qui supportent la relocation supportent également la protection
\subsubsection{Sharing}
Tout mécanisme de protection doit avoir la flexibilité d'autoriser plusieurs processus à accéder à la même portion de main memory. Ex: Si un certain nombre de processus exécutent le même programme, il est avantageux d'autoriser chaque processus à accéder à la même copie du programme plutot que chaque processus en aie leur propre copie. Les mécanismes qui supportent la relocation supportent également le sharing.
\subsubsection{Logical Organization}
La main mémory est organisée comme étant un espace d'adressage linéaire, consistant en une séquence de mots. La mémoire secondaire est similaire d'un point de vue physique. Cela ne correspond pas à la facon dont les programmes sont organisés. Les programmes sont organisés sous forme de modules, ce qui apporte un certain nombre d'avantages: \\
Les modules peuvent être écrits et compilés indépendemment.  \\
Différents degrés de protection peuvent être donnés aux modules \\
On peut introduire des mécanismes avec lesquels les modules peuvent être partagés entre processus.
\subsubsection{Physcial Organization}
On a vu que la mémoire était structurée en au moins deux niveaux : Main et secondary memory. L'organisation du flux d'information entre les 2 mémoires est un problème système majeur. Cela pourrait être de la responsabilité du programmeur, mais ce n'est pas une bonne idée pour 2 raisons: \\
• La main memory disponible pour un programme et ses données peut être insuffisant. Il faut donc pratiquer de l'overlaying (programme qui fait transiter les données nécessaires et non nécessaires dans un espace rservé à l'exécution du programme). Mais même avec une aide du compilateur, cela gaspille du temps pour le programmeur. \\
• Dans un environnement multiprogramming, le programmeur ne sait pas en codant combien d'espace est disponible pour lui.
\subsection{Memory Partitioning}
La VM est basée sur la segmentation et le paging. Avant de parler de ca, on va parler de techniques plus simples qui ne font pas appel à la VM, comme le partitioning. Le tableau 7.2 (\cite[p.~311]{stallings}) est très important car il reprend toutes les techniques développées dans le chapitre et plus loin: \\
• Fixed Partitioning \\
• Dynamic Partitioning \\
• Simple Paging \\
• Simple Segmentation \\
• VM Paging \\
• VM Segmentation \\
\subsubsection{Fixed Partitioning}
On suppose que l'OS occupe une portion fixe de la mémoire et que le rste es disponible pour les processus. Le schéma le plus simple pour gérer cette mémoire disponible est de la partitioner en régions à frontières fixes.
On peut faire un partition avec des blocs de taille égales. Deux problème arrivent: \\
• Un programme trop gros pour une partition doit être redessinée par le programmeur en utilisant des overlays pour que seule une partie du programme soit dans la mémoire au même moment. (voir animation web) \\
• L'utilisation de la main memory est extrèmement innefficace. Même un tout petit programme occupe une paritition entière. Cela s'appelle de l'internal fragmentation. \\
On peut utilser une partition à taille non-fixée, mais cela apporte toujours les même problèmes, malgré le fait que cela les atténue. Un autre problème arrive néanmoins, celui du placement. Il y a deux facons de faire: La plus simple est dans mettre chaque processus dans la plus petite partition dans laquelle elle rentre. Il faut une queue d'attente pour chaque taille de partition. L'avantage est que cela minimise l'internal fragmentation. Mais si cela a l'air optimal d'un point de vue d'une partition individuelle, cela ne l'est pas pour le système tout entier. Il vaut donc mieux avoir une seule file d'attente. Un désavantage supplémentaire est que le nombre de processus maximal pouvant s'effectué en même temps est fixé par le nombre de partitions.
\subsubsection{Dynamic Partitioning}
Quand un processus est amené dans la mémoire, on lui alloue exactement autant e méoire qu'il lui faut, pas plus. L'exemple \cite[p.~314]{stallings} montre bien que la mémoire contient de plus en plus de petits trous, et l'utilisation de la mémoire se détériore. Cela s'appelle l'external fragmentation. Une technique utilisée pour éviter l'external fragmentation est la compaction. C'est à dire que de temps en temps, l'OS déplace tous les processus côtes à côtes pour que tout l'espace libre soit continu. Le problème est que ca prend du temps et que ca gaspille l'utilisation du processeur.
L'algorithme de placement doit être très intelligent, car comme la compaction prend du temps, il doit bien choisir où mettre ses trous dans la mémoire. Il y a trois algorithmes: Best-fit (prend le block le plus proche en taille de la requête), first-fit (prend le premier qui convient depuis le début de la mémoire) et next-fit (prend le prochain bloc qui convient à partir du dernier placé). Etonnement, first-fit est le meilleur, next-fit a tendance à trop fragmenter la fin de la mémoire et best-fit produit des trous si petits qu'il faut constemment faire de la compaction.
Il y a un moment où on est obligé de swapper deux processus, il faut donc un algorithme de remplacement, mais cela est vu plus loin dans le livre.
\subsubsection{Buddy System}
C'est un compromis. Les blocs sont de taille 2L jusque 2U . Voir explication \cite[p.~317]{stallings}. C'est un bon compromis qui dépasse les désavantages des deux techniques vues auparavent.
\subsubsection{Relocation}
On fait la disctinction entre plusieurs types d'adresses: \\
• Logical address: Référence à un endroit de la mémoire indépendant de sa position exacte. une traduction est nécessaire pour pouvoir accéder à son adress physique \\
• Relative address: l'adresse est exprimée par rapport à un point connu. \\
• Physical address: L'emplacement actuel de la valeur dans la main memory. Voir \cite[p.~7]{stallings}.8 pour comprendre le mécanisme de traduction des adresses.
\subsection{Paging}
La mémoire est divisée en petits bouts de taille fixe (frames) et les processus aussi (pages). Il y a donc pas de fragmentation externe, et la fragmentation interne correpond à l'espace gaspillé dans la dernière page du processus. Il faut maintenir une table des pages pour chaque processus, pour savoir où les petits bouts sont mis. En plus, si on se met d'accord et qu'on pose la taille des pages comme étant une puissance de 2, alors l'adresse relative et l'adresse logique sont les même. Il y a deux avantages: \\
• L'adresse logique est transparente au programmeur, à l'assembleur, et au linker.  \\
• Il est facile d'implémenter une fonction hardware qui fait de la traduction d'adresse
dynamique.
\subsection{Segmentation}
Le programme est divisés en segments qui n'ont pas tous la même taille. Ca ressemble à de la dynamic partitioning, et ca souffre aussi de l'external fragmentation. Mais comme les bouts sont plus petit, l'effet est limité. Il n'y a pas de relation simple entre l'adress logiqu et physique dans ce cas-ci. Et il faut tenir une table des segments pour tous les processus.
\subsection{Security Issues}
\subsubsection{Buffer Overflow Attacks}
C'est super dangereux. Voir \cite[p.~327]{stallings} à 330
\subsubsection{Defending against Buffer Overflows}

\newpage

%------------------------------------------------------------------------------
%				Chapitre 8
%------------------------------------------------------------------------------

\section{Chapter 8 : Virtual Memory}

Quelques définitions sont données \cite[p.~341]{stallings}
\subsection{Hardware and Control Structures}
2 caractéristiques de paging et segmentation sont les clés de ce chapitre: \\
• Toutes les références à la mémoire dans un processus sont des adresses logiques qui sont traduites dynamiquement.\\
• Un processus peut être divisé en un nombre de pièrces (segment ou pages) qui n'ont pas besoin d'être côte à côte dans la main memory pendant l'exécution.\\
Un portion de processus qui est actuellement dans la main memory est appelé un resident set du processus. Si le processeur rencontre une adresse logique qui n'est pas dans la main memory, il génère une memory access fault interruption. L'OS met le processus interrompu dans un blocking state. L'OS lance alors une disk I/O request pour pouvoir amener le bloc de mémoire la pièce du processus qui possède l'adresse logique susmentionnée. Après, l'OS dispatch un autre processus pendant que la disk I/O est en cours. Après que le bloc soit amené dans la main memory, une I/O interrupt est lancée, ce qui rend le contrôle à l'OS, lequel place le processus bloqué dans l'état ready.
Cette facon de faire amène 2 considérations:\\
• Plus de processus peuvent être présents dans la main memory \\
• Un processus peut être plus grand que la main memory\\
La table à la page 343 reprend toutes les caractéristiques des (VM) paging et segmentation.
\subsubsection{Locality and VM}
Il faut faire gaffe à ne pas passe plus de temps à swapper des pièces plutot que d'exécuter des instructions, ce qui s'appelle le trashing. Il existe des algorithmes pour ca, qui se basent sur le principe de localité qui, pour rappel, établit que le programme et les références aux données d'un processus ont tendance à se regrouper. Pour que la VM soit pratique et efficace, nous avons besoin de 2 ingrédients:\\
• Il doit y avoir un support hardware pour employer le schéma de segmentation et/ou paging\\
• L'OS doit inclure une série de programme pour gérer le mouvement des pages de la main memory à la secondary memory.\\
\subsubsection{Paging}
Explication de la structure d'une page table, et d'une page table entry ainsi que le mécanisme hardware de traduction d'adresses. Explique ensuite les page tables à plusieurs niveaux de hiérarchie. Explication des Inverted Page Table qui utilisent une fonction de hashage pour avoir une table plus petite.
Translation Lookaside Buffer: Cache spéciale à haute-vitesse pour les page table entries. C'est une cache qui contient les PTE qui ont été les récemment plus utilisées. Voir shéma d'exécution \cite[p.~351]{stallings}. Il a été prouvé que le TLB augmentait fortement les performances. Un schéma complet est fourni \cite[p.~352-353]{stallings}. Voir la discussion très intérressante sur la taille des pages. \cite[p.~354-355]{stallings}.
\subsubsection{Segmentation}
Les avantages sont les suivants:\\
• Cela simplifie la prise en main de structures de données croissantes\\
• Cela autorie les programmes à être modifiés et recompilés indépendamment.\\
• Cela pousse au partage entre processus\\
• Cela pousse à la protection.
\subsubsection{Combined Paging and Segmentation}
Paging, est transparent au programmeur, élimine la fragmentation externe et fournit une utilisation efficace de la main memory.
Segmentation est visible du programmeur, peut gérer les structures de données croissantes, est modulaire et supporte le partage et la protection.
Certains systèmes trop cool supportent les deux techniques.
\subsubsection{Protection and Sharing}
Explique en quoi la segmentation et le pagin fournissent une bonne base pour le partage et la protection.
\subsection{OS Software}
La conception de la partie de geston de la mémoire d'un OS dépend de 3 choix fondamentaux:\\
• Si oui ou non on utilise la technique de mémoire virtuelle\\
• Si on utilise le pagin, la segmentation ou les deux\\
• Quels algorithmes sont employés dans les différents aspect de la gestion de la mémoire.\\
La table à la page 361 montre les éléments clés du chapitre. Il est a souligné que dans tout le reste du chapitre, la préoccupation principale est la performance du système. On veut à tout prix limite le taux de page fault, parce que ca bouffe du temps.
\subsubsection{Fetch Policy}
La fetch policy détermine quand une page devrait être amenée dans la mémoire. Il y a deux alternatives:\\
• Demand Paging: Une page est amenée dans la méoire seulement quand une référence est faite vers un endroit de cette page. Quand un processus est lancé, il y a une pléade de de page faults. Mais avec le principe de localité, le nombre de page fault va décroitre vers un niveau très bas.\\
• Prepaging: Amène dans la mémoire d'autres pages que celle demandée. Tient compte du mouvement rotationnel et de la latence du disque de mémoire secondaire.\\
\subsubsection{Placement Policy}
La placement policy détermine où dans la real memory un morceau de processus doit être placé. Dans un système pure-segmentation, on a vu des algorithmes (best-fit,firstfit,...). Dans un système pure-pagin ou combiné, le placement est généralement inutile car la mécanisme machine de traduction et d'accès peuvent faire le mêmeboulot avec autant d'effiacité. Ca devient intérressant avec des multiprocesseurs NUMA.
\subsubsection{Replacement Policy}
Revient à selectionner choisir une page dans la main memory à virer pour faire rentrer une nouvelle page.
• Basic Algorithms\\
– Optimal\\
– Least recently used (LRU)\\
– First-in-first-out (FIFO) \\
– Clock\\
• Page Buffering
\subsubsection{Resident Set Management Cleaning Policy}
\subsubsection{Cleaning Policy}
\subsubsection{Load Control}
\subsection{UNIX and Solaris Memory Management}
\subsubsection{Paging System}
\subsubsection{Kernel Memory Allocation}
\subsection{Linux Memory Management}
\subsubsection{Linux VM}
\subsubsection{Kernel Memory Allocation}
\subsection{Windows Memory Management}
\subsubsection{Windows Virtual Adress Map }
\subsubsection{Windows Paging}


\newpage
\part{Scheduling}

%------------------------------------------------------------------------------
%				Chapitre 9
%------------------------------------------------------------------------------

\section{Chapter 9 : Uniprocessor Scheduling}

\subsection{Types of Processor Scheduling}
Short, long et medium scheduling vont être des outils pour déterminer quels processus vont attendre et quels processus vont s'exécuter. Fondamentalement, le schéduling est la manière d'organiser les queues pour minimiser les temps d'attentes et d'optimiser les performances dans un environnement de queues d'attente.
\subsubsection{Long-Term Scheduling}
Détermine quels programmes dont admis par le système pour s'exécuter. Il controle le degré de multiprogramming. Il doit prendre deux décisions: \\
• Quand l'OS doit amener un ou plusieurs processus, ce qui est dirigé par le degré d multiprogramming désiré.\\
• Quels processus doit-on amener ensuite. Cela peut se faire sur la base de FirstCome-First-Served (FCFS), ou avec un outils système.
\subsubsection{Medium-Term Scheduling}
Fait partie de la fonction de swapping.
\subsubsection{Short-Term Scheduling}
C'est le dispatcher. C'est le plus fréquemment utilisé des 3 dispositifs, c'est lui qui prend la fine décision de quel processus exécuter ensuite.
\subsection{Scheduling Algorithms }
\subsubsection{Short-Term Scheduling Criteria}
Le but est d'optimiser un ou plusieurs aspects du comportement du système, il y a plusieurs sortes de critères (voir table \cite[p.~401]{stallings}):\\
• User Oriented, Performance Related\\
– Turnaround time \\
– Response time\\
– Deadlines\\
• User Oriented, Other \\
– Predictability \\
• System Oriented, Performance Related \\
– Throughput\\
– Processor utilization \\
• System Oriented, Other\\
– Fairness\\
– Enforcing priorities \\
– Balancing resources\\
Les critères user-oriented sont plus importants et ont plus de conséquences que les system-oriented.
\subsubsection{The Use of Priorities}
Le scheduler choisir toujours un processus de plus haute priorité. Il va donc voir dans les queues de priorité en commancant par la plus haute. Le risque est que les files de basses priorités peuvent souffrir de starvation.
\subsubsection{Alternative Scheduling Policies}
Voir la table 9.3 pour les différentes fonctions de sélection de processus :\\
• First-Come-First-Served (FCFS): Fonctionne mieux pour les gros processus. A tendance à préférer les proccess-bound processes plutot que les I/O-bound processes. C'est pas le plus attractif, mais combiné à un principe de priorités, il peut faire un bon scheduler.\\
• Round Robin: Pour éviter les désavantages de FCFS, on utilise le même principe additionné à un principe de préemption basé sur une clock. (Time slicing). La difficulté est de choisir la longueur de la time slice. C'est plutot efficace pour les time-sharing ou transaction processing system, mais cela peut mal fonctionner avec des I/O bound processes. On parle aussi dans le livre de Virtual Round Robin (VRR), qui introduit une auxilliary queue, ce que augmente le fairness :)\\
• Shortest Process Next (SPN): Exécute en premier les processus qui on le temps d'exécution estimé le plus court. Les grands processus sont désavantagés et cella réduit la prédictability. Il faut estimer le temps d'exécution d'un programme et cela se fait à partir de statistiques (calcul exponentiel de moyenne) voir \cite[p.~410-411]{stallings}. Risque de starvtion. Pas adapté pour time-sharing et transaction processing car non preemptif.\\
• Shortest Remaining Time (SRT): Version préemptive de SPN. Prend celui qui à le temps restant d'exécution le plus court. Risque de starvation. Peut avoir de grns overhead.\\
• Highest Response Ration Next (HRRN): C'est un bon équilibre entre processus courts et longs.\\
• Feedback: Si on a aucune idée de la longueur des processus, SPN, SRT, HRRN sont inutilisables. On se focalise alors sur le temps écoulé et non le temps restant. Chaque fois qu'un processus est preempté, il perd un degré de priorité. Peut souffrir de starvation. Chaque niveau de priorité à un time slice double.\\
Voir la figure 9.5 \cite[p.~405]{stallings} qui illustre toutes ces différents comportements.
\subsubsection{Performance Comparison}
Queuing analysis: Voir les graphes. Les test ont été fait suivant une distribution de poisson. Les résultats confirment bien la théorie avancée plus avant. Simulation modeling: Processus groupés par percentiles. Turnaround time: FCFS très mauvais, surtout pour les petits processus. Voir analyse \cite[p.~420]{stallings}
\subsubsection{Fair-Share Scheduling}
En gros, si un certains nombre d'utilisateurs font partie d'un groupe, on voudrait que les décisions de scheduling soient prisent à l'intérieur de ce groupe. C'est-à-dire que les décisions de scheduling devraient être prises pour que chaque groupe recoivent un service similaire. En UNIX, il existe le Fair-Share Scheduler (FSS). Chaque groupe recoit un système virtuel qui fonctionne proportionellement plus lentement qu'un système complet.
\subsection{Traditional UNIX Scheduling}
A lire, plutot intéressant. \cite[p.~423]{stallings}.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 10
%------------------------------------------------------------------------------

\section{Chapter 10 : Multiprocessor and Real-Time Scheduling}


\subsection{Multiprocessor Scheduling}
On peut classer les systèmes multiprocesseurs dans 3 catégories: \\
• Loosely coupled or distributed multiprocessor or cluster: Vus au chapitre 16 \\
• Functionally specialized processors: ex I/O processors → Chapitre 11 \\
• Tightly coupled multiprocessor: Ensemble de processeurs qui partagent une main memory commune et qui sont sous le controle intégré d'un OS. But de ce chapitre.
\subsubsection{Granularity}
Un manière de caractériser les multiprocesseurs est de les placer dans le contexte de granularité de synchronisation, ou fréquence de synchronisation entre les processus du système. \\
• Independant parallelism: Pas de synchronisation explicite entre processus (ex:time sharing systems). \\
• Coarse and very-coarse-grained parallelism: Synchronisation entre processus, mais à un niveau très brut. \\
• Medium-grainded parallelism: Ex: Processus qui contient un ensemble de threads.\\
• Fine-grained parallelism: Très complexe, c'est une zone très spécialisée et frangmentée (ex: Source engine de Valve)
\subsubsection{Design Issues}
Le scheduling sur multiprocesseurs apporte 3 problèmes interdépendants (\cite[p.~434-435]{stallings}):\\
• L'affectation de processus aux processeurs: On voit deux approches, le master/slave architecture et la peer architecture\\
• L'utilisation de multiprogramming sur des processeurs individuels\\
• Le dispatching reel d'un processus
\subsubsection{rocess Scheduling}
Un étude à montré que contrairement aux systèmes uniprocesseur, les systèmes multiprocesseurs s'en sortent très bien avec une discipline de scheduling FCFS.
\subsubsection{Thread Scheduling}
La puissance des thread se manifeste véritablement dans les systèmes multiprocesseurs, car ils peuvent pleinement exploiter le parallélisme des applications. On voit arriver 4 approches (\cite[p.~437-442]{stallings}):\\
• Load sharing: Processus non assigné à un processeur particulier. \\
• Gang scheduling\\
• Dedicated processor assignment\\
• Dynamic scheduling
\subsection{Real-Time Scheduling}
\subsubsection{Background}
\subsubsection{Characteristics of Real-Time OS}
\subsubsection{Real-Time Scheduling}
\subsubsection{Deadline Scheduling}
\subsubsection{Rate Monotonic Scheduling}
\subsubsection{Priority Inversion}
\subsection{Linux Scheduling}
\subsection{UNIX SVR4 Scheduling}
\subsection{UNIX FreeBSD Scheduling}
\subsection{Windows Scheduling}
\subsection{Linux Virtual Machine Process Scheduling}


\newpage
\part{Input/Output and Files}


%------------------------------------------------------------------------------
%				Chapitre 11
%------------------------------------------------------------------------------

\section{Chapter 11 : I/O Management and Disk Scheduling}

\subsection{I/O Devices}
Il y a trois type d'appareils externes qui ont des relations I/O avec l'ordinateur: \\
• Human readable : terminal, imprimante, écran,... \\
• Machine readable : Clé USB, senseurs, ... \\
• Communication: modems, ... \\
Il y a encore de grandes différences entre les entités de chaques classes: \\
• Data rate \\
• Application \\
• Complexity of Control \\
• Unit of transfer \\
• Data representation  \\
• Error conditions \\

\subsection{Organization of the I/O Function}
Il y a trois techniques pour paratiquer du I/O \\
• Programmed I/O \\
• Interrupt-driven I/O \\
• Direct Memory Adress DMA \\

\subsubsection{The Evolution of the I/O Function}
Explique les différentes étapes par lesquels dont passés les fonctions I/O durant les ages. Apparition des I/O channel et I/O processor.

\subsubsection{Direct Memory Adress}
Quand le processeur veut faire un read ou un write, il envoie une requête au DMA avec les infos suivantes, chacune envoyées sur une ligne de communication: \\
• Si il veut un read ou un write \\
• L'adresse du dispositif I/O visé \\
• L'adresse de départ du read ou write \\
• Le nombre de mot à lire ou écrire \\
Il y a plusieurs structures de DMA possibles. Si tous les modules ont le même system bus. Il y a programmed I/O à l'intérieur du DMA. C'est inefficace. On peut faire de deux façon différentes voir \cite[p.~500]{stallings}.

\subsection{OS Design Issues}

\subsubsection{Design Objectives}
Il y a deux objectifs dans la conception I/O: L'efficacité et la généralité.

\subsubsection{Logical Structure of the I/O Function}
Explique que les interractions I/O peuvent être hiérarchisées en couches.
3 cas de figures sont envisagés: Logical peripheral device, communication port et file system.
Les différentes couches sont énoncées également.

\subsection{I/0 Buffering}
Pour éviter les deadlock et la perte d'efficacité lors d'une opération I/O, on utilise une technique appelée buffering.
Attention à bien faire la différence entre les deux types d'appareils I/O: Les block-oriented devices et les stream-oriented devices.

\subsubsection{Single Buffer}
Une zone de la main memory est associée au transfert I/O.
Dès que le block est remplit, il est déplacé en user space et ainsi de suite (Read ahead).
Cool car on peut swapper un processus dehors et en plus on peut lire un bloc pendant que le processus en traite un autre.
Mais ca complexifie la logique de l'OS.

\subsubsection{Double Buffer}
Le processeur remplit un buffer pendant que l'autre est lu, ou inversément.
Ça augmente encore les performances mais aussi la complexité.
C'est efficace que pour les line-at-atime I/O, parce que pour les byte-at-a-time, c'est des intérraction consumer/producer.


\subsubsection{Circular Buffer}
Si le processeur est rapide pour remplir les buffers, on peut préférer le circular buffer.
Simplement un modèle bounded buffer producer/consumer.
\subsubsection{The Utility of the Buffering}
Même avec de multiples buffers, il se peut qu'ils soit tous remplis et que les processus devront attendre qu'il se vide.
Néanmoins, dans un envirennement multiprogrammé, le buffering peut augmenter les performances.
\subsection{Disk Scheduling}
Explique que la vitesse de lecture de disque a augmenté beaucoup plus lentement que la vitesse des processus,...
Important d'améliorer ce truc
\subsubsection{Disk Performance Parameters}
Explique les différents facteurs qui influent sur le temps nécessaire pour réaliser une opération I/O sur un disque.
Cela apporte des temps d'attente tels que: Seek time, rotational time, transfer time, ...
On explique avec un exemple que la sequential organization des données sur le disque a des répercutions très importantes sur la durée des opérations I/O sur le disque.
\subsubsection{Disk Scheduling Policies}
\begin{itemize}
  \item First-In-First-Out: Si tous les accès sont dans le même secteur et qu'il a peu de processus, c'est un bon algorithme, mais c'est pas souvent le cas.
  \item PRI : Bon si beaucoup de processus légers.
    Mauvais pour les systèmes database.
  \item Shortest Service Time First (SSTF): Prend toujours le plus proche.
    Pas dit que c'est le meilleurs, mais c'est un bon algo
  \item SCAN pour éviter la starvation que peut apporter SSTF.
    Prend tout dans une direction, puis change de sens, etc...
    Similaire a SSTF.
  \item Circular-SCAN (C-SCAN) : Tout dans une direction, puis retourne à l'extrémité opposée et recommence à scanner.
    Diminue le temps d'attente pour les nouvelles requetes.
  \item N-STEP-SCAN et FSCAN: Utilise plusieurs queues pour éviter que le bras ne soit immobile trop longtemps.
\end{itemize}

\subsection{RAID}
Standard de design de base de données multi-disques.
Redundant array of independant disks (RAID).
Consiste en 7 niveaux d'architecture.
\subsection{Disk Cache}
Utilisée dans le même ordre d'idée que pour les processus.
\subsubsection{Design Considerations}
Quel algorithme de remplacement adopter.
Il semble que le LFU est meilleur que le LRU, mais il y a qques problèmes.
En effet, le phénomène de localiser peut biaiser le jugement de l'algorithme, et le pousser à prendre de mauvaises décisions.
Le même algorithme est proposé avec une queue un peu différente, voir \cite[p.~524]{stallings}
\subsubsection{Performance Considerations}
Le Frequency-Based Replacement est supérieur au LRU
\subsection{UNIX SVR4 I/O }
\subsection{Linux I/O}
\subsection{Windows I/O}

\newpage

%------------------------------------------------------------------------------
%				Chapitre 12
%------------------------------------------------------------------------------

\section{Chapter 12 : File Management}

\subsection{Overview}
\subsubsection{Files and File Systems}
Le file system permet aux user de créer des données qui ont pour propriété:
\begin{itemize}
  \item Long-term existence
  \item Sharable between processes
  \item Structure
\end{itemize}

\subsubsection{File Structure}
Quand on parle de file, 4 termes arrivent souvent: field, record, file et database. Les opérations courantes sont reprises \cite[p.~523]{stallings}.
Attention, par exemple, un programme C n'a pas de champ, record, etc...

\subsubsection{File Management Systems}
Donne les objectifs et exigences nécessaires pour avoir un bon file management system. Les fonctions du file system sont expliquée sur un très joli diagramme \cite[p.~526]{stallings}.

\subsection{File Organization ans Access }
\subsubsection{The Pile}
Les données sont collectées dans l'ordre dans lequel elles arrivent. Facile, mais pas super pratique dans la plupart des cas.

\subsubsection{The Sequential File}
Tous les records sont de la même longueur. 70

\subsubsection{The Indexed Sequential File}
On ajoute un index et un overflow file. Chaque record dans l'index file contient 2 champs: Le key field et un pointer vers le main file.

\subsubsection{The Indexed File}

\subsubsection{The Direct or Hashed File}

\subsection{B-Trees}
Les index, c'est cool, mais pour des trucs encore plus gros, on va utiliser un index sous forme de B-TREE.

\subsection{File Directories}

\subsubsection{Contents}

\subsubsection{Structure}

\subsubsection{Naming}

\subsection{File Sharing}

\subsection{Access }

\subsubsection{Rights }

\subsubsection{Simultaneous}

\subsubsection{Access}

\subsection{Record Blocking}

\subsection{Secondary Storage Management}

\subsubsection{File Allocation}

\subsubsection{Free Space Management }

\subsubsection{Volumes}

\subsubsection{Reliability}

\subsection{File System Security UNIX File Management}

\subsection{Linux Virtual File System }
\subsection{Windows File System}

\biblio
\end{document}
