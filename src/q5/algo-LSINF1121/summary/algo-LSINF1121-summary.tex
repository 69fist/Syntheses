\documentclass[en]{../../../eplsummary}

\usepackage{../../../eplcommon}
\usepackage{../../../eplcode}
\usepackage{wrapfig}
\lstset{language=Java, basicstyle=\rm\small\ttfamily, tabsize=2}

\hypertitle[']{Algorithms and data structures}{5}{SINF}{1121}
{Antoine Paris}
{Pierre Schauss}

% TODO
% - translate everything to english
% - improve a lot section on sorting

\section{Piles, files et listes chaînées}
\subsection{Listes chaînées}
\paragraph{Définition} Une \textit{liste chaînée}
est une structure de données récursive qui est soit
vide (\textit{null}) soit une référence vers un
noeud contenant une donnée générique et une référence
vers une liste chaînée.

Pour définir un noeud, on utilise une classe imbriquée :

\begin{lstlisting}
private class Node {
	private Item item;
	private Node next;
}
\end{lstlisting}

Une liste chaînée permet, si on possède un lien vers
le premier et le dernier élément de réaliser les
opérations suivantes en un temps indépendant de la
taille de la liste :

\begin{itemize}
	\item Insérer un élément au début ;
	\item Supprimer un élément au début ;
	\item Insérer un élément à la fin.
\end{itemize}

Pour pouvoir effectuer des insertions/suppressions
arbitraires efficacement, il faut utiliser des listes
doublement chaînées (chaque noeud possède deux liens,
un dans chaque direction).

Les implémentations de \lstinline{Stack}, \lstinline{Queue}
et \lstinline{Bag} reposent sur les listes chaînées. Grâce
à ces dernières, l'espace mémoire requis est proportionnel
au nombre d'items dans la collection et le temps requis
par opération est toujours indépendant de la taille de la
collection (ce ne serait pas le cas en utilisant des tableaux
par exemple).

\subsection{Stack}
Le principe de fonctionnement d'une stack est illustré à la figure
\ref{fig:stack}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=1.0]{img/stack.png}
	\caption{Illustration du fonctionnement d'une Stack LIFO.}
	\label{fig:stack}
\end{figure}

\lstinputlisting{code/Stack.java}

\subsection{Queue}
Le principe de fonctionnement d'une queue est illustré à la figure
\ref{fig:queue}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=1.0]{img/queue.png}
	\caption{Illustration du fonctionnement d'une Queue FIFO.}
	\label{fig:queue}
\end{figure}

\lstinputlisting{code/Queue.java}

\subsection{Bag}
Un bag est simplement une stack sans méthode \lstinline{pop()}.

\section{Algorithmes de tri}
Le but du jeu ici est de trier des tableaux d'items contenant
chacun une clé en suivant un ordre bien particulier (le plus
souvent numérique ou alphabétique). La notion de clé est inclue
dans un mécanisme propre à Java, l'interface \lstinline{Comparable}
(et notamment la méthode \lstinline{compareTo}).

On distingue deux types d'algorithmes de tri, les algorithmes
\textbf{en place} qui n'utilisent pas (ou quasi pas) de mémoire
supplémentaire et ceux qui nécessitent assez de mémoire
supplémentaire pour garder une copie du tableau à trier en
mémoire.

Dans les algorthimes de tri présentés dans cette section,
on utilise le template suivant.

\lstinputlisting{code/SortTemplate.java}

\subsection{Selection Sort}
\subsubsection{Principe}
Le tri par sélection est un des plus simple. Il fonctionne
comme suit : premièrement, trouver l'élément le plus petit
du tableau et l'échanger avec le premier élément, ensuite
trouver le deuxième élément le plus petit du tableau et
l'échanger avec le deuxième élément, et ainsi de suite.
La traduction en Java est immédiate.

\subsubsection{Implémentation}
\lstinputlisting{code/Selection.java}

\subsubsection{Propriétés}
\paragraph{Le temps d'éxécution est indépendant de l'entrée}
L'algorithme mettra autant de temps à trier à tableau déjà
trié qu'un tableau aléatoirement ordonné.

\paragraph{Mouvement minimal}
Le nombre d'échange est une fonction linéaire de la taille
du tableau à trier ($N$ échanges sont effectués). 

\subsubsection{Complexité}
Dans tous les cas $\frac{N^2}{2}$ comparaisons et $N$ échanges.

\subsection{Insertion Sort}
\subsubsection{Principe}
C'est l'algorithme qu'on utilise en général pour trier
ses cartes. On considère une carte à la fois, et on
l'insère à sa place parmis les cartes déjà triées. La
seule différence est qu'ici il faut faire de la place
pour insérer l'item considéré à sa position, on doit
donc décaler tout les items plus grand que lui d'une
position vers la droite.

\subsubsection{Implémentation}
\lstinputlisting{code/Insertion.java}

\subsubsection{Propriétés}
Contrairement au tri par sélection, le temps d'éxécution
du tri par insertion est dépendant de l'entrée, il est par
exemple \textbf{excellent pour des tableaux presque triés}.

C'est également une méthode correcte pour de très petits
tableaux.

\subsubsection{Complexité}
Dans le pire cas, $\frac{N^2}{2}$ comparaisons et
$\frac{N^2}{2}$ échanges.

\subsection{Shell Sort}
\subsubsection{Principe}
Le shellsort est juste une simple extension du tri
par insertion. Dans le tri par insertion, les échanges
n'impliquent que des éléments adjacents, les items ne
peuvent donc se déplacer que d'un élément à la fois.
Le shellsort gagne en rapidité en autorisant des échanges
d'éléments éloignés dans le tableau.

\paragraph{Tableau $h$-trié} un tableau $h$-trié ($h$-sorted
array) est un tableau dont tous les éléments séparés
de $h$ positions sont triés.

\subsubsection{Implémentation}
Concrètement, l'algorithme s'implémente en utilisant le
tri par insertion pour obtenir successivement des 
tableaux $h$-triés avec $h$ décroissant.

\lstinputlisting{code/Shell.java}

\subsubsection{Propriétés}
Lorsqu'un tableau $h$-trié est $k$-trié, il reste
$h$-trié.

\subsubsection{Complexité}
Un peu moins quadratique, $N^{\frac{4}{3}}$, $N^{\frac{5}{4}}$
ou $N^{\frac{6}{5}}$ dans le pire cas.

\subsection{Mergesort}
Le principe du tri par fusion est assez simple : pour trier
un tableau, diviser le en deux moitiés, trier chaque moitié
(récursivement) et ensuite fusionner les résultats.

La complexité des algorithmes de tri par fusion est
logarithmique en $N\log N$. Le tri par fusion est
un algorithme de tri basé sur des comparaisons
asymptotiquement optimal (on ne peut pas faire mieux).

Les deux implémentations du tri par fusion vues ici
se base sur la méthode \lstinline{merge} suivante :

\lstinputlisting{code/mergeMethod.java}

\subsubsection{Top-down mergesort}
Ici, on divise le problème en sous-problèmes et on les résout
récursivement. 

\lstinputlisting{code/Merge.java}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/mergesortTD.png}
	\caption{Trace des résultats de merge pour le top-down mergesort.}
\end{figure}

\subsubsection{Bottom-up mergesort}
Ici, on construit des petites solutions que l'on rassemble
pour en former des plus grandes à chaque fois. 

\lstinputlisting{code/MergeBU.java}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/mergesortBU.png}
	\caption{Trace des résultats de merge pour le bottom-up mergesort.}
\end{figure}

C'est la méthode à privilégier pour trier une liste chaînée.

\subsection{Quicksort}
\subsubsection{Principes}
Le tri rapide est probablement le plus utilisé, et ce pour
les deux raisons suivantes : il est en place et s'éxécute
en un temps proportionnel à $N \log N$.
Il fonctionne selon le principe du \textit{diviser pour
mieux régner}. Il partitionne le tableau en deux parties
et les trie ensuite indépendemment comme illustré sur
la figure \ref{fig:quicksort-overview}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{img/quicksort-overview.png}
	\caption{Quicksort overview.}
	\label{fig:quicksort-overview}
\end{figure}

L'algorithme de partionnement doit réarranger le tableau pour que
les 3 conditions suivantes soient vérifiées :

\begin{itemize}
	\item L'entrée \lstinline{a[j]} est à sa position finale pour
	un certain $j$ ;
	\item Les entrées \lstinline{a[lo..j-1]} sont inférieures ou
	égales à \lstinline{a[j]} ;
	\item Les entrées \lstinline{a[j+1..hi]} sont supérieures ou
	égales à \lstinline{a[j]}.
\end{itemize}

Ces 3 conditions sont résumées sur la figure \ref{fig:part-overview}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{img/partitioning-overview.png}
	\caption{Partioning overview.}
	\label{fig:part-overview}
\end{figure}

\subsubsection{Implémentation}
La trace du code suivant se trouve à la figure \ref{fig:part-trace}

\lstinputlisting{code/Quick.java}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/partitioning.png}
	\caption{Partioning trace.}
	\label{fig:part-trace}
\end{figure}

\subsubsection{Complexité}
Dans le pire cas, le tri rapide a une complexité quadratique.
Il est possible d'éviter ce pire cas en mélangeant aléatoirement
le tableau avant de le trier (cela fait du tri rapide un
algorithme \textit{randomisé}).

Dans le cas moyen, le tri rapide a une complexité en $\mathcal{O}(n\log(n))$.

\subsection{3-way quick sort}
\subsubsection{Principes}
Si le tableau à trier contient beaucoup d'éléments dupliqués (voir \textit{Dutch
National Flag problem}), il est possible d'atteindre une complexité
linéaire en modifiant un peu le tri rapide.

L'idée est d'utiliser 3 partitions plutôt que 2, comme indiqué sur
la figure \ref{fig:part3-overview}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{img/partitioning3-overview.png}
	\caption{3-way partioning overview.}
	\label{fig:part3-overview}
\end{figure}

Pour cela, une solution est d'effectuer un simple scan de la gauche
vers la droite dans le tableau en maintenant une variable \lstinline{lt}
tel que \lstinline{a[lo..lt-1]} sont inférieurs à $v$, une variable
\lstinline{gt} tel quel \lstinline{a[gt+1..hi]} sont plus grands que $v$,
une variable $i$ tel que \lstinline{a[lt..i-1]} sont égals à $v$ et
\lstinline{a[i..gt]} n'ont pas encore été examinés.

% TODO : add summary of complexity for sorting algorithms

\subsubsection{Implémentation}
La trace du code suivant se trouve à la figure \ref{fig:part3-trace}

\lstinputlisting{code/Quick3way.java}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{img/partitioning3.png}
	\caption{3-way partioning trace.}
	\label{fig:part3-trace}
\end{figure}

\section{Searching}
\subsection{Symbol tables}
\begin{mydef}
A \textit{symbol table} is a data structure for key-value pairs that
supports two operations: \textit{insert} (put) a new pair into the
table and \textit{search} for (get) the value associated with a given
key.
\end{mydef}

Here we adopt the \textit{associative array abstraction}, where you can
think of a symbol table as being just like an array where keys are
indices and values are array entries.

The API for a generic basic symbol table is given in figure \ref{fig:st-api}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/symbol-table-api.png}
	\caption{API for a generic basic symbol table.}
	\label{fig:st-api}
\end{figure}

For applications where keys are \lstinline{Comparable}, we usually talk about
\textit{ordered symbol tables}, where keys are kept in order.

The API for a generic ordered symbol table is given in figure \ref{fig:ost-api}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/ordered-symbol-table-api.png}
	\caption{API for a generic ordered symbol table.}
	\label{fig:ost-api}
\end{figure}

\subsubsection{Sequential search in an unordered linked list}
One straightforward option for the underlying data structure for a
symbol table is a linked list of nodes that contain keys and values.

Implementations of both \lstinline{get()} and \lstinline{put()} consist
of scanning through the list. Hence, it is easy to show that the worst
case is $\mathcal{O}(n)$ for both. In conclusion, a linked-list
implementation with sequential search is too slow to be used to solve
huge problems.

\subsubsection{Binary search in an ordered array}
\begin{lstlisting}
public class BinarySearchST<Key extends Comparable<Key>, Value>
{
\end{lstlisting}

Now, we consider a full implementation of our ordered symbol-table API.
The underlying data structure is a pair of parallel arrays, one for the
keys and one for the values.

\begin{lstlisting}
    public BinarySearchST(int capacity) { 
        keys = (Key[]) new Comparable[capacity]; 
        vals = (Value[]) new Object[capacity]; 
    }
    
    public int size() {
        return N;
    }
\end{lstlisting}

The heart of the implementation is the \lstinline{rank()} method, which
returns the number of keys smaller than a given key. For \lstinline{get()},
the rank tells us precisely where the key is to be found it is in the table.

\begin{lstlisting}
	public Value get(Key key) {
		if (isEmpty()) return null;
		int i = rank(key); 
		if (i < N && keys[i].compareTo(key) == 0) return vals[i];
    		return null;
	} 
\end{lstlisting}

For \lstinline{put()}, the rank tells us precisely where to update the value
when the key is in the table, and precisely where to put the key when the
key is not in the table. We move all larger keys one position to make room
and insert the given key and value into the proper positions in their
respective arrays.

\begin{lstlisting}
	public void put(Key key, Value val)  {
		int i = rank(key);

		if (i < N && keys[i].compareTo(key) == 0) {
			vals[i] = val; return;
		}

		for (int j = N; j > i; j--)  {
			keys[j] = keys[j-1]; vals[j] = vals[j-1];
		}
        
		keys[i] = key; vals[i] = val;
		N++;
	} 
\end{lstlisting}

To find the rank of a given key, we take advantage of the fact that keys
in the array are ordered and use \textit{binary search}. An iterative
implementation of the \lstinline{rank()} method is given below.

\begin{lstlisting}
	public int rank(Key key) {
		int lo = 0, hi = N-1;
		while(lo <= hi) {
			int mid = low + (hi - lo)/2;
			int cmp = key.compareTo(keys[mid]);
			if		(cmp < 0) hi = mid-1;
			else if 	(cmp > 0) hi = mid+1;
			else	 return mid; 		
		}
		
		return lo;	
	}
\end{lstlisting}

Here, it is easy to show that the complexity of binary search
is $\mathcal{O}(n\log(n))$. Unfortunately, insertion is still
to slow : $\mathcal{O}(n)$.

\subsection{Binary search tree}
Here we'll combine the flexibility of insertion in a linked-list
with the efficiency of search in an ordered array.

\begin{mydef}
A \textit{binary search tree} (BST) is a binary tree where each node
has a \lstinline{Comparable} key (and a associated value) and
satisfies the restriction that the key in any node is larger than
the keys in all nodes in that nodes' left subtree and smaller
than the keys in all nodes in that node's right subtree.
\end{mydef}

This restriction is illustrated in figure \ref{fig:bst-anatomy}.
Note that on a binary tree that is not a binary \textit{search}
tree, we don't have this restriction.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.65]{img/bst-anatomy.png}
	\caption{BST anatomy.}
	\label{fig:bst-anatomy}
\end{figure}

\begin{myrem}
It's interesting to note that there are many different BSTs that
represent the same set.
\end{myrem}

\subsubsection{Basic implementation}
\begin{lstlisting}
public class BST<Key extends Comparable<Key>, Value>
{
	private Node root;
\end{lstlisting}

Just as we did for linked lists, we define a private nested class
to define nodes in BSTs.

\begin{lstlisting}
	public class Node {
		private Key key;
		private Value val;
		private Node left, right;
		private int N;
		
		public Node(Key key, Value val, int N) {
			this.key = key; this.val = val; this.N = N;		
		}	
	}
\end{lstlisting}

Each node contains a key, a value, a left and right link and a node
count. This last field facilitates the implementation of various
ordered symbol-table operations.

\begin{lstlisting}
	public int size() {
		return size(root);	
	}
	
	private int size(Node x) {
		if (x == null) return 0;
		else return x.N;	
	}
\end{lstlisting}

Thanks to the recursive definition of a tree, we can implement
the \lstinline{get()} easily in a recursive way.

\begin{lstlisting}
	public Value get(Key key) {
		return get(root, key);	
	}
	
	private Value get(Node x, Key key) {
		if (x == null) return null;
		
		int cmp = key.compareTo(x.key);
		if      (cmp > 0) return get(x.right, key);
		else if (cmp < 0) return get(x.left, key);
		else	 return x.val;
	}
\end{lstlisting} 

The same holds for the \lstinline{put()} method.

\begin{lstlisting}
	public void put(Key key, Value val) {
		root = put(root, key, val);	
	}
	
	private Node put(Node x, Key key, Value val) {
		if (x == null) return new Node(key, val, 1);
		
		int cmp = key.compareTo(x.key);
		if      (cmp > 0) x.right = put(x.right, key, val);
		else if (cmp < 0) x.left = put(x.left, key, val);
		else x.val = val;
		x.N = size(x.left) + size(x.right) + 1;
		return x;
	}
\end{lstlisting}

To get a better understanding of the dynamics of these
recursive implementations, you can think of the code
\textit{before} the recursive calls as happening on the
way \textit{down} the tree. Then, think of the code \textit{after}
the recursive calls as happeing on the way \textit{up} the tree.

Now, let's look at how to delete a node in a BST. This is the most
difficult operation to implement. We choose here to use the
\textit{Hibbard deletion}. With this method, we delete a node
$x$ by replacing it with its \textit{successor}. If $x$ has
a right child, its successor is the node with the smallest
key in its right subtree. This task is accomplished
in 4 steps :

\begin{itemize}
	\item Save a link to the node to be deleted in \lstinline{t} ;
	\item Set \lstinline{x} to point to its successor
	\lstinline{min(t.right)} ;
	\item Set the right link of \lstinline{x} to
	\lstinline{deleteMin(t.right)} ;
	\item Set the left link of \lstinline{x} to \lstinline{t.left}.
\end{itemize}

\begin{lstlisting}
	public void delete(Key key) {
		root = delete(root, key);	
	}
	
	private Node delete(Node x, Key key) {
		if (x == null) return null;
		int cmp = key.compareTo(x.key);
		if      (cmp < 0) x.left = delete(x.left, key);
		else if (cmp > 0) x.right = delete(x.right, key);
		else {
			if(x.right == null) return x.left;
			if(x.left == null) return x.right;
			/* Choose successor to replace x (Hibbard)
			if the node to delete has two children */
			Node t = x;
			x = min(t.right);
			x.right = deleteMin(t.right);
			x.left = t.left;		
		}	
		
		x.N = size(x.left) + size(x.right) +1;
		return x;
	}
\end{lstlisting}

Even though this method does the job, it has a flaw that might
cause performance problems in some practical situations. The problem
is that the choice of using the successor is arbitrary and not
symmetric. This will cause the BST to become skewed toward the
left on the long term for random deletions and insertions
\footnote{See the last video on 
\url{http://algs4.cs.princeton.edu/32bst/}.}.

\begin{myrem}
Note that deletion in a BST is not commutative. Try for
example to delete 4 and then 3 and 3 and then 4 in
the tree given in figure \ref{fig:nc-deletion}.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\node [circle,draw] {4}
		child {node [circle,draw] {3}}
		child {node [circle, draw] {7}
			child {node [circle,draw] {6}}
			child {node [] {}}};
	\end{tikzpicture}
	\caption{Example of non-commutative deletion.}
	\label{fig:nc-deletion}
\end{figure}
\end{myrem}

\begin{myrem}
Others methods from the API are in the book.
\end{myrem}

\subsubsection{Analysis}
The running times of algorithms on binary search trees depend
on the shapes of the trees, which in turn, depend on the
order in which keys are inserted. In the best case, a tree
with $N$ nodes could be perfectly balanced, with $\log(n)$
nodes between the root and each null links (we call this the
\textit{depth} of a tree). In the worst case
there could be $N$ nodes on the search patch.

% Better to say "proposition" than "property" ? Do I need to change
% eplcommon.sty to add this?
\begin{myprop}
Search hits in BST build from $N$ random keys requires $\sim 2\log N$
(about $1.39\log(n)$) compares, on the average.
\end{myprop}

\begin{myprop}
Insertions and search misses in a BST build from $N$ random
keys require $\sim 2\log N$ (about $1.39\log(n)$) compares on the
average.
\end{myprop}

\begin{myprop}
In a BST, all operations take time proportional to the height of
the tree in the worst case.
\end{myprop}

\subsubsection{Binary tree traversal methods}
We will use the BST given in figure \ref{fig:bst-anatomy}
for the following examples and assume that the method
\lstinline$void visit(Node h)$ print the key of node
\lstinline$h$.

\paragraph{Preorder (\textit{préfixe)}}
Visit a node, then its left child and right child.
\begin{lstlisting}
public static void preOrder(Node h) {
	if(t != null) {
		visit(t);
		preOrder(t.left);
		preOrder(t.right);	
	}
}
\end{lstlisting}
Applied on figure \ref{fig:bst-anatomy} :
S E A C R H X.

\paragraph{Inorder (\textit{infixe})}
This method is equivalent to projecting the keys
of the tree on a line at the bottom of the tree :
\textbf{keys appear in sorted order}.

\begin{lstlisting}
public static void inOrder(Node h) {
	if(t != null) {
		inOrder(t.left);
		visit(t);
		inOrder(t.right);	
	}
}
\end{lstlisting}
Applied on figure \ref{fig:bst-anatomy} :
A C E H R S X.

\paragraph{Postorder (\textit{postfixe} ou \textit{suffice})}
Visit each node after having visited each children.
\begin{lstlisting}
public static void postOrder(Node h) {
	if(t != null) {
		postOrder(t.left);
		postOrder(t.right);
		visit(t);
	}
}
\end{lstlisting}
Applied on figure \ref{fig:bst-anatomy} :
C A R H E X S.

\subsection{Balanced search tree}
The algorithms in the previous subsection have poor worst-case,
performance. Here we introduce a type of binary search tree where
costs are \textit{guaranteed} to be logarithmic. Ideally, we would
like to keep our binary search tree perfectly balanced, i.e. in an
$N$-node tree, we would like the height to be $\log(N)$. Unfortunately,
maintaining this property is too expensive. Hence we will consider
a data structure that slightly relaxes the perfect balance
requirement to provide guaranteed logarithmic performance.

\begin{mydef}[2-3 search trees]
A \textit{2-3 search tree} is a tree that is either empty or
\begin{itemize}
	\item A \textit{2-node}, with one key (and associated value)
	and two links, a left link to 2-3 search tree with smaller
	keys, and a right link to a 2-3 search tree with larger keys ;
	\item A \textit{3-node}, with two keys (and associated values)
	and \textit{three} links, a left link to 2-3 search tree with
	smaller keys, a middle link to a 2-3 search tree with keys
	between the node's key, and a right link to a 2-3 search tree
	with larger keys.
\end{itemize}
As usual, we refer to a link to an empty tree as a \textit{null link}.
\end{mydef}

\begin{myprop}
A \textit{perfectly balanced} 2-3 search tree is one whose null
links are all the same distance from the root.
\end{myprop}

Searching in a 2-3 search tree is almost the same as searching
in a BST. The only difference occurs when we encounter a 3-node,
instead of just looking if the target key is smaller or lager
than the keys, we also have to check if the target key is
between the two keys of the 3-node.

Inserting in a 2-3 search tree is more complicated. The primary reason
that 2-3 trees are useful is that we can do insertions and still
maintain perfect balance. To achieve this, we have to take into
account different cases illustrated below 
(see figure \ref{fig:23tree-inserta}, \ref{fig:23tree-insertb}
and \ref{fig:23tree-insertc}).

\begin{myprop}
Transformations caused by the 2-3 tree insertion algorithms
are purely \textit{local}: no part of the tree needs to be examined
or modified other than the specified nodes and links. Moreover, these
local transformations preserve the global properties that the tree
is ordered and perfectly balanced.
\end{myprop}

\begin{myprop}
Search and insert operations in a 2-3 tree with $N$ keys are
guaranteed to visit at most $\log N$ nodes. Indeed, the height
of an $N$-node 2-3 tree is between $\log_3 N$ (if the tree is
all 3-nodes) and $\log N$ (if the tree is all 2-nodes).
\end{myprop}

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
  		\includegraphics[width=.7\linewidth]{img/23tree-insert2.png}
 		\caption{Inserting into a 2-node.}
  		\label{fig:23tree-insert2}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=0.5\linewidth]{img/23tree-insert3a.png}
  		\caption{Inserting into a single 3-node}
  		\label{fig:23tree-insert3a}
	\end{subfigure}
	\caption{}
	\label{fig:23tree-inserta}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.55]{img/23tree-insert3b.png}
	\caption{}
	\label{fig:23tree-insertb}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
  		\includegraphics[width=.7\linewidth]{img/23tree-insert3c.png}
 		\caption{Insert intro a 3-node whose parent is a 3-node.}
  		\label{fig:23tree-insert3c}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=0.6\linewidth]{img/23tree-split.png}
  		\caption{Splitting the root.}
  		\label{fig:23tree-split}
	\end{subfigure}
	\caption{}
	\label{fig:23tree-insertc}
\end{figure}

\subsubsection{Implementation}
To implement a 2-3 tree, we consider a simple representation
known as \textit{red-black BST}. The basic idea behind red-black
BSTs is to encode 2-3 tree by starting with standard BSTs and adding
extra information to encode 3-nodes. We think of the links as being of
two different type: \textit{red} links, which bind together two 2-nodes
to represent a 3-nodes, and \textit{black} links, which bind together
the 2-3 tree. Specifically, we represent 3-nodes as two 2-nodes connected
by a single red link that leans left. This is illustrated in figure
\ref{fig:redblack-encoding}.

One advantage of using such a representation is that it allows us
to use our \lstinline$get()$ code for standard BST search without
modification.

\begin{mydef}[Equivalent definition]
A red-black BST is a BST having red and black links and satisfying
the following three restrictions :
\begin{itemize}
	\item Red links lean left ;
	\item No node has two red links connected to it ;
	\item The tree has \textit{perfect black balance}: every path from
	the root to a null link has the same number of black links - we refer
	to this number as the tree's \textit{black height}.
\end{itemize}
\end{mydef}

\begin{myprop}
There is a 1-1 correspondence between red-black BSTs and 2-3 trees.
\end{myprop}

This last property is illustrated in figure \ref{fig:redblack-1-1}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.5]{img/redblack-encoding.png}
		\caption{Encoding a 3-node with two 2-nodes connected by
		a left leaning red link.}
		\label{fig:redblack-encoding}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.5]{img/redblack-1-1.png}
		\caption{1-1 correspondence between red-black BSTs and 2-3 trees.}
		\label{fig:redblack-1-1}
	\end{subfigure}
	\caption{}
	\label{fig:redblack-principles}
\end{figure}

\begin{lstlisting}
public class RedBlackBST<Key extends Comparable<Key>, Value>
{
	private Node root;
\end{lstlisting}

As usual, we define a private nested class to define a node.

\begin{lstlisting}
	private static final boolean RED = true;
	private static final boolean BLACK = false;
	
	private Class Node {
		Key key; Value val;
		Node left, right;
		int N;
		boolean color;	
	
		Node(Key key, Value val, int N, boolean color) {
			this.key = key; this.val = val;
			this.N = N; this.color = color;		
		}	
	}
	
	private boolean isRed(Node x) {
		if(x == null) return false;
		return x.color == RED;	
	}
\end{lstlisting}

When we refer to the color of a node, we are referring
to the color of the link pointing to it. By convention, null
links are black. Now we consider two methods
\lstinline$Node rotateLeft(Node h)$ and \lstinline$Node rotateRight(Node h)$.
These two methods will help us to write other methods later. They
are illustrated in figure \ref{fig:redblack-left-rotate} and
\ref{fig:redblack-right-rotate}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.5]{img/redblack-left-rotate.png}
		\caption{Left rotate.}
		\label{fig:redblack-left-rotate}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.5]{img/redblack-right-rotate.png}
		\caption{Right rotate.}
		\label{fig:redblack-right-rotate}
	\end{subfigure}
	\caption{}
	\label{fig:redblack-rotations}
\end{figure}

\begin{lstlisting}
	private Node rotateLeft(Node h) {
		Node x = h.right;
		h.right = x.left; x.left = h;
		x.color = h.color; h.color = RED;
		x.N = h.N;
		h.N = 1 + size(h.left) + size(h.right);
		return x;
	}
	
	private Node rotateRight(Node h) {
		Node x = x.left;
		h.left = x.right; x.left = h;
		x.color = h.color; h.color = RED;
		x.N = h.N;
		h.N = 1 + size(h.left) + size(h.right);
		return x;
	}
\end{lstlisting}

Next we consider a simple method which allows us to
split a 4-node.

\begin{lstlisting}
	private void flipColors(Node h) {
		h.color = RED;
		h.left.color = BLACK;
		h.right.color = BLACK;	
	}
\end{lstlisting}

This method is illustrated in figure \ref{fig:color-flip}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.55]{img/color-flip.png}
	\caption{Flipping colors to split a 4-node.}
	\label{fig:color-flip}
\end{figure}

With the last three methods, we have everything we need to
implement insertion in red-black BSTs.

\begin{lstlisting}
	public void put(Key key, Value val) {
		root = put(root, key, val);
		root.color = BLACK;	
	}
	
	private Node put(Node h, Key key, Value val) {
		// Do standard insert, with red link to parent.
		if(h == null) return new Node(key, val, 1, RED);
		
		int cmp = key.compareTo(h.key);
		if      (cmp > 0) h.right = put(h.right, key, val);
		else if (cmp < 0) h.left = put(h.left, key, val);
		else h.val = val;
		
		if (isRed(h.left)  && isRed(h.left.left)) h = rotateRight(h);
		if (!isRed(h.left) && isRed(h.right))     h = rotateLeft(h);
		if (isRed(h.left)  && isRed(h.right))     flipColors(h);
		
		h.N = 1 + size(h.left) + size(h.right);
		return h:
	}	
\end{lstlisting}

A summary of rotations and colors flipping involved in insert is
given in figure \ref{fig:insert-summary}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.55]{img/insert-summary.png}
	\caption{Operations involved in insert.}
	\label{fig:insert-summary}
\end{figure}

% TODO : deletion in a red-bl	ack BST

\subsubsection{Analysis}
\begin{myprop}
The height\footnote{Here we are not talking about black height!} 
of a red-black BST with $N$ nodes
is no more than $2\log N$. Indeed, the worst case
is a 2-3 tree that is all 2-nodes except that the leftmost
is made up of 3-nodes. The path taking left links from the
root is twice as long as the path of length $\sim \log N$
that involves just 2-nodes.
\end{myprop}

The worst case of the last property corresponds to the first
tree of figure \ref{fig:redblack-1-1} with $S$ removed.

\begin{myprop}
The average length of a path from the root to a node
in a red-black BST with $N$ nodes is $\sim 1.00\log N$.
\end{myprop}

\begin{myprop}
In a red-black BST, the following operations take logarithmic time
in the worst case: search, insertion, finding the minimum/maximum,
floor, ceiling, rank, select, delete the minimum/maximum, delete
and range count.
\end{myprop}

\subsection{Hash tables}

If keys are small integers, we can use an array to implement
an unordered symbol table, by interpreting the key as an array
index so that we can store the value associated with the key
\lstinline$i$ in array entry \lstinline$i$, ready for
\textbf{immediate access}. Thanks to \textit{hashing}, we can
transform any type of key to array indexes. Ideally, different
keys would map to different indices. However, this ideal
is generally beyond our reach, so we have to face the possibility
that two or more different keys may hash to the same array index :
we need a \textit{collision resolution} process. We'll consider
two different approaches to collision resolution : \textit{separate
chaining} and \textit{linear probing}.

\subsubsection{Hash functions}

\begin{wrapfigure}{R}{0.3\textwidth}
  \begin{center}
    \includegraphics[width=0.15\textwidth]{img/modular-hashing.png}
  \end{center}
  \caption{Modular hashing.}
  \label{fig:modular-hashing}
\end{wrapfigure}

If we have an array that can hold $M$ key-value pairs, then we
need a hash function that can transform any given key into an
integer in the range $[0, M-1]$. We seek a hash function that both
is \textbf{efficient to compute} and \textbf{uniformly distributes}
the keys.

\paragraph{Hashing positive integers}
The most commonly used method for hashing integers is called
\textit{modular hashing}: we choose the array size $M$ to be prime
and, for any positive integer key $k$, compute the remainder
when dividing $k$ by $M$ : \lstinline$k % M$.

If $M$ is not prime, it may be the case that not all the bits
of the key play a role, which amounts to missing an opportunity
to disperse the values evenly. For example, if the keys are
base 10 numbers and $M$ is $10^k$, then only the $k$ least
significant digits are used (see figure \ref{fig:modular-hashing}).

\paragraph{Hashing floating-point numbers}
One way to do this is to use modular hashing on the binary
representation of the key (this is what Java does).

\paragraph{Hashing strings}
Again, we can apply modular hashing on each character of
the string :

\begin{lstlisting}
int hash = 0;
for(int i = 0; i < s.length(); i++)
	hash = (R * hash + s.charAt(i)) % M;
\end{lstlisting}

This algorithm is called the \textit{Horner's method}.
The use of a small primer integer such as 31 ensures
that the bits of all the characters play a role.

\paragraph{Java conventions}
In Java, every data type inherits a method called
\lstinline$hashCode()$ that returns a 32-bit integer.
The implementation of \lstinline$hashCode()$ for a data
type muste be \textbf{consistent with equals}, if
\lstinline$a.equals(b)$, then \lstinline$a.hashCode()$
must have the same numerical value as
\lstinline$b.hashCode()$. Conversely, if
\lstinline$hashCode()$ values
are different, then objects are different. \textbf{But},
if \lstinline$hashCode()$ values are the same, we still
have to use \lstinline$equals()$ to check whether or not
objects are equal (as collisions may occur).

\paragraph{Converting a hashcode to an array index}
Since our goal is an array index, not a 32-bit integer, we
combine \lstinline$hashCode()$ with modular hashing
to produce integers between 0 and $M-1$ :

\begin{lstlisting}
private int hash(Key x) {
	return (x.hashCode() && 0x04fffffff) % M);
}
\end{lstlisting}

This code also masks off the sign bit (to turn the
32-bit number into a 31-bit non-negative integer).
We usually use a prime number for the hash table
size $M$ to attempt to make use of all the bits of
the hash code.

\paragraph{Software caching}
If computing the hash code is expensive, it may be
worthwhile to \textit{cache the hash} for each key.
That is, we maintain an instance variable \lstinline$hash$
in the key type that contains the value of
\lstinline$hashCode()$ for each key object. Java uses
this technique for \lstinline$String$ objects.

\subsubsection{Hashing with separate chaining}

\subsection{Summary}
The cost-summary for symbol-tables implementations is given
in table \ref{tab:cost-summary-searching}.

% auto-generated from here http://www.tablesgenerator.com/ #flemme
\begin{table}[ht]
\centering
\begin{tabular}{c|ccccc}
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}algorithm \\(data structure)\end{tabular}} &
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}worst-case cost\\ (after N inserts)\end{tabular}} &
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}average-case cost\\ (after N random inserts)\end{tabular}} &
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}efficiently support\\ordered operations?\end{tabular}} \\
                                                                                      & search                                         & insert                                         & search hit                                          & insert                                             &                                                                                                      \\ \hline
\begin{tabular}[c]{@{}c@{}}sequential search\\ (unordered linked list)\end{tabular}   & $N$                                            & $N$                                            & $N/2$                                               & $N$                                                & no                                                                                                   \\
\begin{tabular}[c]{@{}c@{}}binary search\\ (ordered array)\end{tabular}               & $\log N$                                      & $N$                                            & $\log N$                                            & $N/2$                                              & yes                                                                                                  \\
\begin{tabular}[c]{@{}c@{}}binary tree search\\ (BST)\end{tabular}                    & $N$                                            & $N$                                            & $1.39 \log N$                                       & $1.39 \log N$                                      & yes                                                                                                  \\
\begin{tabular}[c]{@{}c@{}}2-3 tree search\\ (red-black BST)\end{tabular}             & $2 \log N$                                     & $2 \log N$                                     & $1.00 \log N$                                       & $1.00 \log N$                                      & yes                                                                                                 
\end{tabular}
\caption{Cost summary for symbol-table implementations.}
\label{tab:cost-summary-searching}
\end{table}

\end{document}
