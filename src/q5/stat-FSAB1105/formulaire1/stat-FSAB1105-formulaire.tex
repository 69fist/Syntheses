\documentclass[10pt,landscape,en]{../../../eplformulaire}
%\documentclass[10pt, landscape]{../../../eplsummary}
% \usepackage[utf8]{inputenc}
% \usepackage[french]{babel}
% \usepackage{amsmath,empheq}
% \usepackage{amsfonts}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage{hyperref}
% \usepackage{multicol}
% \usepackage{color}
% \usepackage{listings}

\usepackage[left=0.1cm,right=0.1cm,top=0.1cm,bottom=0.1cm]{geometry}
\usepackage{empheq}

\setlength{\columnseprule}{1pt}
\def\columnseprulecolor{\color{black}}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\hypertitle{Probabilit√© et Statistiques}{5}{FSAB}{1105}
{Charles Momin}
{Anour El Ghouch et Rainer von Sachs}

\begin{multicols*}{4}
\scriptsize
\textbf{\footnotesize Data set properties}

\begin{itemize}
\item{\textbf{Mean}: $\bar{X} = \dfrac{1}{n} \displaystyle{\sum_{i=1}^{n} X_i }$}
\item{\textbf{Variance}: \\ \textbullet $s^2 = \dfrac{1}{n-1} \displaystyle{\sum_{i=1}^n \left( X_i - \overline{X}\right) ^2}$ }
\item{\textbf{Standard deviation}: $s = \sqrt{s^2}$}
\item{\textbf{Quantile}: \\ for a sample of $n$ data \\ $q_p = X \left( 1 + (n-1)\cdot p\right)$}
\end{itemize}


\textbf{\footnotesize Probability properties}

\begin{itemize}
\item[\textbullet]{\textbf{$P(A) = 1 - P \left( \overline{A} \right)$}}
\item[\textbullet]{\textbf{$P \left( A\cap B\right) = P(A) \cdot P(B)$ (if indep.)}}
\item[\textbullet]{\textbf{$P(A) = P \left( A \cap B \right)+ P \left( A \cap \overline{B} \right) $}}
\item[\textbullet]{\textbf{$P(A\mid B) = \dfrac{P(A \cap B)}{P(B)}$}}
\item[\textbullet]{\textbf{$P(A\mid B) = P(A)$ (if indep.)} }
\item[\textbullet]{\textbf{$P(B\mid A) = \dfrac{P(A\mid B) \cdot P(B)}{P(A)}$}}
\end{itemize}


\textbf{\footnotesize Enumerative combinations}
\begin{itemize}
\item{\textbf{Permutation:} \\ for $n$ distincts objects = $n!$ }
\item{\textbf{Partial permutation:} \\ for $r$ objects selected without replacement. Order import\\ $A_n^r = P_n^r = \dfrac{n!}{(n-r)!}$ \\ if replacement possible permutations = $n^r$}
\item{\textbf{Combination:} \\  for $r$ objects without replacement and order \\ $C_n^r = \dfrac{n!}{r!(n-r)!}$}
\end{itemize}

\textbf{\footnotesize Random var. properties}

\begin{itemize}
\item{\textbf{Probability:} \\ $P(X\in I) = \displaystyle{\int_I f(x) ~\textrm{d}x}$}
\item{\textbf{Expectation}: \\for $p(x)$ a density function \\ \textbullet $E(X) = \mu _X = \displaystyle{\int_{-\infty}^{\infty}} x \cdot p(x) ~\textrm{d}x$ \\ \textbullet $E(X) = \mu _X = \displaystyle{\sum_{i=1}^{n} X_i \cdot  p\left( X_i \right)}$ \\ \\ \textit{Property:} \\ \textbullet $E(aX + b) = a \cdot E(X) + b$ \\ \textbullet $E \displaystyle{\left(  \sum_i X_i\right) = \sum_i E\left( X_i \right) }$ \\ \textbullet $E(XY)\neq E(X)E(Y)$}
\item{\textbf{Variance:} \\ $\sigma^2_X = E(X-\mu_X)^2 = E(X^2) - (E(X))^2$ \\ $SD=\sqrt{\sigma^2_X}$ \\ \textit{Property} \\ \textbullet $Var(a) =0$ \\ \textbullet $Var(a+bX)=b^2 Var(X)$}
\item{\textbf{Momentum:} \\ $m_X(t) = E(exp(tX))$ \\ (k-om) $\displaystyle{\dfrac{d^k m_X(t)}{dt^k}\mid_{t=0} = E(X^k)  }$}
\end{itemize}

\textbf{\footnotesize Discrete distributions}

\begin{itemize}
\item{\textbf{Uniform:} \\ \textbullet $P(X=x) = \dfrac{1}{n}$ \\ \textbullet $\mu_X = \dfrac{X_1 + ... + X_n}{n}$ \\ \textbullet $\sigma_X^2 = \dfrac{X_1^2 + ... + X_n^2}{n} - \mu_X^2$}
\item{\textbf{Bernouilli (nbr succ. in $n$ trials):} \\ if $X\sim Be(p)$ \\ \textbullet $P(X=x)=p^x(1-p)^{1-x}$ \\ \textbullet $\mu_X = p$ \\ \textbullet $\sigma_X^2 = pq = p(1-p)$ \\ if $X\sim Bi(n,p)$ \\ \textbullet $P(X=x)=C_n^xp^xq^{n-x}$ \\ \textbullet $\mu_X = np$ and $\sigma_X^2=npq$ \\ \textbullet $m_X(t)=(pe^t+q)^n$ \\ \textbullet $m'_X(t) = npe^t(pe^t+q)^{n-1}$ \\ \textbullet $npe^t(npe^t+q)(pe^t+q)^{n-2}$}
\item{\textbf{Geometric (nbr. rep. until $1^{st}$ succ.}}
\item{\textbf{Poisson (events occured in speficied boundaries):} $X\sim Po(\lambda)$ \\ \textbullet Homogen($\lambda u = cst$), Indep., Rare($\lim_{n\Rightarrow \infty} p =0$) \\ \textbullet if $n\gg$, $p\ll$ and $\lambda=np<7$ then $Bi(n,p)\approx Po(np)$ \\ \textbullet $P(X=x)=e^{-\lambda}\dfrac{\lambda^x}{x!}$ \\ \textbullet $Bi(n,\dfrac{\lambda}{n}) \Rightarrow Po(\lambda)$ (for $n \Rightarrow \infty$) \\ \textbullet $\mu_X=\lambda$ and $\sigma_X^2=\lambda$}
\end{itemize}


\textbf{Continuous distributions}
\begin{itemize}
\item{\textbf{Normal:} if $X\sim N(\mu, \sigma^2)$ \\ \textbullet $P(Z\geq a)=1-P(Z\geq -a)$}
\item{\textbf{Standard Normal:} $Z\sim N(0,1)$\\ \textbullet Normal with $\mu=0$ and $\sigma=1$ \\ for $Z\sim N(0,1)$ and $\alpha \in (0,1)$: \\ \textbullet $z_{\alpha} = q_{1-\alpha}\mid P(Z\geq z_{\alpha})=\alpha$ \\ \textbullet $z_{\alpha}=-z_{1-\alpha}$ \\ \textbullet if $X\sim N(\mu, \sigma^2)$ then $a+bX\sim N(a+b\mu, b^2\sigma^2)$ and : \\  if $X\sim N(\mu, \sigma^2)$ then $Z=\dfrac{X-\mu}{\sigma}\sim N(0,1)$ \\ \textbullet if $X \sim N(\mu, \sigma^2)$: \\  $P(X\leq x) = P(\dfrac{X-\mu}{\sigma} \leq \dfrac{x-\mu}{\sigma}) = P(Z\leq \dfrac{x-\mu}{\sigma})$ \\ \textbullet $\%\alpha$ of $N(\mu,\sigma^2) = \sigma z_{\alpha} + \mu$ \\ \textbullet $P(\mid X - \mu \mid \leq k\sigma) \geq 1 -\dfrac{1}{k^2}$ \\ in partucal if $k=4.47$ then $P(X \in \left[ \mu - k \sigma , \mu + k \sigma \right] ) \geq 0.95 \approx 1$}
\item{\textbf{Exponential:} \\ \textbullet Defining the lifetime of a event or the waiting time  for an occurence of a event}
\item{\textbf{Gamma:} \\ \textbullet $\Gamma (\alpha) = \displaystyle{\int_0^{\infty} t^{\alpha-1} e^{-t} dt}$ \\ \textbullet $\Gamma (\alpha+1)=\alpha \Gamma (\alpha)$ \\
\textbullet $\Gamma (1)=1$ \\ \textbullet $\Gamma (n+1)=n!$}
\end{itemize}

\textbf{\footnotesize Multivariable dist.}

\begin{itemize}
\item{\textbf{Joint distribution:} for the vector $(X,Y)$: \\ \textbullet p(x,y)=P(X=x, Y=y) \\ \textbullet for $A = \left[ a,b \right] \times \left[ c,d \right] \subset R^2$: \\ $P((X,Y)\in A) = \displaystyle{\sum_{(x,y) \in A} p(x,y)}$ \\ \textbullet for $f(x,y) \mid f\geq0$ and \\ $\displaystyle{\int_{\infty}^{\infty} \int_{\infty}^{\infty} f(x,y) dxdy =1}$, then: \\ $P((X,Y) \in A) = \displaystyle{\int \int _{A} f(x,y) dxdy}$ \\ \textbullet $F(x,y)=P(X\leq x, Y\leq y)$: \\ \textbf{(dis.)} $F(x,y)= \displaystyle{\sum_{s\leq x, t\leq y} p(s,t)}$ \\ \textbf{(cont.)} $F(x,y)=\displaystyle{\int_{\infty}^y \int_{\infty}^x f(s,t) dsdt}$ \\ \textbullet $f(s,t)= \dfrac{\partial ^2 F}{\partial x \partial y}$} 
\item{\textbf{Marginal}: \\ \textbullet if $X,Y$ have joint pdf: \\
$f_X(x) = P(X=x)=$ \\ (dis.) $\displaystyle{\sum_y P(X=x,Y=y)}$ \\ (con.) $\displaystyle{\int_{\infty}^{\infty} f(x,y)dy}$ \\ \textbullet if $X,Y$ have joint cdf: \\ $F_X(x) = P(X\leq x)= F(x, \infty)$}
\item{\textbf{Conditionnal}: if $P(X\in A)>0$: \\ $P(Y \in B \mid X\in A) = $ \\ \textbf{(disc.)} $\displaystyle{\sum_{y\in B} \dfrac{\sum_{x\in A} p(x,y)}{\sum_{x \in A} p_X(x)}}$ \\
\textbf{(cont.)} $\displaystyle{\int_B \dfrac{\int_A f(x,y)dx}{\int_A f_X(x)dx} dy}$ \\ \textbf{discrete} \\ \textbullet  $p(y\mid x)=P(Y=y \mid X=x) = \dfrac{p(x,y)}{p_X(x)}$ \\ \textbullet $P(Y\in B \mid X=x)= \displaystyle{\sum_{y\in B} p(y\mid x)}$ \\ \textbullet $F(y\mid x)=P(Y\leq y \mid X=x)= \displaystyle{\sum_{t\leq y} p(t\mid x)}$ \\ \textbf{conti.} \\ \textbullet $P(Y \in B, X=x)= \displaystyle{ \int_B f(y\mid x)dy}$ \\ \textbullet $F(y\mid x)=P(Y\leq u \mid X=x)= \displaystyle{\int_{\infty}^y f(t \mid x) dt}$ \\ \textbullet $\dfrac{\partial F}{\partial y}(y\mid x)=f(y\mid x)$}
\item{\textbf{Momentum:} \\ \textbullet $m_{X,Y}(s,t)=m(s,t)=E(e^{sX+tY})$ \\ \textbullet $\dfrac{\partial ^{k+n}m}{\partial s^k \partial t^n}(0,0) = E(X^kY^n)$ \\ \textbullet $m(s,0)=$ marg. mgf of X \\ \textbullet $m(0,s)=$ marg. mgf of Y}
\item{\textbf{Covariance} \\ \textbullet $\sigma_{XY}\equiv Cov(X,Y) = E((X-E(X))(Y-E(Y))) = E(XY)-E(X)E(Y)$ \\ \textbullet $Cov(X,Y)=Cov(Y,X)$ \\ \textbullet $Cov(X,X)=V(X)$ \\ \textbullet $Cov(a,X)=0$}
\item{\textbf{Correlation}: \\ \textbullet $\rho_{XY}\equiv \rho(X,Y)=\dfrac{\rho_{XY}}{\rho_X \rho_Y}$ \\ \textbullet $\mid \rho_{XY} \mid \leq 1$ (Cauchy-swartz) \\ \textbullet $\rho_{XY}=\pm 1 \Leftrightarrow Y = a+bX$}
\item{\textbf{Mutlinomial dist}: \\ \textbullet $P(Y_1=y_1, ..., Y_n=y_n) = \dfrac{n!}{n_1!...n_n!}p_1^{n_1}...p_n^{n_n}$}
\end{itemize}


\textbf{Independnant r.v.}
\begin{itemize}
\item[\textbullet]{$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$}
\item[\textbullet]{\textbf{Condition neces.}: \\ \textbullet $F(x,y) = F_X(x)F_Y(y)$ \\ \textbullet $f(x,y)=f_X(x)f_Y(y)$ \\ \textbullet $f(y\mid x)=f_Y(y)$ \\ \textbullet $f(x\mid y)=f_X(x)$)}
\item{\textbf{Conditionnal expect.}: \\ \textbullet \begin{empheq}[left={$f(x)$= \empheqlbrace}]{align}
& \sum g(y)p(y\mid x)  \\ & \int^y g(y)f(y\mid x) dy 
\end{empheq} \\ \textbullet $\mu_Y = E(Y\mid X=x)$ \\ \textbullet $\sigma^2_Y(x)\equiv Var(Y\mid X=x) = E(Y^2\mid X=x) - \mu^2_Y(x)$}
\end{itemize}


\textbf{Function of rv and transf.}

\begin{itemize}
\item{\textbf{Monotomic function}: \\ \textbullet $f_Y(y)=\dfrac{f_X(x)}{|g'(x)|} |_{x=g^{-1}(y)}$}
\item{\textbf{Lognormale dist}: if $X\sim N(\mu, \sigma^2)$,  $Y=e^X \sim LN(\mu, \sigma^2)$ and fdp. of $Y$= \\
$f(y)=\dfrac{1}{y\sqrt{2\pi \sigma^2}}e^{-\dfrac{1}{2}\left( \dfrac{ln y - \mu }{\sigma} \right)^2}$}
\item{\textbf{Chi squarred dist} if $X\sim N(0,1)$ and $Y=X^2$ then $Y\sim \chi^2_1$ (v=2)}
\item{\textbf{Bivarate transf}: \\ \textbullet $f_Y(y) = \dfrac{f_X(x)}{\mid J_g(x)\mid} |_{x=g^-1(y)'}$ \\ \textbullet $|J_g(x)|=$abs$\left[ \dfrac{\partial g_1}{\partial x_1}\dfrac{\partial g_2}{\partial x_2}-\dfrac{\partial g_1}{\partial x_2}\dfrac{\partial g_2}{\partial x_1}\right]$ \\E ET VAR!!!!!!!!}
\end{itemize}

\textbf{Sampling distributions}

\begin{itemize}
\item{\textbf{Mean \& Variance:} For $X_i$,i=1...n \\ \textbullet $\mu=E(X_i)$, $\sigma^2=Var(X_i)$ \\ \textbullet $\eta_4 = E((X_i-\mu)^4)$ \\ \textbullet $E(\overline{X})=\mu$ and $Var(\overline{X})=\dfrac{\sigma^2}{n}$ \\ \textbullet $E(S^2)=\sigma^2$ and $Var(S^2)=\dfrac{1}{n}\left( \eta_4 - \dfrac{n-3}{n-1}\sigma^4 \right)$ \\ \textbullet if $X\sim Be(p)|\hat{p_n} = n^{-1}\displaystyle{\sum_{i=1}^n X_i}$, then: \\ \textbullet $E(\hat{p_n})=p$, $Var(\hat{p_n})=\dfrac{p(1-p)}{n}$}
\item{\textbf{Normal population} if $X\sim N(\mu, \sigma^2)$ then: \\ \textbullet $\dfrac{\overline{X}-\mu }{\sigma /\sqrt{n}} \sim N(0,1)$ \\ \textbullet $\overline{X} \indep S^2$ \\ \textbullet $(n-1)S^2/\sigma^2 \sim \chi^2_{n-1}$ \\ \textbullet $\dfrac{\overline{X}-\mu }{S /\sqrt{n}} \sim t_{n-1}$}
\item{\textbf{Central Limit th.} if $n\Rightarrow \infty$\\ \textbullet $Z_n = \sqrt{n}\dfrac{\overline{X_n}-\mu}{\sigma} \Rightarrow Z \sim N(0,1)$ \\ for $n\gg$ \\ \textbullet $\displaystyle{\sum^n_{i=1} X_i \approx N(n\mu , n\sigma^2)}$}
\item{\textbf{Normal approx } \\ if $n>9\dfrac{max(p,q)}{min(p,q)}$ \\ \textbullet $Bin(n,p)\approx N(np,np(1-p))$ \\ \textbullet $Po(\lambda) \approx N(\lambda , \lambda)$, ($\lambda\gg$) \\ \textbullet $Gamma(\alpha, \beta) \approx N(\alpha \beta , \alpha \beta ^2)$,($\alpha \gg$) \\ \textbullet $\chi^2 \approx N(n,2n)$,($n\gg$)}
\end{itemize}

\textbf{Point estimations}
\begin{itemize}
\item{\textbf{Joint pdf.} for \textbf{X}($X_1$,...)\\ \textbullet $f_n(x;\theta) = \displaystyle{\prod_{i=1}^n f(x_i; \theta)}$}
\item{\textbf{Bias and MSE} \\ \textbullet Unbiased if $E(\hat{\theta}) = \theta$ \\ \textbullet $Bias(\hat{\theta}) = E(\hat{\theta})-\theta$ \\ \textbullet $MSE(\hat{\theta})=E\left[ \left( \hat{\theta} - \theta \right) ^2 \right]$ \\ $=Bias^2(\hat{\theta}) + Var(\hat{\theta})$}
\item{\textbf{Consestency} $\hat{\theta}$ cnst. if : \\ \textbullet $P(\mid \hat{\theta} - \theta \mid > \epsilon) \Rightarrow 0$, $n\Rightarrow \infty$ \\ \textbullet $MSE(\hat{\theta}) \Rightarrow 0$ \\ \textbullet if $\hat{\theta}$ cnst. for $\theta$,$g(\hat{\theta})$ cnst. for $g(\theta)$}
\item{\textbf{Method of Moments} \\ \textbullet $\mu_k = E(X^k)$ \\ \textbullet $\hat{\mu_k}=n^{-1} \sum_{i=1}^n X_i^k$ \\ Solve $\Rightarrow \hat{\mu_k} = \mu_k(\theta)$ in $\theta$}
\item{\textbf{Maximum Likelihood} \\ \textbullet $L_n(\theta )=f_n(x_1,...,x_n;\theta ) =_iid \displaystyle{\prod_{i=1}^n f(x_i;\theta )}$ \\ \textbullet $LL_n(\theta ) = \displaystyle{\sum_{i=1}^n ln(f(x_i;\theta ))}$ \\ S\textbullet Solve $S_n(\theta) = \left( \dfrac{\partial LL}{\partial \theta_1} (\theta), \dfrac{\partial LL}{\partial \theta_2} (\theta) \right)^T =0$ \\ Check $2^{nd}$ to be sure of the maximum \\ \textbullet $\hat{\theta}$ equivarian and consit.}
\end{itemize}

\textbf{Expect. and varia. properties}

\begin{itemize}
\item{\textbf{General}: \\ \textbullet $E(a+bX+cY)=a+bE(X)+cE(Y)$ \\ \textbullet $Var(a+bX+cY)=b^2Var(X)+c^2Var(Y)+2bcCov(X,Y)$ \\ \textbullet $Cov(a+bX,c+dY)=bdCov(X,Y)$ \\ \textbullet $Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)$ \\ \textbullet $E(h(X))\neq h(E(X))$ \\ \textbullet $E(XY)\neq E(X)E(Y)$ \\ \textbullet $Var(X+Y)\neq Var(X)+Var(Y)$}
\item{\textbf{if $A\indep B$}: \\ \textbullet $E(g(X)h(Y))=E(g(X))E(h(Y))$ \\ \textbullet $m_{X+Y}(t) = m_X(t)m_Y(t)$ \\ \textbullet $\sigma_{XY}=0=\rho(X,Y)$ \\ \textbullet $Var(a+bX+cY)=b^2Var(X)+c^2Var(Y)$}
\end{itemize}

\textbf{Confidence interval}
\begin{itemize}
\item{\textbf{Goal}: \\ \textbullet $P(\hat{\theta_L} \leq \theta \leq \hat{\theta_U})=1-\alpha$}
\item{\textbf{Example}: \\ \textbf{Mean} \\ \textbullet $CI=\left[ \overline{X}-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}, \overline{X}+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}} \right]$ \\ and $l=2z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}$ (not random!)}
\item{\textbf{Deriving a CI} for $Z_n(\theta )\equiv Z(\textbf{X},\theta )$\\ \textbullet $P(a\leq Z_n(\theta ) \leq b)=1-\alpha$ \\ $\Rightarrow P(L\leq \theta \leq U)=1-\alpha$}
\end{itemize}

\textbf{Additionnal informations}

\begin{itemize}
\item{\textbf{Geometric serie sum:} \\ \textbullet $\displaystyle{\sum_{i=0}^n q^i} = \dfrac{1-q^n}{1-q}$}
\item{\textbf{Tchebyshev theorem:} \\ \textbullet $P( | X- \mu |\geq \sigma k) \leq \dfrac{1}{k^2}$}
\item{\textbf{Markov theorem:} \\ \textbullet $P(|X|\geq a)\leq \dfrac{E|X|}{a}$}
\end{itemize}
\end{multicols*}

\end{document}




