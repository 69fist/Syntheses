\documentclass[en]{../../../eplsummary}
\usepackage{listings}

\usetikzlibrary{calc, trees, positioning, arrows, shapes, shapes.multipart, shadows, matrix, decorations.pathreplacing, decorations.pathmorphing}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}

\hypertitle{cloud-INGI2145}{7}{INGI}{2145}
{Houtain Nicolas, Thibault Gérondal}
{Canini Marco}

\section{Cloud computing}

\subsection{Introduction}

\begin{center}
  \textit{Cloud computing is a model for enabling convenient, 
    on-­demand network access to a shared pool of configurable 
    computing resources (e.g., networks, servers, storage, 
    applications, and services) that can be rapidly provisioned 
    and released with minimal management effort or 
  service provider interaction.}
\end{center}

\begin{tabular}{cm{10cm}}
  \textbf{Cloud commandments} &
  \begin{enumerate}
    \item Partition everything
    \item Use asynchony everywhere
    \item Automate everything
    \item Remember that everything fails
    \item Embrace inconsistency
  \end{enumerate}
\end{tabular}

\begin{tabular}{cm{10cm}}
  \textbf{Cloud benefit} &
  \begin{itemize}
    \item Elastic, just-­in-­time infrastructure
    \item More efficient resource utilization
    \item Pay for what you use
    \item Potential to reduce processing time (parallelization)
    \item Leverage multiple data centers (high availability)
  \end{itemize}
\end{tabular}


\subsubsection{Hardware scalability}

Cloud need for \textbf{scalability} because modern application require
huge amounts of processing and data. Cluster (\textit{room-sized}) and datacenter 
(\textit{building-sized}) can provide the resources needed.
They are composed of \textbf{rack} which is a aggregation of storage devices, many
nodes and switch to connect nodes together. Unfortunately, they are not perfect.
\begin{enumerate}
  \item Difficult to dimension because they must be provisioning for the peak load
  \item Expensive in hardware invest, expertise (ex: special software) and maintenance
  \item Difficult to scale because adding new machines is not easy
\end{enumerate}


\subsubsection{Model}
Cloud computing is a business models where everything is a service:
\begin{itemize}
  \item \textbf{SaaS}: Software as a service
  \item \textbf{PaaS}: Platform as a service
  \item \textbf{IaaS}: Infrastructure as a service
\end{itemize}

\subsubsection{Types}
There also have three types of cloud : 
\begin{itemize}
  \item \textbf{Public}: commercial commercial service open to almost anyone.
  \item \textbf{Community}: shared by several similar organization
  \item \textbf{Private}: shared within a organization
\end{itemize}
In this course we focus on public cloud.

\subsubsection{Applications}

Typically, applications that involve large amounts of computation, storage,
bandwidth Especially when lots of resources are needed quickly or load varies
rapidly.

\begin{itemize}
  \item \textbf{Web app}: Near the edge of the application focus is on vast 
    numbers of clients and rapid response
  \item \textbf{Processing pipelines}: Inside we find data-­intensive services that 
    operate in a pipelined manner, asynchronously
  \item \textbf{Batch processing}: Deep inside the application we see a world of 
    virtual computer clusters that are scheduled to 
    share resources and on which applications like 
    MapReduce (Hadoop) are very popular
\end{itemize}


\subsubsection{Virtualization}
IS used to simulate multiple physical machine for the consumer with different
capabilities. It's powerful for security and isolation because VM cannot influence
other. In the other hand, performance is hard to predict because other VM run on
the same physical machine.

\subsubsection{Challenge}
\begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
  \begin{itemize}
    \item Availability
    \item Data lock-in (moving data)
    \item Data confidentiality and auditability 
    \item Data transfer bottlenecks
    \item Performance unpredictability for VM
  \end{itemize}
  &
  \begin{itemize}
    \item Scalable storage
    \item Bugs in large distributed systems
    \item Scaling quickly
    \item Reputation fate sharing
    \item Software licensing
  \end{itemize}
\end{tabular}

\section{Summary of Eventually Consistent}

Data inconsistency in large-scale reliable distributed systems has to be tolerated for two reasons: improving read and write performance under highly concurrent conditions; and handling partition cases where a majority model would render part of the system unavailable even though the nodes are up and running.

Whether or not inconsistencies are acceptable depends on the client application. In all cases the developer needs to be aware that consistency guarantees are provided by the storage systems and need to be taken into account when developing applications.

\begin{description}
\item[Strong consistency] After the update completes, any subsequent access (by A, B, or C) will return the updated value.

\item[Weak consistency] The system does not guarantee that subsequent accesses will return the updated value. A number of conditions need to be met before the value will be returned. The period between the update and the moment when it is guaranteed that any observer will always see the updated value is dubbed the inconsistency window.

\item[Eventual consistency] This is a specific form of weak consistency; the storage system guarantees that if no new updates are made to the object, eventually all accesses will return the last updated value. If no failures occur, the maximum size of the inconsistency window can be determined based on factors such as communication delays, the load on the system, and the number of replicas involved in the replication scheme. The most popular system that implements eventual consistency is DNS (Domain Name System). Updates to a name are distributed according to a configured pattern and in combination with time-controlled caches; eventually, all clients will see the update.

\item[The eventual consistency has a number of variations that are important to consider:]

\item[Causal consistency] If process A has communicated to process B that it has updated a data item, a subsequent access by process B will return the updated value, and a write is guaranteed to supersede the earlier write. Access by process C that has no causal relationship to process A is subject to the normal eventual consistency rules.

\item[Read-your-writes consistency] This is an important model where process A, after it has updated a data item, always accesses the updated value and will never see an older value. This is a special case of the causal consistency model.

\item[Session consistency] This is a practical version of the previous model, where a process accesses the storage system in the context of a session. As long as the session exists, the system guarantees read-your-writes consistency. If the session terminates because of a certain failure scenario, a new session needs to be created and the guarantees do not overlap the sessions.

\item[Monotonic read consistency] If a process has seen a particular value for the object, any subsequent accesses will never return any previous values.

\item[Monotonic write consistency] In this case the system guarantees to serialize the writes by the same process. Systems that do not guarantee this level of consistency are notoriously hard to program.

\end{description}

A number of these properties can be combined. For example, one can get monotonic reads combined with session-level consistency. From a practical point of view these two properties (monotonic reads and read-your-writes) are most desirable in an eventual consistency system, but not always required. These two properties make it simpler for developers to build applications, while allowing the storage system to relax consistency and provide high availability.


\section{Design for scale}

A system is scalable if it can easily adapt to 
increased (or reduced) demand.

\subsection{Parallelism programming}

\subsubsection{Vocabulary}
\begin{description}
  \item[Parallelism]: refers to techniques to make programs faster by
    performing several computations in parallel.

  \item[Concurrency]: is the composition of independently executing
    computations.
\end{description}

\subsubsection{Parallelization}
\begin{itemize}
  \item \textbf{Amdahl's law}:

    \begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
      $$\quad S = \frac{1}{\alpha +
      \frac{1-\alpha}{N}}$$ where $\alpha$ is the sequential part and $N$ the
      number of cores.
      &
      \includegraphics[width=6cm]{img/amdahl.png}
    \end{tabular}

    The coarse-grain (opposited to fine-grain) parallelism is more efficient
    because he limits the communication and coordination overheads by allow
    bigger task.
  \item \textbf{Dependencies}: Some task need to be after other which limits the degree of 
    parallelism. $\rightarrow$ Scheduling problem
\end{itemize}

\subsubsection{Architecture}
\begin{itemize}
  \item Symmetric multiprocessing (SMP): all processors share same memory.

    \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
      \begin{itemize} 
        \item[+] Simplicity and easy to load balance 
        \item[-] Scalability limited and expensive
      \end{itemize}
      &
      \begin{tiny}
        \begin{tikzpicture}
          \node[draw, rectangle] (1) {CPU};
          \node[draw, rectangle] (2) [right=of 1] {CPU};
          \node[draw, rectangle] (3) [right=of 2] {CPU};

          \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
          \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
          \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

          \node (12) [below=0.3cm of 11] {};
          \node (22) [below=0.3cm of 21] {};
          \node (32) [below=0.3cm of 31] {};

          \node (tmp) [below left= -0.2cm and 0.7cm of 12] {};
          \node (tmp2) [below right=-0.2cm and 0.7cm of 32] {};

          \draw (tmp) edge[double, <->] node[below](p) {Memory bus} (tmp2);
          \draw (12) edge[<->] (11);
          \draw (22) edge[<->] (21);
          \draw (32) edge[<->] (31);

          \node[draw, rectangle] (mem) [below=1.0cm of 21] {Memory};
          \draw (mem) edge[<->] (p);
        \end{tikzpicture}
      \end{tiny}
    \end{tabular}

  \item Non Uniform memory architecture (NUMA)

    \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
      \begin{itemize} 
        \item[+] Better scalability and faster memory
        \item[-] Complicates programming and scalability limited
      \end{itemize}
      &
      \begin{tiny}
        \begin{tikzpicture}
          \node[draw, rectangle] (1) {CPU};
          \node[draw, rectangle] (2) [right=of 1] {CPU};
          \node[draw, rectangle] (3) [right=of 2] {CPU};

          \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
          \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
          \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

          \node[draw, rectangle] (12) [below=0.3cm of 11] {Memory};
          \node[draw, rectangle] (22) [below=0.3cm of 21] {Memory};
          \node[draw, rectangle] (32) [below=0.3cm of 31] {Memory};

          \node (13) [below=0.3cm of 12] {};
          \node (23) [below=0.3cm of 22] {};
          \node (33) [below=0.3cm of 32] {};

          \node (tmp) [below left= -0.2cm and 0.7cm of 13] {};
          \node (tmp2) [below right=-0.2cm and 0.7cm of 33] {};

          \draw (tmp) edge[double, <->] node[below](p) {Memory bus} (tmp2);
          \draw (12) edge[<->] (11);
          \draw (22) edge[<->] (21);
          \draw (32) edge[<->] (31);

          \draw (12) edge[<->] (13);
          \draw (22) edge[<->] (23);
          \draw (32) edge[<->] (33);
        \end{tikzpicture}
      \end{tiny}
    \end{tabular}

  \item Shared Nothing

    \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
      \begin{itemize} 
        \item[+] Nice scalability
        \item[-] Requires different programming model
      \end{itemize}
      &

      \begin{tiny}
        \begin{tikzpicture}
          \node[draw, rectangle] (1) {CPU};
          \node[draw, rectangle] (2) [right=of 1] {CPU};
          \node[draw, rectangle] (3) [right=of 2] {CPU};

          \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
          \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
          \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

          \node[draw, rectangle] (12) [below=0.3cm of 11] {Memory};
          \node[draw, rectangle] (22) [below=0.3cm of 21] {Memory};
          \node[draw, rectangle] (32) [below=0.3cm of 31] {Memory};

          \node[draw, rectangle] (14) [below=0cm of 12] {Net card};
          \node[draw, rectangle] (24) [below=0cm of 22] {Net card};
          \node[draw, rectangle] (34) [below=0cm of 32] {Net card};

          \node (13) [below=0.3cm of 14] {};
          \node (23) [below=0.3cm of 24] {};
          \node (33) [below=0.3cm of 34] {};

          \node (tmp) [below left= -0.2cm and 0.7cm of 13] {};
          \node (tmp2) [below right=-0.2cm and 0.7cm of 33] {};

          \draw (tmp) edge[double, <->] node[below](p) {Network} (tmp2);
          \draw (12) edge[<->] (11);
          \draw (22) edge[<->] (21);
          \draw (32) edge[<->] (31);

          \draw (13) edge[<->] (14);
          \draw (23) edge[<->] (24);
          \draw (33) edge[<->] (34);
        \end{tikzpicture}
      \end{tiny}
    \end{tabular}

\end{itemize}


\subsection{Distributed programming}

\subsubsection{Vocabulary}
\begin{description}
  \item[Faults]: Some component is not working correctly
  \item[Failure]: System as a whole is not working correctly
\end{description}

\subsubsection{Wide-area network}
The transfert speed for some data are defined by some attributs:
\begin{enumerate}
  \item Propagation delay
  \item Bottlenecks capacity on the path
  \item Queueing delay, loss, reordering, congestion, rtt 
    (take in account by TCP)
\end{enumerate}

$\rightarrow$ wide-area networks complicates the communication and faults are
more common

\subsubsection{Faults}

Fault are a common event in distributed system and some faults
are correlated.

\begin{itemize}
  \item \textbf{Crash faults}: node simply stop
  \item \textbf{Rational behavior}: owner manipulates node to increase profit
    (ex: lies on the routes to have traffic through it's own AS)
  \item \textbf{Byzantine faults}: faulty node could do anything (ex: stop, send spam,
    attack other, tell lies,...)
\end{itemize}

To prevent fault, we can \textit{prevent} them by using verification,
\textit{detect} them or \textit{mask} them by using replicas. The problem of
using \textbf{replicas} is to be able to maintain consistency between them!

\subsubsection{Consistency}

\begin{itemize}
  \item \textbf{Strong consistency}: After update completes, all subsequent
    accesses will return the updated value

  \item \textbf{Weak consistency}: After update completes, accesses do not
    necessarily return the updated value;; some condition must be satisfied
    first (such as update needs to reach all the replicas)

  \item \textbf{Eventual consistency}: Specific form of weak consistency: If no
    more updates are made to an object, then eventually all reads will return
    the latest value

  \item \textbf{Causal consistency}: If client A has communicated to client B that it
    has updated a data item, a subsequent access by B will return the updated
    value, and a write is guaranteed to supersede the earlier write. Client C
    that has no causal relationship to client A is subject to the normal
    eventual consistency rules

  \item \textbf{Read-your-writes consistency}: Client A, after it has updated a data
    item, always accesses the updated value and will never see an older
    value.

  \item \textbf{Session consistency}: Like previous case but in the context of a
    session, for as long as the sessions remains alive.

  \item \textbf{Monotonic read consistency}: If client A has has seen a particular value
    for the object, any subsequent accesses will never return any previous
    values

  \item \textbf{Monotonic write consistency}: In this case the system guarantees to
    serialize the writes by the same process
\end{itemize}

We can combine them, and monotonic reads + read-your-write are most desirable
than eventual consistency.

\paragraph{Storage system consistency: example}

We have N nodes that can store data.
To write a value: Pick W replicas and write the value to each, using a fresh timestamp
To read a value:
\begin{itemize}
\item Pick R replicas and read the value from each
\item Return the value with the highest timestamp
\item If any replicas had a lower timestamp, send them the newer value
\end{itemize}

Strong consistency :
\begin{description}
\item[Majority quorum] Always write to and read from a majority of nodes. At least one node knows the most recent value. tolerate up to ⌈N/2⌉ - 1 crashes. But have to read/write ⌊N/2⌋ + 1 values.
\item[Read/write quorums] Read R nodes, write W nodes, s.t. R + W > N. Adjust performance of reads/writes. But availability can suffer.

\begin{tabular}{m{5cm}m{12cm}}
  \includegraphics[width=3cm]{img/replicas} &
  \includegraphics[width=7cm]{img/replicas2}
  \begin{itemize}
    \item[$\rightarrow$] Strong consistency
  \end{itemize} \\
\end{tabular}
\item[Consensus solutions] Paxos (for crash faults), PBFT (for Byzantine faults). Idea : Correct replicas ``outvote'' faulty ones.
\end{description}
 
The cap theorem : We can get at most two out of the three
\begin{itemize}
\item Consistency : All clients single up-to-data copy of the data, even in the presence of concurrent updates
\item Availability: Every request (including updates) received by a non-failing node in the system must result in a response, even when faults occur
\item Partition-tolerance: Consistency and availability hold even when the network partitions
\end{itemize}

Dealing with network partitions when a partition is cut : Enter an explicit partition mode that can limit some operations.


\includegraphics[width=\linewidth]{img/part}




\paragraph{Consensus}
\begin{enumerate}
  \item Clients send requests to each of the replicas
  \item Replicas coordinate and each return a result
  \item Client chooses one of the results, e.g., the one that is 
    returned by the largest number of replicas
  \item If a small fraction of the replicas returns the wrong result, or 
    no result at all, they are 'outvoted' by the other replicas
\end{enumerate}


\subsubsection{CAP theorem}
\begin{center}
  \begin{tikzpicture}
    \node (A) {\textcolor{red}{A}};
    \node (C) [below left= 2.5cm and 1.5cm of A] {\textcolor{red}{C}};
    \node (P) [below right= 2.5cm and 1.5cm of A] {\textcolor{red}{P}};

    \node[right =1cm of A, text width=4cm] {\scriptsize \textbf{Availability}: Each client can always read
    and write despite node failures};

    \node[left =1cm of C, text width=4cm] {\scriptsize \textbf{Consistency}: all clients always have the
    same view of the data at the same time};

    \node[right =1cm of P, text width=4cm] {\scriptsize \textbf{Partition-tolerance}: the
    system continues to operate despite arbitrary message loss};

    \draw (A) edge[-] node[left] {C\&A} (C);
    \draw (A) edge[-] node[right] {A\&P \tiny (Many replicas + relaxed consistency)} (P);
  \draw (C) edge[-] node[below] {\begin{tabular}{c}C\&P\\ \tiny (Many replicas + consensus) \end{tabular}} (P);
  \end{tikzpicture}
\end{center}


\paragraph{Actually}, we have a lot of partition and we have a trade-off
between C and A. \textbf{ACID} (atomicity, consistency, isolation, durability)
become \textbf{BASE} (basically available, soft-state, eventually consistent).


\subsection{Structure}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}

    \node[draw, rectangle, minimum height= 2cm] (L) {\rotatebox{90}{Load balancer}};
    \node[draw, rectangle, fill=black!40, minimum height= 2cm, text width=1cm, right= of L] (W) {};
    \node[draw, rectangle, fill= white, minimum height= 0.5cm, text width=1cm, below=-1cm of W] (W1) {};
    \node[above=0cm of W] (1) {Web server};

    \node[draw, rectangle, fill=black!40, minimum height= 2cm, text width=1cm, right= of W] (A) {};
    \node[draw, rectangle, fill=white, minimum height= 0.5cm, text width=1cm, below=-1cm of A] (A1) {};
    \node[above=0cm of A] (1) {App server};

    \node[draw, rectangle, minimum height= 2cm, right= of A] (C) {\rotatebox{90}{Caching}};
    \node[draw, rectangle, minimum height= 2cm, right= of C] (D) {Data store};

    \draw[->] (-2, 0) -- (L);
    \draw[->] (-2, -1) -- (L);
    \draw[->] (-2, 1) -- (L);

    \node[left=2cm of L] (R) {\rotatebox{90}{Request}};

  \end{tikzpicture}
  \caption{Cloud structure with 
  caching which is central to responsiveness}
\end{figure}

\begin{itemize}
  \item \textbf{Stateless server}: Views a client request as an independent 
    transaction and responds to it


    Easy to scale and more robuste because instance failure does not require
    overheas restoring state

  \item \textbf{Statefull server}: Scaling is challenging because we need to maintains 
    the state.

    \paragraph{Traditionnal approach is replication}:

    \begin{tabular}{m{12cm}m{5cm}}
      \begin{itemize}
        \item Data is written to a \textit{master server} and then
          replicated to one or more \textit{slave servers}
          (synchronously or asynchronously)
        \item Read operations can be handled by the slaves
      \end{itemize}
      But in this case, master becomes the write bottleneck and single point of failure.
      &
      \centering
      \begin{tikzpicture}
        \node[draw, rectangle] (M) {Master};
        \node[draw, rectangle, below=1cm of M] (S) {Slave};
        \node (I) [right= of M] {};
        \node (O) [right= of S] {};

        \draw[->, double] (I) edge node[above] {in/out} (M);
        \draw[<-, double] (I) edge (M);
        \draw[<-, double] (O) edge node[below] {out}(S);
        \draw[->, double] (M) edge (S);
      \end{tikzpicture}
    \end{tabular}


    \paragraph{Sharding with partitionning}
    The idea is to split data between multiple 
    machines and have a way to make sure that
    the data is available on the right place.
    Typically, define a \textbf{sharding key} and create a \textbf{shard mapping}.

    \begin{tabular}{cm{10cm}}
      This allow to have &
      \begin{itemize}
        \item a really high availability
        \item increase read and write throughput
        \item the possibility 
          of doing more work in parallel within the application server.
        \item[But] the challenge is to find a good partitionning scheme.
      \end{itemize}
    \end{tabular}

    \textbf{Sharding} is not only for partitioning data 
    within a database, but can also be use to partition data across caching 
    servers (memcached, redis).
\end{itemize}

\paragraph{First-tier parallelism}
Parallelism is vital for fast interactive services, and 
parallel actions must focus on the \textbf{critical path}. This is the
contributor delay for the response delay and do
not care about asynchronous that are shortly.

\begin{center}
  \includegraphics[width=9cm]{img/critical}
\end{center}


Note that update for replicas are typically done in asynchronous way, so we
might not experience much delay on the critical path.

$\rightarrow$ Asynchronous operations decouple systems and 
enable quicker responses at the expense strong 
consistency.
Indeed, this can rise some issues which result in inconsistency:
\begin{itemize}
  \item We don't know in which order replicas applying the update
  \item The leader can crash before replicas change and lose some
    update
\end{itemize}


\section{Cloud storage}

\begin{center}
  \includegraphics[width=12cm]{img/storage}
\end{center}

Many cloud services have a similar structure, 
But the actual storage service is very simple (Read/write 'blocks',
similar to a giant hard disk) and the translation done by the web service.

\paragraph{Ideal store}
\begin{itemize}
  \item Perfect durability – nothing would ever disappear in a crash
  \item 100\% availability – we could always get to the service
  \item Zero latency from anywhere on earth – no delays!
  \item Minimal bandwidth utilization – we only send across the network what we absolutely need
  \item Isolation under concurrent updates – make sure data stays consistent
\end{itemize}

BUT he “cloud” exists over a physical network (communication take time + limited bandwith)
and The “cloud” has imperfect hardware (failures)

\paragraph{Observation}
\begin{itemize}
  \item Read-­only (or read-­mostly) data is easiest to support
  \item Granularity matters: “Few large-­object” tasks generally 
    tolerate longer latencies than “many small-­object” tasks


    But it’s much more expensive to replicate or to update!

  \item[->] Maybe it makes sense to develop separate solutions for large 
    read-­mostly objects vs. small read-­write objects!
    Different requirements → different technical solutions
\end{itemize}


\subsection{Key-value stores (KVS)}
The key-­value store (KVS) is a simple 
abstraction for managing persistent state where data is 
organized as (key,value) pairs with three basic operation:
\begin{enumerate}
  \item \texttt{PUT(key, value)}
  \item \texttt{value = GET(key)}
  \item \texttt{DEL(key)}
\end{enumerate}

\paragraph{Concurrency control}
Most systems use locks on individual items.



\subsubsection{Amazon dynamo}
\begin{figure}[!h]
  \centering
  \includegraphics[width=5cm]{img/AWSplat}
  \caption{Amazon dynamo. 
    AWS is \textbf{Availability\&Partition-tolerance} and follow 
  \textbf{BASE} philosophy.}
\end{figure}

\begin{tabular}{cm{10cm}}
  Dynamo is :&
  \begin{itemize}
    \item High performance with low latency
    \item High scalable
    \item High available key storage, especially for writes
    \item Partition/fault-tolerant
    \item Eventually consistent (sacrify for high availability)
  \end{itemize}
\end{tabular}
Very low latency writes, reconciliation in reads.

\paragraph{Techniques}
\begin{itemize}
  \item \textbf{Consistent hashing}: dynamically partitions a set of keys
    over a set of storage nodes (hash keys and give 
    a key of m-bits). One key range per virtual node.

    \paragraph{Data partitioning and load balancing}
    A key is assigned to the closest successor node id 
    (key $k$ is assigned to the first node with $id \geq k$)

    \includegraphics[width=5cm]{img/consistent}

    \begin{center}
      Each node has at most $(1+\epsilon) \frac{K}{N}$ keys with $N$ nodes, $K$ keys
      and $\epsilon=O(log(N))$
      %TODO: why epsilon=0 with virtual nodes?
    \end{center}

    \paragraph{Replication}
    Replication is done on the successors node because if the node fail, it's the successor
    which will take failed node keys.

    \begin{center}
    \includegraphics[width=5cm]{img/dynamoReplica}
    \end{center}

    Replication is may in asynchronous way (eventual consistency), and
    key data versioning technique is done with vector clocks.


    If the replica A in the preference list is down then
    another replica is created on a new node.
    In this case, coordinator will involve D that substitutes
    A until A comes back again. When D gets info A is
    back up it hands back the data to A


  \item \textbf{Vector clocks}: Each write to a key k is associated with a 
    vector clock VC(k) which is an array (map) of integers (one entry VC(k)[i] for each node i).

    This is use for tracking causal dependencies among different versions 
    of the same key (data)

    \begin{center}
    \includegraphics[width=5cm]{img/vectorClock}
    \end{center}

  \item \textbf{Sloppy quorums}: In order to enforce consistency, to response
    to a read or write operation the node must asl to a \textit{quorum}
    which is the minimum number of votes that a distributed transaction has
    to obtain.

    \begin{itemize}
      \item $R$ number of nodes that participate in a get, $W$ number
        of node that participate in a write and $R+W > N$

      \item \textbf{Put}: Generate new $VC$, write locally, send value $VC$
        to N selected nodes from preference list and wait for $W-1$

      \item \textbf{Get}: Send read to $N$ selected nodes from preference list, 
        wait for $R$, select highest versions per $VC$ and return
        all such versions. 

        Writeback merge versions.

      \item[Note]
        \begin{enumerate}
          \item Note that each node has routing info to all other node
            to reduce latency (but this is lower scalable)
            %TODO: slide 99 for 03
        \end{enumerate}
    \end{itemize}

    \textbf{Sloppy} allow availability under a much wider range of 
    partitions (failures) but sacrifice consistency. The
    N selected nodes are the first N healthy 
    nodes.

  \item \textbf{Anti-entropy protocol using hash/merkle tree}:
    Each Dynamo node keeps a Merkle tree for 
    each of its key ranges. Hash trees can be used to verify any kind of data stored.

    \begin{center}
    \includegraphics[width=5cm]{img/merkle}
    \end{center}

    Compares the root of the tree with replicas
    \begin{itemize}
      \item If equal, all keys in a range are equal (replicas in sync)
      \item If not equal 
        Traverse the branches of the tree to pinpoint the children that differ
        The process continues to all leaves
        Synchronize on those keys that differ
    \end{itemize}

  \item \textbf{Gossip-based group membership protocol}: 
    %TODO slides 106 for 03


    Membership info are also eventually 
    consistent — propagated by background 
    gossip protocol.
    \begin{itemize}
      \item Node contacts a random node every 1s
      \item 2 nodes reconcile the membership info and partitioning/placement
        metadata
    \end{itemize}


    \paragraph{Unreliable failure detection (FD)}
    Used to refresh the healthy node info in the extended 
    preference list.

    %TODO 107 for 04

\end{itemize}


\section{MapReduce}

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[node distance=2cm]
    \node[draw, rectangle] (I0) {Input split 0};
    \node[draw, rectangle, below=0cm of I0] (I1) {Input split 1};
    \node[draw, rectangle, below=0cm of I1] (I2) {Input Split 2};

    \node[draw, circle, right=of I0 ] (W2) {Worker};
    \node[draw, circle, below=of W2 ] (W1) {Worker};
    \node[draw, circle, above=of W2 ] (W3) {Worker};

    \node[draw, minimum height=1cm,rectangle, right=of W1 ] (F1) {};
    \node[draw, minimum height=1cm, rectangle, right=of W2 ] (F2) {};
    \node[draw, minimum height=1cm, rectangle, right=of W3 ] (F3) {};

    \node[draw, rectangle, minimum height=1cm, right=0cm of F1 ] (FF1) {};
    \node[draw, rectangle, minimum height=1cm,right=0cm of F2 ] (FF2) {};
    \node[draw, rectangle, minimum height=1cm, right=0cm of F3 ] (FF3) {};

    \node[draw, circle, above right=1cm and 2.5cm of FF1 ] (R1) {Reducer};
    \node[draw, circle, below right=1cm and 2.5cm of FF3 ] (R2) {Reducer};

    \node[draw, rectangle, right=of R1 ] (O1) {Output file 0};
    \node[draw, rectangle, right=of R2 ] (O2) {Ouput file 1};

    \draw (I0.0) edge[->](W1);
    \draw (I1.0) edge[ ->](W3);
    \draw (I2.0) edge[ ->](W2);

    \draw (W1.0) edge[->] (F1.180);
    \draw (W2.0) edge[->] node 
    {\rotatebox{90}{ \textcolor{red}{\textbf{map}: (key, val) $\rightarrow$ (red\_key, val)}}} (F2.180);
    \draw (W3.0) edge[->] (F3.180);

    \draw (FF1.0) edge[->] (R1.180);
    \draw (FF1.0) edge[->] (R2.180);
    \draw (FF2.0) edge[->] node[below=-2.5cm]
    {\rotatebox{90}{ \textcolor{red}{\textbf{shuffled}: redistribution by output's key}}} (R2.180);
    \draw (FF2.0) edge[->] (R1.180);
    \draw (FF3.0) edge[->] (R1.180);
    \draw (FF3.0) edge[->] (R2.180);

    \draw (R1.0) edge[->] node[above=-2cm]
    {\rotatebox{90}{ \textcolor{red}{\textbf{reduce}: (red\_key, \{set of values\}) $\rightarrow$ result}}} (O1.180); 
    \draw (R2.0) edge[->] (O2.180);

    \node[below=1cm of W1] (2) {Map phase};
    \node[left=of 2] (1) {Input files};
    \node[right=0.8cm of 2] (3){Intermediate files};
    \node[right=0.7cm of 3] (4){Reduce phase};
    \node[right=1cm of 4] (5){Output files};


  \end{tikzpicture}

  \centering
\begin{itemize}
  \item \textbf{File system} distributed across all nodes with replication
  \item \textbf{Driver program} on the master to keeping all node busy
  \item \textbf{Runtime system} which control nodes
\end{itemize}
  \caption{MapReduce where mapper and reducer should be \textbf{stateless}
  }
\end{figure}


A variety of different tasks can be expressed 
as a single-­pass MapReduce program which are specifically designed
for \textbf{batch operation} over large amounts of data:
\begin{itemize}
  \item filter, collect, aggregate, join on shared element
\end{itemize}

\paragraph{Not for MapReduce}
\begin{itemize}
  \item sorting don't work 
  \item algorithms that depend on shared global state during 
processing are difficult to implement.
  \item Process live data at high throughput and low latency

\end{itemize}


\subsection{Failure}
On worker crash we rely on the file system being shared 
across all the nodes. 
\begin{itemize}
  \item If the node wrote its output and then crashed, 
    the file system is likely to have a copy of the complete output.
  \item If the node crashed before finishing its output, the master 
    see that the job isn’t making progress, and restarts the 
    job elsewhere on the system
\end{itemize}

\subsection{Optimization}

\begin{enumerate}
  \item \textit{locality}: Master tries to do work on nodes that 
    have replicas of the data
  \item \textit{Stragglers}: re-execute slow machines task somewhere else.

  \item \textit{Combiner}: use between mapper and reducer in order to be more efficient.
    Typically, it's job is to pass $(xyz, k)$ instead of $k$ copies of $(xyz, 1)$.
\end{enumerate}

%TODO intersection/join WTF?

\subsection{Shuffle}

\begin{itemize}
  \item \textbf{Sorting on key}
    Runtime guarantees that reduce keys will be 
    presented to reduce in sorted order. Shuffle really
    consists of two parts: (1) Partition and (2) Sort.

  \item \textbf{Sorting on value}
    %TODO slide 45 for 04
\end{itemize}

\subsection{Iterative}
\begin{tikzpicture}
  \node[rectangle, draw] (IM) {Map};
  \node[rectangle, draw, right= of IM] (IR) {Reduce};

  \node[rectangle, draw, above right= of IR] (CM) {Map};
  \node[rectangle, draw, right= of CM] (MR) {Reduce};

  \node[rectangle, draw, right= of MR] (TM) {Map};
  \node[rectangle, draw, right= of TM] (TR) {Reduce};

  \node[rectangle, draw, below right= of TR] (OM) {Map};
  \node[rectangle, draw, right= of OM] (OR) {Reduce};

  \draw (IM) edge[->] node[below=1cm, text width=2cm] {Init state} (IR);
  \draw (CM) edge[->] node[above=0.5cm, text width=2cm] {Iterative} (MR);
  \draw (TM) edge[->] node[above=0.5cm, text width=2cm] {Test} (TR);
  \draw (OM) edge[->] node[below=1cm, text width=2cm] {Output result} (OR);

  \draw (IR) edge[->, bend left] (CM);
  \draw (MR) edge[->, ] (TM);
  \draw (TR) edge[->, bend left] (OM);
\end{tikzpicture}

Iterative MapReduce can be done if the reduce
output is compatible with map input but this require to passing the
entire state and doing a lot of network and disk I/O.
  

\subsection{Graphs algorithm}
$G = (V, E)$ where $V$ is vertices, $E$ edges of the form $(v_1, v_2, cost, attr)$
where $v_1, v_2 \in V$.

\begin{itemize}
  \item \textbf{Single Source Shortst Path (SSSP)}: based on dijkstra algorithm idea but parallelized.
    \begin{description}
      \item[Init]: for each node $id$, $<\infty, -, \{<succId, cost>\}>$
      \item[Map]: for each node $id$, $<dst, next, \{<succId, cost>\}>$:
        \begin{itemize}
          \item emit $id$, $<dst, next, \{<succId, cost>\}>$
          \item for each successor:
            \begin{itemize}
              \item emit $succId, <dst+cost, id>$ 
            \end{itemize}
        \end{itemize}
      \item[Reduce]: emit $id$, $<minDst, nextWithMinDst, \{<succId, cost>\}>$
    \end{description}

    This algorithm is based on a \textit{wave} which go on at each iteration.

  \item \textbf{k-clustering}: the idea is to assign $k$ random centroid and move them 
    until be stable.
    \begin{description}
      \item[Init]: choose random point
      \item[Map]: Assign each point to the closest centroid
        $$S_i^{(t)} = \{ x_j : x_j − m_i^{(t)} ≤ x_j − m_i^{(* t )} , i* = 1,..., k \}$$
      \item[Reduce]: Recenter with $m_i$ the new centroid for its points
        $$m_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{x_j \in S_i^{(t)}} x_j$$
    \end{description}

  \item \textbf{Classification with naïve Bayes}: where it's \textit{naïve} because probability
of events are independent.
      $$\textrm{Probability messages "XYZ" is SPAM?} = \frac{p(spam) p(containsXYZ | spam)}
      {p(containsXYZ)}$$ 

     \begin{itemize}
       \item \texttt{p(spam)} : Nbr spam email / Nbr email
       \item \texttt{p(containsXYZ)} : Nbr emails with XYZ / Nbr emails
       \item \texttt{p(containsXYZ|spam)} : Nbr emails with XYZ / Nbr emails with XYZ
     \end{itemize}

     \begin{tabular}{m{2cm}cm{13cm}}
       Nbr spam with XYZ &:& \begin{description}
       \item[map]: for each message $m <class, \{words\}>$, emit $<word, class> \rightarrow 1$
       \item[reduce]: emit $<word, class> \rightarrow count$
     \end{description}
   \end{tabular}

     \begin{tabular}{m{2cm}cm{12cm}}
       Nbr email with XYZ&:& \begin{description}
       \item[map]: for each message $m <class, \{words\}>$, emit $<word> \rightarrow 1$
       \item[reduce]: emit $<word> \rightarrow count$
     \end{description}
   \end{tabular}

   \item \textbf{PageRank}: The idea is to allow $\frac{1}{N}$ vote par page
     at the initialization, and each page vote for all the page it has a
     link to. To ensure fairness, pages voting for more than one page must
     split their vote equally between them. Voting proceeds in rounds: in
     each round, each page has the number of votes it received in the
     previous round.


   \begin{itemize}
     \item \textsc{Random surfer model}: Imagine a random surfer, who starts on a 
       random page and, in each step
       \begin{enumerate}
         \item Click on a random link on the page with probability $d$ 
         \item Jump to a random page with probability $1-d$ 
       \end{enumerate}
       The PageRank of a page can be interpreted 
       as the fraction of steps the surfer spends on 
       the corresponding page


   \item \textsc{Naïve PageRank}
     $$rank_i = \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
   the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

      This can't be able to manage vertex which have no outgoing edge.

      \begin{tabular}{m{7cm}m{7cm}}
        \textbf{Sinks} & \textbf{Hogs}\\
      \begin{tikzpicture}
        \node[rectangle, draw] (G) {Google};
        \node[rectangle, draw, below= of G] (A) {Amazon};
        \node[rectangle, draw, right= of A] (Y) {Yahoo};

        \draw[->] (G) -| (Y);
        \draw[->] (G.300) -- (A.60);
        \draw[<-] (G.240) -- (A.120);
        \draw[->] (A) -- (Y.180);
        \draw[->] (A) -- (Y.180);
      \end{tikzpicture}&
      \begin{tikzpicture}
        \node[rectangle, draw] (G) {Google};
        \node[rectangle, draw, below= of G] (A) {Amazon};
        \node[rectangle, draw, right= of A] (Y) {Yahoo};

        \draw[->] (G) -| (Y);
        \draw[->] (G.300) -- (A.60);
        \draw[<-] (G.240) -- (A.120);
        \draw[->] (A) -- (Y.180);
        \draw[->] (A) -- (Y.180);
        \draw[->, loop right] (Y) edge[loop right] (Y);
      \end{tikzpicture}\\
      PageRank is lost after each round and $\forall_i rank_i = 0$ & 
      PageRank is accumulates on Yahoo and $\forall_i rank_i = 0$ behalf 
      $rank_{yahoo} = 1$
    \end{tabular}

  \item \textsc{Improved PageRank}
     $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
   the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 
   \end{itemize}

   \begin{description}
     \item[Init]: page $p <1/Nn, \{outgoingLink\}>$
     \item[Map]: page $p$ propagate $\frac{1}{N_p} * d * weigth_p$
     \item[Reduce]: page $p$ = $1-d + \sum_{incomingWeight}$
   \end{description}

\end{itemize}


\section{Hadoop}

\subsection{HDFS}
Hadoop use a distributed file system (HDFS) specialized for particular
types of applications. Files are stored as sets of blocks (64MB) which
are replicated for durability and availability.

\begin{center}
\includegraphics[width=11cm]{img/hdfs}
\end{center}

\begin{itemize}
  \item Namespace is managed by a single name node and transfert
    is done directly between client and data node
  \item State stored in two files:

    \begin{tabular}{m{7cm}m{9cm}}
    \begin{itemize}
      \item fsimages: snapshot of file system metadata
      \item edits: change since last snapshot
    \end{itemize} &
    \includegraphics[width=9cm]{img/namenode}
  \end{tabular}
\end{itemize}
%TODO slide 65 for 04


\begin{table}[!h]
\begin{tabular}{m{8cm}m{8cm}}
  \textbf{Does well} & \textbf{Does not well} \\
\begin{itemize}
  \item very large read-only or append-only file (Terabytes EASY)
   \item Sequential access parten
   \item High throughput and high capacity
\end{itemize} &
\begin{itemize}
  \item Low-latency access
    \item Multiple writers
    \item Not append-only file
    \item Storing small files
\end{itemize} \\
& $\rightarrow $ This is well done by Network File System (NFS)
\end{tabular}
\caption{HDFS advantage}
\end{table}


\subsection{Hive Query Language (HQL)}
A data warehouse infrastructure built on top 
of Hadoop for providing data summarization, 
query and analysis.

\subsubsection{SQL recap}
\begin{itemize}
    \item \texttt{SELECT}: Projection and remapping/renaming 
    \item \texttt{JOIN}: Cartesian product 
    \item \texttt{WHERE}: Filtering 
    \item \texttt{UNON, INTERSECT}: Set operations 
    \item \texttt{GROUP BY, MIN, MAX, AVG}: Aggregation 
    \item \texttt{ORDER BY}: Sorting 
    \item \texttt{SELECT .. FROM (SELECT .. FROM ..)}: Composition 
\end{itemize}

\subsubsection{HQL}

\begin{itemize}
    \item Suitable for processing structured data
    \item Create a table structure on top of HDFS
    \item Queries are compiled in to MapReduce jobs
\end{itemize}

\subsection{Pig and Pig latin}
%TODO: slide 29-30 for 06 what say ?
%TODO: really understand

Somewhere between a programming language and a DBMSa which allows
distributed  programming with explicit parallel dataflow operators.

\begin{description}
    \item[Pig]: runtime system
    \item[Pig latin]:
        A dataflow language that compiles to MapReduce.
        Collection-­valued   expressions  whose  results  get  
        assigned  to  variables.
        \begin{itemize}
            \item A  program  does  a  series  of  assignments  in  a  dataflow
            \item It  gets  compiled  down  to  a  sequence  of  MapReduces. 

                Note that Pig Latin has its own query langage (not SQL)
        \end{itemize}
\end{description}

\subsubsection{Basic SQL-like operation}
Operations are \textbf{explicitly} specified.
%TODO

\subsubsection{Implementation}
\begin{center}
\includegraphics[width=10cm]{img/pig}
\end{center}

\subsubsection{Work-sharing techniques}

\begin{center}
\includegraphics[width=10cm]{img/work}
\end{center}


\section{Batch processing}
\subsection{Pregel: Bulk Synchonous Parallel}
Much  of  the  mismatch  stems  from  the  lack  of  shared  global  state.
Complex  applications  and  interactive  queries  both  need  one  thing  that
MapReduce lacks: efficient  primitives  for  data  sharing.


With \textbf{Pregel}, we consider the  nodes  with a  state  (memory)  
that  carries  from  superstep to  superstep.

\subsubsection{Model}

It's a sequence of \textbf{superstep} where at superstep $S$:
\begin{enumerate}
    \item Compute in parallel at each $V$
        \begin{itemize}
            \item Read message sent to $V$ in $S-1$
            \item Update value/state
            \item Optionnaly change topology
        \end{itemize}
    \item Send messages to neighbors
    \item Synchronization
\end{enumerate}

\begin{tabular}{m{9cm}m{7cm}}
$\rightarrow $ Algorithm terminates when all vertices are simultaneously inactive.
& \includegraphics[width=7cm]{img/halt}
\end{tabular}

\subsubsection{Algorithm}
\begin{itemize}
    \item \textbf{Find Maximum Value}

        \begin{tabular}{m{11cm}m{5cm}}
            \begin{enumerate}
                \item At superstep 0, propagating value to neighbors
                \item In each step, if he learn a larger number, then propagate
                    it to neighbors else vote to halt
                \item Finish when all vote halt
            \end{enumerate}
            & \includegraphics[width=5cm]{img/FMV}
        \end{tabular}

    \item \textbf{PageRank}
     $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
   the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

   \begin{itemize}
       \item Each  page  $j$  distributes  its  importance  to  all  of  the  pages  it  
           points  to  (so  we  scale  by  $\frac{1}{N_j}$)
       \item Page  p’s  importance  is  increased  by  the  importance  of  its  
           back  set
       \end{itemize}
       %TODO: example pregel code ?

\end{itemize}


\subsection{Spark: Resilient Distributed Dataset}

Basically, Spark is a distributed  memory  abstraction  that  is  both  fault-­tolerant  
and  efficient. Note that there is some restriction form of distributed shared memory
in order to be called \textit{Resilient Distributed Datasets (RDDs)}
\begin{itemize}
        \item Immutable, partitioned collections of records
        \item Can  only  be  built  through  coarse-­grained deterministic  
            transformations  (map,  filter,  join,  ...)
\end{itemize}

Despite their restrictions,  RDDs can express many parallel algorithms (These
naturally  apply the same operation to many items). RDDs also unify many
programming models.

\begin{tabular}{m{7cm}cm{7cm}}
\begin{itemize}
    \item MapReduce,  DryadLINQ
    \item Pregel graph  processing
    \item Iterative  MapReduce
    \item SQL:  Hive  on  Spark  (Shark)
    \end{itemize}
    & $\Rightarrow$  &
    All are based on coarse-­grained  
    operations
\end{tabular}

\begin{center}
    \includegraphics[width=11cm]{img/databaseComp}
\end{center}

\subsubsection{Efficient fault recovery}
Spark use \textbf{lineage}:

\begin{tabular}{m{9cm}m{7cm}}
    \begin{itemize}
        \item Log  one  operation  to  apply  to  many  elements
        \item Recompute lost  partitions  on  failure
        \item No  cost  if  nothing  fails
    \end{itemize}
    &
    \includegraphics[width=7cm]{img/sparkFault}
\end{tabular}

\subsubsection{Spark operation}
\begin{itemize}
    \item Transformations (define a new RDD) : \begin{tabular}{ccc}
            map & flatMap & union \\ join & cogroup & cross \\ mapValues & filter & sample\\
            groupByKey & reduceByKey& sortByKey\\
        \end{tabular}

    \item Action (return a result to driver program) : \begin{tabular}{ccc}
            collect& reduce& count\\ save& lookupKey& take\\
        \end{tabular}
\end{itemize}

\subsubsection{Spark scheduler}

\begin{tabular}{m{7cm}m{8cm}}
    \begin{itemize}
        \item Pipelines  functions within  a  stage
        \item Locality  \&  data  reuse aware
        \item Partitioning-­aware to  avoid  shuffles
    \end{itemize}
    &
    \includegraphics[width=8cm]{img/sparkScheduler}
\end{tabular}

\subsubsection{Algorithm}
\begin{itemize}
    \item \textbf{PageRank}:
        $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
        the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

        \begin{lstlisting}[mathescape]
        var links   =  //  RDD  of  (url,   neighbors)   pairs
        var ranks   =  //  RDD  of  (url,   rank)   pairs
        for  (i $\leftarrow$ 1 to ITERATIONS)   {
            contribs =  links.join(ranks).flatMap {
                (url,   (links,   rank))   =>
                links.map(dest =>  (dest,   rank/links.size))
            }
            ranks   =  contribs.reduceByKey((x,   y)  =>  x  +  y)
            .mapValues(sum   =>  0.85*sum   +  0.15/N)
        }
        \end{lstlisting}
        Note that as RDDs are  immutable, contribs and  ranks  are  new  RDDs!

        %TODO optimizing placement slide 20 for 07
\end{itemize}


\section{Stream processing}
Many important applications must process large streams of live data
and provide results in near-­real-­time.
Distributed  stream  processing  framework  is  required  to  scale  large
clusters  (100s  of  machines) and achieve  low  latency  (few  seconds).


\begin{tabular}{m{10cm}m{6cm}}
Streaming  systems  have  a  record-at-a-time processing  model
\begin{itemize}
    \item Each node has mutable state
    \item For  each  record,  update  state   and  
        send  new  records
    \end{itemize}
    &
    \includegraphics[width=5cm]{img/stream}
\end{tabular}

Note that state  is  lost  if  node  dies, so make a stateful stream
processing  be  fault-­tolerant  is  challenging.

\subsection{Storm}

Framework  for  distributed  stream  processing which provides:  
(1) stream  Partitioning, (2) fault  tolerance  and (3)  parallel  execution.
\begin{center}
    \includegraphics[width=10cm]{img/storm}
\end{center}

\begin{itemize}
    \item \textbf{Spout} is the source of stream where \textbf{bolt} process the stream
        and output new streams
    \item Nimbus  node  (master  node): 
        \begin{enumerate}
            \item Distributes  code,  launches  workers  across  the  cluster
            \item Monitors  computation  and  reallocates  workers  as  needed
        \end{enumerate}
    \item ZooKeeper nodes (coordinate  the  cluster)
    \item Supervisor  nodes: Start  and  stop  workers  according  to  signals
        from  Nimbus
\end{itemize}

\subsubsection{Fault tolerance}
torm  can  guarantee  that  every  tuple  will  be  process  at  least  once or
at  most  once,  but  not  exactly  once. (Exactly  once  guarantee  requires
a  durable  data  source  that  can  replay  any  message  or  set  of
messages  given  the  necessary  selection  criteria)
%TODO really see slide 29 for 07 why this


If  a  supervisor  node  fails,  Nimbus  reassigns  that  node’s  task  to
other  nodes  in  the  cluster. Any  tuples  sent  to  a  failed  node  will
time  out  and  be  replayed. (Delivery  guarantee  dependent  on  a  reliable
data  source)

\subsection{Spark streaming}
Run  a  streaming  computation  as  a  series  of  
very  small,  deterministic  batch  jobs. This 
combine  the  efficiency  of  in-­memory  
distributed   processing  of  Spark  with  stream  
processing  mode.


\subsubsection{Work}

\begin{tabular}{m{10cm}m{6cm}}
\begin{itemize}
    \item Chop  up  the  live  stream  into  batches  of  X  seconds  
    \item Spark  treats  each  batch  of  data  as  RDDs  and  processes  them
        using  RDD  operations
    \item Finally,  the  processed  results  of  the  RDD  operations  are
        returned  in  batches
    \end{itemize}
    &
    \includegraphics[width=4cm]{img/sparkStream}
\end{tabular}

At the end, a new RDDs are created for every batch

\subsubsection{Fault tolerance}

\begin{tabular}{m{10cm}m{6cm}}
\begin{itemize}
    \item RDDs  remember  the  operations  that  created  them.
    \item Batches  of  input  data  are  replicated  in  memory  for
        fault-­tolerance.
    \item Data  lost  due  to  worker  failure,  can  be  recomputed  from
        replicated  input  data.
    \end{itemize}
    &
    \includegraphics[width=6cm]{img/faultSpark}
\end{tabular}


\section{Data Center Galaxy}

\subsection{Infrastructure}

\begin{tabular}{m{8cm}m{7cm}}
Streaming  systems  have  a  record-at-a-time processing  model
\begin{itemize}
    \item Servers organized in racks with a Top Of Rack (ToR) switch
    \item Aggregation switch interconnect ToR switch
\end{itemize}
    &
    \includegraphics[width=7cm]{img/dcStruct}
\end{tabular}

\subsection{Traffic}

\paragraph{Internal traffic}

Internal traffic (East-west) is clearly a critical component compared
to external traffic (North-south).
\begin{center}
\begin{tabular}{m{7cm}m{7cm}}

    \includegraphics[width=7cm]{img/internalCrit}
    &
    \includegraphics[width=7cm]{img/internalTraffic}
\end{tabular}
\end{center}

\subsubsection{Solutions}

\begin{tabular}{m{12cm}m{4cm}}
The goal is to allow each server to talk to any other server at its full access
link rate.
We can see the DC as a giant switch between server.
&
\includegraphics[width=3cm]{img/giant}
\end{tabular}

\begin{itemize}
    \item \textbf{Full Bisection Bandwith }: it's the \textit{scale up } approach.

        \begin{tabular}{m{11cm}m{7cm}}
        A traditional tree topology is really expensive
        because it requires a lot of switch with high bandwidth.
            &
            \includegraphics[width=3cm]{img/FBBW}
        \end{tabular}

        \paragraph{Oversubscription}
        A solution is to provision less
        than full Bisection Bandwidth because we can reasonably say
        that all server will know talk with all server simultaneously. 


    \item \textbf{Fat Tree}: it's the \textit{scale out} approach.

        \begin{tabular}{m{11cm}m{7cm}}
            Fat tree offers high bisection bandwidth but
            the system must be to exploit this available capacity:
            \begin{itemize}
                \item Routing must use all path
                \item Transport protocol must fill into all pipe
            \end{itemize}

            Note that this topology only require 10Gb/s links.
            &
            \includegraphics[width=3cm]{img/fattree}
        \end{tabular}
\end{itemize}

\subsection{Network stack}


\paragraph{Models}
Traditional \textbf{TCP} enforce \textit{per-flow min/max fairness}, but
Data center operators want to enforce other models such
as tenant-based or deadline-based.

\paragraph{Low latency}
\begin{itemize}
    \item 55\% of flow $<100KB$ $\Rightarrow$ Delay-sensitive
So low latency is \textbf{critical}!
    \item 5\% of flow $>10MB$ $\Rightarrow$ Throughput-sensitive
\end{itemize}

\begin{center}
\begin{tabular}{cc}
    High throughput & Low latency\\
    \hline
    Deep queue at switches& Shallow queues at switches \\
    which increase delays & which
    is bad for burst and throughput\\
\end{tabular}
\end{center}

The goal is to have in DC low queue occupancy (to achieve
low RRTs within DC approache $1µs$) and high throughput.


\subsubsection{Solutions}
\begin{itemize}
    \item \textbf{DC-TCP} which use \textbf{Explicit Congestion Notification (ECN)} 
        \begin{enumerate}
            \item React early and quickly by using ECN which avoid large
                buildup in queue and allow low latency!

                \paragraph{Switch} If queue length > $k$ set ECN bit

            \item React in proportion to the extend of congestion, not
                in presence.

                \paragraph{Sender} Maintain running average of \textbf{fraction
                of packets marked} ($\alpha$)
                $$ alpha = (1-g) \alpha + g \frac{\# marked ACK}{\# ACK} $$
               and adapts the window based on $\alpha$
               $$ W = (1-\frac{\alpha}{2}) W$$
            \end{enumerate}

        \item \textbf{pFabric}: use priorities
            \begin{enumerate}
                \item Packets carry a single priority number.

                    priority = remaining flow size (e.g., \#bytes un-acked)

                \item Switches have small queues (10 packets), send highest
                    priority and drop lowest priority

                \item Server (re)transmit aggressively it means at full link rate
                    and drop transmission rate only under extreme loss (timeout).
                \end{enumerate}
\end{itemize}

\paragraph{Flow Completion time (FCT)}: is
the time from when flow started at the sender, to when all
packets in the flow were received at the receiver.
\begin{center}
\includegraphics[width=8cm]{img/fct}
\end{center}

\subsection{Management}

\begin{description}
    \item[Data plane]: 
        \begin{tabular}{m{8cm}m{3cm}}
            process packet by using local forwarding state and packet header.
        &
        \includegraphics[width=2cm]{img/app}
    \end{tabular}
    \item[Control plane]: Compute forwarding state.

        Actually, we have a protocol which solve
        \begin{itemize}
            \item Consistent with particular low-level hardware/software
            \item  Based on entire network topology
            \item  For all routers/switches (i.e., must configure each one)
        \end{itemize}

        But it's too much, and \textbf{SDN} allow to perform some abstraction.
\end{description}

\subsubsection{Software Defined Network (SDN)}
\begin{center}
    \includegraphics[width=8cm]{img/SDN}
\end{center}

\subsubsection{Open Flow}
is a protocol for remotely controlling the forwarding table of a switch or
router which is one element of SDN.



\section{Paper}





\section{Summary Pregel: A System for Large-Scale Graph Processing}

\subsection{State of art}

Process a large graph can be done via :
\begin{description}
\item[New custom algorithm] implementation effort that mustbe repeated for each new algorithm
\item[Distributed computing platform]  ill-suited for graph processing (map reduce)
\item[Single-computer graph algorithm] doesn't scale
\item[Parallel graph system] Not fault tolerant
\end{description}

\subsection{Pregel}
Pregel computations consist of a sequence of iterations, called supersteps.


During a superstep the framework invokes a userdefined function for each vertex, conceptually in parallel. 
The function specifies behavior at a single vertex V and a single superstep S.
Messages are typically sent along outgoing edges, but a message may be sent to any vertex whose identifier is known.

\subsection{Pregel model}

A typical Pregel computation consists of input, when the graph is initialized, followed by a sequence of supersteps separated by global synchronization points until the algorithm terminates, and finishing with output.

Algorithm termination is based on every vertex voting to halt. In superstep 0, every vertex is in the active state; all active vertices participate in the computation of any given superstep. A vertex deactivates itself by voting to halt. This means that the vertex has no further work to do unless triggered externally, and the Pregel framework will not execute that vertex in subsequent supersteps unless it receives a message. If reactivated by a message, a vertex must explicitly deactivate itself again. The algorithm as a whole terminates when all vertices are simultaneously inactive and there are no messages in transit.

% TODO : Improve this graph (need edges : message received, vote to halt)
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node[draw, circle] (Active) {Active node};
        \node[draw, circle, right=of Active ] (Inactive) {Inactive node};

        \draw (Active) edge[->](Active);
        \draw (Active) edge[ ->](Inactive);
        \draw (Inactive) edge[ ->](Inactive);
        \draw (Inactive) edge[ ->](Active);
    \end{tikzpicture}
    \\
    \includegraphics[width=0.5\linewidth]{pregelprop.png}
\end{figure}

\subsection{Pregel API}
\subsubsection{Message passing}
A vertex can send any number of messages in a superstep. All messages sent to vertex V in superstep S are available. There is no guaranteed order of messages in the iterator, but it is guaranteed that messages will be delivered and that they will not be duplicated.

A vertex could learn the identifier of a non-neighbor from a message received earlier.

\subsubsection{Combiners}
Sending a message, especially to a vertex on another machine, incurs some overhead. This can be reduced in some cases with help from the user.  For example, suppose that Compute() receives integer messages and that only the sum
matters, as opposed to the individual values. In that case the system can combine several messages intended for a vertex V into a single message containing their sum, reducing the number of messages that must be transmitted and buffered.


\subsubsection{Aggregators}
Pregel aggregators are a mechanism for global communication,
monitoring, and data. Each vertex can provide a value
to an aggregator in superstep S, the system combines those
values using a reduction operator, and the resulting value
is made available to all vertices in superstep S + 1.

For instance, a sum
aggregator applied to the out-degree of each vertex yields the
total number of edges in the graph.

\subsubsection{Topology Mutations}
Just as a user’s Compute() function can send messages, it can also
issue requests to add or remove vertices or edges.

Additions follow removals, with vertex
addition before edge addition, and all mutations precede
calls to Compute(). This partial ordering yields deterministic
results for most conflicts. (compute -> remove -> addition)

\subsubsection{Input and output}
Users with unusual needs can write their own
by subclassing the abstract base classes Reader and Writer.

\subsection{Architecture}
\subsubsection{Partition}
 The default partitioning function is just hash(ID)
mod N, where N is the number of partitions, but users can
replace it.

\subsubsection{Processing}
\begin{itemize}
\item Many copies of the user program begin executing on
a cluster of machines. One of these copies acts as the
master. It is not assigned any portion of the graph, but
is responsible for coordinating worker activity. The
workers use the cluster management system’s name
service to discover the master’s location, and send registration
messages to the master.
\item The master determines how many partitions the graph
will have, and assigns one or more partitions to each
worker machine. The number may be controlled by
the user. Having more than one partition per worker
allows parallelism among the partitions and better load
balancing, and will usually improve performance. Each
worker is responsible for maintaining the state of its
section of the graph, executing the user’s Compute()
method on its vertices, and managing messages to and
from other workers. Each worker is given the complete
set of assignments for all workers.
\item The master assigns a portion of the user’s input to
each worker. The input is treated as a set of records,
each of which contains an arbitrary number of vertices
and edges. The division of inputs is orthogonal to the
partitioning of the graph itself, and is typically based
on file boundaries. If a worker loads a vertex that belongs
to that worker’s section of the graph, the appropriate
data structures (Section 4.3) are immediately
updated. Otherwise the worker enqueues a message to
the remote peer that owns the vertex. After the input
has finished loading, all vertices are marked as active.
\item The master instructs each worker to perform a superstep.
The worker loops through its active vertices, using
one thread for each partition. The worker calls
Compute() for each active vertex, delivering messages
that were sent in the previous superstep. Messages are
sent asynchronously, to enable overlapping of computation
and communication and batching, but are delivered
before the end of the superstep. When the worker
is finished it responds to the master, telling the master
how many vertices will be active in the next superstep.
This step is repeated as long as any vertices are active,
or any messages are in transit
\item After the computation halts, the master may instruct
each worker to save its portion of the graph.
\end{itemize}

\subsubsection{Fault tolerance}
Fault tolerance is achieved through checkpointing. At the
beginning of a superstep, the master instructs the workers
to save the state of their partitions to persistent storage,
including vertex values, edge values, and incoming messages;
the master separately saves the aggregator values.

If a worker does not receive a ping message after a specified interval, the worker
process terminates. If the master does not hear back from
a worker, the master marks that worker process as failed.
When one or more workers fail, the current state of the
partitions assigned to these workers is lost. The master reassigns
graph partitions to the currently available set of workers,
and they all reload their partition state from the most
recent available checkpoint at the beginning of a superstep S. 


\end{document}
