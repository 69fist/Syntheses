\documentclass[en]{../../../eplsummary}

\hypertitle{cloud-INGI2145}{7}{INGI}{2145}
{Houtain Nicolas}
{Canini Marco}

\section{Introduction}

\subsection{Computing at scale}

Cloud need for scalability because modern application require
huge amounts of processing and data. Cluster (room-sized) and datacenter 
(building-sized) can provide the resources needed.
They are composed of \textbf{rack} which is a aggregation of storage devices, many
nodes and switch to connect nodes together. Unfortunately, they are not perfect.
\begin{enumerate}
  \item Difficult to dimension because they must be provisioning for the peak load
  \item Expensive in hardware invest, expertise (ex: special software) and maintenance
  \item Difficult to scale because adding new machines is not easy
\end{enumerate}

\subsection{Cloud computing}

\begin{center}
\textit{Cloud computing is a model for enabling convenient, 
on-Â­demand network access to a shared pool of configurable 
computing resources (e.g., networks, servers, storage, 
applications, and services) that can be rapidly provisioned 
and released with minimal management effort or 
service provider interaction.}
\end{center}

$\rightarrow$ On-demand self service, Broad network access, Resource pooling, Rapid elasticity, Measured service.

\subsubsection{Model}
Cloud computing is a business models where everything is a service:
\begin{itemize}
  \item SaaS: Software as a service
  \item PaaS: Platform as a service
  \item IaaS: Infrastructure as a service
\end{itemize}

\subsubsection{Types}
There also have three types of cloud : 
\begin{itemize}
  \item Public: commercial commercial service open to almost anyone.
  \item Community: shared by several similar organization
  \item Private: shared within a organization
\end{itemize}
In this course we focus on public cloud.

\subsubsection{Applications}

Typically, applications that involve large amounts of computation, storage,
bandwidth Especially when lots of resources are needed quickly or load varies
rapidly.


\subsubsection{Virtualization}
IS used to simulate multiple physical machine for the consumer with different
capabilities. It's powerful for security and isolation because VM cannot influence
other. In the other hand, performance is hard to predict because other VM run on
the same physical machine.

\subsubsection{Challenge}
\begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
\begin{itemize}
  \item Availability
  \item Data lock-in (moving data)
  \item Data confidentiality and auditability 
  \item Data transfer bottlenecks
  \item Performance unpredictability for VM
\end{itemize}
&
\begin{itemize}
  \item Scalable storage
  \item Bugs in large distributed systems
  \item Scaling quickly
  \item Reputation fate sharing
  \item Software licensing
\end{itemize}
\end{tabular}

\section{Design for scale}

A system is scalable if it can easily adapt to 
increased (or reduced) demand.

\subsection{Parallelism programming}

\subsubsection{Vocabulary}
\begin{description}
  \item[Parallelism]: refers to techniques to make programs faster by
    performing several computations in parallel.

  \item[Concurrency]: is the composition of independently executing
    computations.
\end{description}

\subsubsection{Parallelization}
\begin{itemize}
  \item \textbf{Amdahl's law}:

    \begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
    $$\quad S = \frac{1}{\alpha +
    \frac{1-\alpha}{N}}$$ where $\alpha$ is the sequential part and $N$ the
    number of cores.
    &
    \includegraphics[width=6cm]{img/amdahl.png}
  \end{tabular}

  The coarse-grain (opposited to fine-grain) parallelism is more efficient
  because he limits the communication and coordination overheads by allow
  bigger task.
  \item \textbf{Dependencies}: Some task need to be after other which limits the degree of 
    parallelism. $\rightarrow$ Scheduling problem
\end{itemize}

\subsubsection{Synchronization and consistency}

\begin{itemize}
  \item \textbf{Sequential consistency}: the result is the same as if the
    operation are done in sequential order.

  \item \textbf{Strong consistency}: After update completes, all subsequent
    accesses will return the updated value

  \item \textbf{Weak consistency}: After update completes, accesses do not
    necessarily return the updated value;; some condition must be
    satisfied first (such as update needs to reach all the replicas)

  \item \textbf{Eventual consistency}: Specific form of weak consistency: If
    no more updates are made to an object, then eventually all reads will
    return the latest value
\end{itemize}

\subsubsection{Architecture}
\begin{itemize}
  \item Symmetric multiprocessing (SMP): all processors share same memory.
    
    \begin{tabular}{m{0.6\linewidth}m{0.4\linewidth}}
      \begin{itemize} 
        \item[+] Simplicity and easy to load balance 
        \item[-] Scalability limited and expensive
      \end{itemize}
      &
      \includegraphics[width=5cm]{img/SMP}
    \end{tabular}

  \item Non Uniform memory architecture (NUMA)
    
    \begin{tabular}{m{0.6\linewidth}m{0.4\linewidth}}
      \begin{itemize} 
        \item[+] Better scalability and faster memory
        \item[-] Complicates programming and scalability limited
      \end{itemize}
      &
      \includegraphics[width=5cm]{img/NUMA}
    \end{tabular}

  \item Shared Nothing
    
    \begin{tabular}{m{0.6\linewidth}m{0.4\linewidth}}
      \begin{itemize} 
        \item[+] Nice scalability
        \item[-] Requires different programming model
      \end{itemize}
      &
      \includegraphics[width=5cm]{img/Nothing}
    \end{tabular}

\end{itemize}


\subsection{Distributed programming}

\subsubsection{Vocabulary}
\begin{description}
  \item[Faults]: Some component is not working correctly
  \item[Failure]: System as a whole is not working correctly
\end{description}

\subsubsection{Wide-area network}
The transfert speed for some data are defined by some attributs:
\begin{enumerate}
  \item Propagation delay
  \item Bottlenecks capacity on the path
  \item Queueing delay, loss, reordering, congestion, rtt 
    (take in account by TCP)
\end{enumerate}

$\rightarrow$ wide-area networks complicates the communication and faults are
more common

\subsubsection{Faults}

Fault are a common event in distributed system and some faults
are correlated.

\begin{itemize}
  \item \textbf{Crash faults}: node simply stop
  \item \textbf{Rational behavior}: owner manipulates node to increase profit
    (ex: lies on the routes to have traffic through it's own AS)
  \item \textbf{Byzantine faults}: faulty node could do anything (ex: stop, send spam,
    attack other, tell lies,...)
\end{itemize}

To prevent fault, we can \textit{prevent} them by using verification,
\textit{detect} them or \textit{mask} them by using replicas. The problem of
using \textbf{replicas} is to be able to maintain consistency between them!

\subsubsection{Consistency}

\begin{itemize}
  \item \textbf{Strong consistency}: After update completes, all subsequent
      accesses will return the updated value

  \item \textbf{Weak consistency}: After update completes, accesses do not
      necessarily return the updated value;; some condition must be satisfied
      first (such as update needs to reach all the replicas)

  \item \textbf{Eventual consistency}: Specific form of weak consistency: If no
      more updates are made to an object, then eventually all reads will return
      the latest value

  \item Causal consistency: If client A has communicated to client B that it
      has updated a data item, a subsequent access by B will return the updated
      value, and a write is guaranteed to supersede the earlier write. Client C
      that has no causal relationship to client A is subject to the normal
      eventual consistency rules

  \item Read-your-writes consistency:  Client A, after it has updated a data
      item, always accesses the updated value and will never see an older
      value.

  \item Session consistency: Like previous case but in the context of a
      session, for as long as the sessions remains alive.

  \item Monotonic read consistency: If client A has has seen a particular value
      for the object, any subsequent accesses will never return any previous
      values

  \item Monotonic write consistency: In this case the system guarantees to
      serialize the writes by the same process
\end{itemize}

We can combine them, and monotonic reads + read-your-write are most desirable
than eventual consistency.


\paragraph{Storage system example}


\begin{tabular}{m{5cm}m{12cm}}
    \includegraphics[width=3cm]{img/replicas} &
    \includegraphics[width=7cm]{img/replicas2}
    \begin{itemize}
        \item[$\rightarrow$] Strong consistency
    \end{itemize} \\
\end{tabular}


\paragraph{Consensus}
\begin{enumerate}
    \item Clients send requests to each of the replicas
    \item Replicas coordinate and each return a result
    \item Client chooses one of the results, e.g., the one that is 
        returned by the largest number of replicas
    \item If a small fraction of the replicas returns the wrong result, or 
        no result at all, they are 'outvoted' by the other replicas
\end{enumerate}


\subsection{CAP theorem}
%TODO tikz
\includegraphics[width=5cm]{img/CAP}

\begin{itemize}
    \item C\&P -> Many replicas + consensus
    \item A\&P -> Many replicas + relaxed consistency
\end{itemize}

Actually, we have a lot of partition and we have a trade-off
between C and A. \textbf{ACID} (atomicity, consistency, isolation, durability)
become \textbf{BASE} (basically available, soft-state, eventually consistent).




\section{Paper}



\end{document}
