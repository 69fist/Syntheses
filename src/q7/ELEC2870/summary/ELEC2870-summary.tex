\documentclass[en]{../../../eplsummary}
\usepackage{../../../eplcode}

\usepackage{esdiff}

\hypertitle{Machine Learning}{7}{ELEC}{2870}
{Guillaume Derval}
{Michel Verleysen \& John Lee}

\section{Linear regression}
\begin{itemize}
\item \textbf{Main notation}: $y = w^tx$, with $x=(1,x_1,x_2,...,x_N)^t$ and $w=(w_0,w_1,w_2,...,w_N)^t$.
\item \textbf{Error criterion (most common)}: $\frac{1}{P}\sum_{p=1}^P(t_p - w^t x_p)^2=\frac{1}{P}\|t-w^t X\|^2$
\end{itemize}
\subsection{Pseudo-inverse}
\begin{itemize}
\item \textbf{Derivative of error criterion}: $\diffp{E}{w}^t = \frac{2}{P}(w^tX-t^t)X^t$
\item \textbf{Minimum error at}: $w=(XX^t)^{-1}Xt$
\end{itemize}
\subsection{Gradient descent}
\begin{itemize}
\item \textbf{Gradient descent}: $x(t+1) = x(t)-\alpha \diffp*{x}{t}{x(t)}$
\item \textbf{Gradient descent on error criterion}: $w(t+1) = w(t) - \frac{2\alpha}{P}X(t-X^tw(t))$
\item \textbf{Stochastic gradient descent}: As we can write that $E=\frac{1}{P}\sum_{p=1}^PE_p$, we can optimize the $E_k$ one at a time. $w(t+1) = w(t) - \frac{2\alpha}{P}(t-x_k^tw(t))x_k$
\end{itemize}
\subsection{Linear associative memory}
\begin{itemize}
\item Learning: $w=\sum_{p=1}^P t_px_p$.
\item Magic: iff $x$'s are orthogonal and normalized.
\end{itemize}
\subsection{Perceptron}
\begin{itemize}
\item With $sgn$ as activation function, classification model ($-1$ or $1$)
\item Object is correctly classified if $w^tx_kt_k > 0$
\item \textbf{Ideal error criterion}: $E = -\sum_{w^tx_kt_k < 0} 1$ (not continuous)
\item \textbf{Perceptron criterion}: $E = -\sum_{w^tx_kt_k < 0} w^tx_kt_k$ (continuous, but gradient is only piece-wise linear)
\item \textbf{Perceptron convergence theorem}: If a set is linearly separable, then gradient descent on the perceptron criterion will converge in a finite number of steps. Proof: verify that $\|w\|$ grows slower than $\sqrt{n}$ and that $w^T_{SOL}w$ is bounded below by a linear function of $n$.
\end{itemize}
\section{Multi-layer perceptron}
\subsection{Single layer}
\begin{itemize}
\item \textbf{Activation function}: $\sigma(w^tx)$
\item \textbf{Error function}: $E = \frac{1}{P}\sum_{p=1}^P(t_p - \sigma(w^t x_p))^2$
\item \textbf{Stochastic gradient descent rule}: $w(t+1) = w(t) - \frac{2\alpha}{P}(t-x_k^tw(t))x_k \diffp*{\sigma}{p}{w(t)^t x_k}$
\end{itemize}
\subsection{Multi-layer}
\begin{itemize}
\item \textbf{Model}:  $y(x) = h(w^{(2)}g(w^{(1)}x))$, $h$ can be linear, but not $g$.
\item \textbf{Universal approximation property}: 2-layer MLP can approximate everything to an arbitrary precision if there are enough hidden units.
\item \textbf{Learning}: Algorithm
	\begin{enumerate}
		\item Apply an input vector $x_k$ through the network, computing all values
		\item Evaluate error  on the last layer (difference with $t_k$)
		\item Back-propagate the error on each layer
		\item Compute all the derivatives of $E$
		\item Gradient descent step
	\end{enumerate}
\end{itemize}
\subsection{Improvement to gradient descent}
\begin{itemize}
\item \textbf{Momentum}: add $\beta(w(t)-w(t-1))$ after the gradient descent step ($\beta\approx 0.9$)
\item \textbf{Adaptive learning rate}: making $\alpha$ a function, increasing (resp. decreasing) by a constant when the new direction is nearly the same as previous one (resp. not the same) (scalar product is $> 0$)
\end{itemize}
\subsection{Weight adjustment}
TODO
\section{PCA}
\subsection{Background}
\begin{itemize}
\item \textbf{Decorellation}: Data $(X,Y)$ are decorellated iff $C_{X,Y}$ is in the form of $\begin{pmatrix}
   \sigma_X^2 & 0 \\
   0 & \sigma_Y^2
\end{pmatrix}$
\item \textbf{Whiteness}: Data $(X,Y)$ is white iff $C_{X,Y}$ is the identity matrix and $|X|=|Y|=0$.
\item \textbf{Whitening}: find a transformation $V$ for $C_{XX}$ that whitens it. Works for $V=\Lambda^{-\frac{1}{2}} \Theta^2$, where $\Lambda$ is the eigenvalues of $C_{XX}$ and $\Theta$ are the eigenvectors.
\end{itemize}
\subsection{PCA Axes}
\begin{itemize}
\item Find axes that maximise the variance after projection
\item Best choice at each step is the eigenvector with the maximum eigenvalue
\item \textbf{Variance kept}: sum of the eigenvalues taken / sum of all the eigenvalues
\item \textbf{Error}: sum of the eigenvalues not taken / sum of all the eigenvalues
\item \textbf{Whiteness invariance}: rotations are white too (rotation matrix $T$ is orthogonal)
\end{itemize}
\section{Vector Quantization}
\subsection{Background}
\begin{itemize}
\item \textbf{Goal}: find $Q$ vectors(\textbf{centroids}) ($Q<P$), simplifying the database, while minimizing the loss of information.
\item \textbf{Voronoi zone} (around a centroid): Subset of the space that lies closer to this centroid than to any other.
\item \textbf{Minkowskiâ€™s $L_P$ norm} $d_p(x_i,y_j) = (\sum_{k=1}^D x_{i,k}^p - y_{j,k}^p)^{1/p}$
\item \textbf{Euclidian distance} $d_2$ -> least square error.
\item \textbf{Weighted distances} $d_W(x_i,y_i) = (x_i-y_i)^tW(x_i-y_i)$.
	\begin{itemize}
		\item \textbf{Mahalanobis distance} $W=C_{XY}$
		\item  \textbf{$W$ symmetrical} $d_W(x_i,y_j) = d_2(Px_i,Py_j)$ with $W=P^tP$
	\end{itemize}
\end{itemize}
\subsection{Lloyd's principle}
\begin{itemize}
\item \textbf{Lloyd's Principle} VQ is a two stage process: $x_i \rightarrow \text{encoder } \alpha \rightarrow j \rightarrow \text{decoder } \beta \rightarrow y_j$.
\item \textbf{$\alpha$ known}: $\beta(j) = argmin_{y_j}(E(d(x_i,y_j)|\alpha(x^i)=j))$ (center of mass)
\item \textbf{$\beta$ known}: $\alpha(x_i) =  argmin_j(d(x_i,y_j)) \text{ where } y_j =\beta(j)$ (nearest neighbour)
\item \textbf{Lloyd's algorithm}
	\begin{enumerate}
		\item Choice initial centroids
		\item Evaluate current Error
		\item If Error small enough, stop
		\item Replace all centroids $y_j$ with the center-of-mass of the values associated to it
		\item Go back to step 2
	\end{enumerate}
	\begin{itemize}
		\item \textbf{Hypothesis}: Probability of finding a point on the border of a voronoi zone is $0$
		\item \textbf{Limitation}: Local minimum of the error
	\end{itemize}
\item \textbf{Initialisation of Lloyd's algo}:
	\begin{itemize}
		\item At random in the input space (no data taken into account)
		\item First $Q$ points of $X$
		\item Randomly chosen points in $X$
		\item Product codes: product of scalar quantizers
		\item Growing initial set (choose randomly in $X$, but keep only if distance is sufficient)
		\item Pairwise nearest neighbor (begin with all $X$, and at each step merge the two closest centroids). Variant: merge the two centroids giving the lowest increase of error.
		\item Splitting: Begin with two centroids, apply Lloyd's, then disturb them with two new, etc.
	\end{itemize}
\end{itemize}
\subsection{Competitive learning}
\begin{itemize}
	\item For each $x_i$, change the position of the nearest centroid: $y_k(t+1)=y_k(t)+\alpha(x_i-y_k)$
	\item $E = \int(x-y_{j(x)})^2p(x) dx$
	\item Robbins-Monro conditions on $\alpha$: $\sum_{t=0}^\infty \alpha(t) = \infty$ and $\sum_{t=0}^\infty \alpha(t)^2 < \infty$
	\item Local minima, lose centroids
	\item \textbf{Frequency-sensitive learning}: multiply the distances by a function $u$ that is incremented each time this centroid is selected
\end{itemize}
\subsection{Learning Vector Quantization}
\subsubsection{LVQ1}
\begin{itemize}
\item With classes
\item Same algo as competitive learning if centroid and $x_i$ are in the same class: $y_k(t+1)=y_k(t)+\alpha(x_i-y_k)$
\item But increase distance of centroids of different classes than $x_i$: $y_k(t+1)=y_k(t)-\alpha(x_i-y_k)$
\end{itemize}
\subsubsection{LVQ2}
\begin{itemize}
\item Try to reaches Bayes(optimal) boundary
\item Selection of two winners
\item Move the two winners if
	\begin{enumerate}
		\item One winner($y_{a}$) is in the same class as $x_i$ AND
		\item The other one($y_{b}$) is not AND
		\item $x_i$ is in between the two centroids, in a window of fixed size
	\end{enumerate}
\item Move: $y_a(t+1)=y_a(t)+\alpha(x_i-y_a(t))$ and $y_b(t+1)=y_b(t)+\alpha(x_i-y_b(t))$
\end{itemize}
\subsubsection{Improvements}
\begin{itemize}
\item \textbf{Forgetting factor}: progressively ignore data for from boundary
\item \textbf{Dynamic Vector Quantization}: Add centroids if needed ($x_i$ of class 1 is the nearest point to centroid of class 2 or distance between point and centroid too wide)
\end{itemize}
\subsection{Winner-take-all/Winner-take-most}
\begin{itemize}
\item adapt the winner and other centroids as well
\item Example: \textbf{Neural gas}: add a factor $e^{-h(j,x_i)/\lambda}$ on the updates, where $h$ is the the number of the centroid when ordered by distance.
\end{itemize}
\section{Radial-basis function networks}
\begin{itemize}
\item \textbf{Cover's theorem}: The probability of $\phi$-separability ($\exists \text{vector} w$ that linearly separate $\phi(X)$) tend to one if $\phi_i$ are not linear and more than the number of dimensions
\item \textbf{RBFN}: We search for $F:R_D\rightarrow R$ such that $F(x_p)=t_p \forall p \in P$. \\\
Solution (with regularization $\lambda$): $$F(x) = \sum_{p=1}^P w_pG(x,x_p)$$ $$w=(G+\lambda I)^{-1}t$$ ($G+\lambda I$ non-singular if $x_k$ distincts (Michelli)).
\item \textbf{Generalized RBFN}: $$F(x) = \sum_{i=1}^K w_p\phi(\|x-c_i\|)$$ $$\phi(\|x-c_i\|)=exp(-\frac{\|x-c_i\|^2}{2\sigma_i^2})$$
\item \textbf{Park \& Sandberg}: For any function $f$, it exists a function $F$ with an unspecified $K$ that approximates $f$ as good as willed.
\item \textbf{Learning}:
	\begin{enumerate}
		\item \textbf{Centers} $c_i$: use Vector quantization to find the centers ($c_i$ are centroids)
		\item \textbf{Widths} $\sigma_i$: choose them according to the standard deviation around the centroids
		\item \textbf{Weigths} $w_i$: with $c_i$ and $\sigma_i$, finding $w_i$ becomes linear. Simply use pseudo-inverse or SVD.
	\end{enumerate}
\item \textbf{Standard deviation of the widths}:
	\begin{itemize}
		\item $\sigma = \frac{d_{max}}{\sqrt{2K}}$ where $d_{max}$ is the max distance between centroids
		\item $\sigma_i = \frac{1}{q}\sqrt{\sum_{j=1}{q}\|c_i-c_j\|^2}$ scan the $q$ nearest centroids
		\item $\sigma_i = r \min_j(\|c_i-c_j\|)$ where $r$ is an overlap constant
	\end{itemize}
\item \textbf{Improvements}: 
	\begin{itemize}
		\item Optionnal computation step: gradient descent on all parameters
		\item Add constant and linear terms to $F(x)$
		\item Normalize the $\phi_i$ to make them bounded to $[0,1]$
	\end{itemize}
\end{itemize}
\section{Model selection}
\begin{itemize}
\item Goal: optimize the hyper-parameters of the model
\item \textbf{AIC} $\hat{E}_{gen}(\phi)=E_\phi(X,t)+\frac{2}{N}\dim(\phi)$
\item \textbf{BIC/MDL} $\hat{E}_{gen}(\phi)=E_\phi(X,t)+\frac{\ln(N)}{N}\dim(\phi)$
\item \textbf{Validation}: use a validation set
\item \textbf{Cross-validation}: repeat validations
\item \textbf{K-fold}: Divide in fold of fixed size $=N/K$. Learn with $K-1$ fold, test with the remaining one, then change the fold use to test, $K$ times.
\item \textbf{Leave-one-out}: K-fold with $K=N$.
\end{itemize}
\subsection{Bootstrap}
\begin{itemize}
\item \textbf{Plug-in principle}: use the empirical distribution of the parameter in a sample instead of the true distribution
\item \textbf{Notation $E_{A,B}$}: $A$ is the learning set, $B$ is the test set
\item \textbf{Bootstrap}: We want to approximate the error of our model on the real population ($E_{S,P}$). We try to find $\hat{E} = E_{S,P} = E_{S,P} + E_{S,S}-E_{S,S} = E_{S,S} + (E_{S,P}-E_{S,S} \approx E_{S,S} + (E_{S*,P*}-E_{S*,S*}) = E_{S,S} + \text{mean of error on random sub-samples}$
\item \textbf{Bootstrap .632}: Compute bootstrap replication with $P*=\bar{S*}$. $\hat{E}=0.368E_{S,S}+0.632*\hat{\text{optimism}}$
\end{itemize}
\subsection{Confusion Matrix}
\begin{itemize}
\item \textbf{Confusion matrix}: Matrix where entry $c_{ij}$ is the number of data of class $j$ that have been classified in $i$
\item \textbf{Bayes confusion matrix}: optimal confusion matrix using Bayes boundary
\end{itemize}
\subsection{Pruning}
\begin{itemize}
\item \textbf{Direct pruning}: remove neurons/weight if it is not useful (output fixed, correlated with another, random,...). Dangerous.
\item \textbf{Local least squares}: When removing a weight, compensate the difference by modifying the other weights.
\item \textbf{Optimal brain damage}: When removing a weight, assuming that learning to tend minimize $E$ and $E$ is quadratic, we have that $\delta E=\diffp[2]{w_i}{E}(w_i)^2$. Delete only the weight with minimum $\delta E$.
\item \textbf{Optimal brain surgeon}: Same, but compensate with other weights
\end{itemize}
\section{Feature selection}
\begin{itemize}
\item Most of the data, we have lot of dimensions, and not enough data
\item Distance means "nothing" in high-dimensional data
\item \textbf{Reducing the dimensionality}
\end{itemize}

\subsection{Filters}
\begin{itemize}
\item Does not use models.
\item \textbf{Correlation}: $r=\frac{\sum_{j=1}^N((x_j-\bar{x})(y_j-\bar{y}))}{\sqrt{\sum_{j=1}^N((x_j-\bar{x})^2)\sum_{j=1}^N((y_j-\bar{y})^2)}}$

\item \textbf{Entropy}: measure of uncertainty of a random variable: $H(x) = -E[\log(P[x])]$.
\item \textbf{Mutual information}: $I(y;x) = H(y)-H(y|x)=H(x)-H(x|y)$
\item \textbf{Relevance}: $I(x_i,t)$ (trying to maximise)
\item \textbf{Redundancy}: $I(x_i,x_j)$ (trying to minimise)
\item \textbf{Multi-objective optimisation}
\end{itemize}
\subsection{Wrappers}
\begin{itemize}
\item Uses models (select features that makes the model behave better)
\item Slower than filters
\item Relevance criterion easy to estimate
\item Less intuitive than filters
\end{itemize}
\subsection{Greedy search method}
\begin{itemize}
\item Testing $2^d$ subset is too much
\item Be greedy
\item Initial subset, simple update strategy + stop strategy
\item \textbf{Forward search}: begin with the empty subset, add the best feature, then stop when it decreases the performance
\item \textbf{Backward search}: idem, but with all the features as initial subset, then remove features
\item \textbf{Forward-backward}: allow to add and remove variable
\item \textbf{Genetic algorithms}
\end{itemize}
\subsection{Embedded methods}
\begin{itemize}
\item Feature selection + Model selection together
\end{itemize}
\section{Self-Organizing Maps}
\begin{itemize}
\item VQ with a supplementary grid space
\item \textbf{Topology conservation}: if centroids lie closely in the grid space, they are more likely to lie closely in data space
\item \textbf{Adaptation of weights}: Take the winner $k=\text{argmax}_i(w_i^tx)$ and update the weights: $$w_j(t+1)= w_j(t) + \alpha(t)(x-w_j(t))V(k,j)$$
\item $\alpha(t)$ and $r(t)$ decrease over time.
\item $\alpha$ must follow Robbins-Monro conditions
\item \textbf{Hard neighbourhood}: $$V(k,j)=\left\{ \begin{array}{l l}
1 & \text{if} d(k,j)<r(t) \\
0 & \text{otherwise}
\end{array} 
\right.$$
\item \textbf{Soft neighbourhood}: $V(k,j)=exp(-\frac{d^2(k,j)}{r^2(t)})$
\item \textbf{Butterfly effect}: If $r(t)$ decreases too fast with respect to $\alpha(t)$
\item \textbf{Pinch effect}: If $r(t)$ decreases too fast
\end{itemize}
\subsection{Measures of quality}
\begin{itemize}
\item \textbf{Quality of VQ}: use VQ techniques
\item \textbf{Quality of Organization}: $dw-dr$
\item $dw$: distance of centroids in weight space
\item $dr$: distance between centroids in grid space
\item $dw-dr$: should be a straight line
\end{itemize}
\section{Support Vector Machines}
\subsection{Linear}
\begin{itemize}
\item Decision surface: hyperplane
\item Tries to maximise margin
\item \textbf{Hard-margin} Find $w,b$ such that $\|w\|^2$ is minimised and $y_i(w^tx_i+b)\geq 1, \forall x_i$. Quadratic, convex, simple to compute
\item \textbf{Soft-margin} Allow errors, but penalize them with a parameter $C$. Constraint: $y_i(w^tx_i+b)\geq 1-\epsilon_i, \forall x_i$, with $\epsilon_i$ the (positive) error. Trying to minimise $\|w\|^2+C\sum_i\epsilon_i$.
\item Minimizing the number of misclassification is NP-complete
\end{itemize}
\subsection{Non-linear with Kernel trick}
\begin{itemize}
\item We try to find a map $\Phi(x)$ to allow to find non-linear surfaces: $y_i(w^t\Phi(x_i)+b)\geq \epsilon_i, \forall x_i$
\item \textbf{Dual of the problem}:
$$\min_{\alpha_i}\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jt_it_j(\Phi(x_i)^t\Phi(x_j))-\sum_i\alpha_i$$
$$\text{s.t.} 0 \leq \alpha_i \leq C, \forall x_i$$
$$\sum_i \alpha_i t_i=0$$
\item \textbf{Kernel Trick}: We can find $K(x_i,x_j)=\Phi(x_i)^t\Phi(x_j)$, that does the product without "directly" mapping
\item $\text{sgn}(w^t\Phi(x)+b)=\text{sgn}(\sum_i\alpha_it_iK(x_i,x)+b)$ allows to classify
\item $b$ can be extracted by solving $\alpha_j(t_j\sum_i\alpha_it_iK(x_i,x_j)+b-1)=0 \text{ for a } j \text{ s.t. } \alpha_j \neq 0$
\end{itemize}
\subsection{Kernels}
\begin{itemize}
\item \textbf{Polynomial kernel of degree $p$}: $K(x,z)=(x^tz+1)^p$
\item \textbf{Gaussian kernel}: $K(x,z)=exp(-\frac{\|x-z\|}{2\sigma^2})$ (parameter $\sigma$)
\item \textbf{Kernels must be symmetric}
\item \textbf{Any symmetric function does not involves the existence of an associated mapping}
\item \textbf{Gram matrix}: matrix where $G_{ij}=K(x_i,x_j)$
\item \textbf{Mercer condition}: a mapping $\Phi(x)$ exists when $G$ is semi-positive definite
\end{itemize}
\subsection{Comparison with MLP}
\begin{tabular}{l|l|l}
& MLP & SVM \\
\hline
Mapping to & Moderate dimensional space & Very high dimensional space \\
Unique minimum? & No & Yes \\
Training & Expensive & Efficient \\
Classification & Efficient & Efficient \\
(Hyper-)parameters & Number of hidden layers, units & kernel, kernel hp, $C$\\
Robust & Moderately & Yes
\end{tabular}
\section{Independent component analysis}
\begin{itemize}
\item \textbf{Make features independent}
\item \textbf{Independance} between $X,Y$: $E[f(X)g(Y)]=E[f(X)]E[g(X)] \forall\ f,g$, $P(X|Y)=P(X)P(Y)$
\item Try to find independent unknown signal $s(t)$ by applying a matrix $W$ to the measured signals $x(t)=As(t)$ ($A$ unknown). We must find $W\approx A^{-1}$.
\item \textbf{Hypothesis}: mixture linear and additive (matrix $A$), random variables are signals, no delays, $E[s_is_i^t]=1$, $A$ constant over time
\item To find $W$, we try to make $y=Wx$ independent.
\item \textbf{Indeterminacies}: order of signals, scaling factor
\item \textbf{Whitening}: whitening $x$ allows to reduces $W$ to an orthogonal matrix ($VA$ is orthogonal), reducing the number of parameters to $n(n-1)/2$ from $n^2$.
\end{itemize}
\subsection{Non-gaussian approach}
\begin{itemize}
\item PDF of a sum of $n$ independant random variables tends to be Gaussian
\item Try to find $W$ such that output of PDF are as different as possible from Gaussian
\item \textbf{Minimum differential entropy} Trying to minimise entropy of $y$
\item \textbf{Negentropy}: Difference of entropy with a gaussian: $J(x)=\int p_x(u)\log\frac{p_x(u)}{p_{x_G}(u)}\ du$. Maximise $J(y)$.
\item \textbf{Kurtosis}: $\kappa_4(X)=E[X^4]-3E[X^2]^2$. For Gaussian functions, $\kappa_4(X_G)=0$. For others, $\neq 0$. Try to maximise $\sum_{i=1}^m|\kappa_4(Y_i)|$.
\item \textbf{Gram-Charlier Expansion} approximate PDF around a gaussian PDF
\end{itemize}
\subsection{Minimum dependance approach}
\begin{itemize}
\item \textbf{Minimise mutual information} (need joint PDF)
\item \textbf{Minimise sum of entropy}
\end{itemize}
\subsection{Practice}
\begin{itemize}
\item We do not know PDF
\item Density estimation to approximate PDF
\item Direct estimation of the independance (cross-cumulants)
\end{itemize}
\end{document}
