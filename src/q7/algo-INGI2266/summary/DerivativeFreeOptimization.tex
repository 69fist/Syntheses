Usually when we try to optimize a function with a continuous domain we use derivative.
However it is not always possible because:\newline
1) The gradient is not computable\newline
2) We dont know the objective function\newline
3) The gradient is to expensive to compute\newline
4) We don't know how to compute the gradient\newline

We can use Derivative free optimization.
We dont know the objective function but we have comparison function that allows us to compare solutions.

\subsection{Grid Search}
Since the domain is continuious we can't compare every point. To solve this problem we will cut each dimension of the domain in m. We evaluate eaach one of these combination and we output the minimum.
We will always have $m^n$ evaluation(because we have n dimensions that we cut in m parts).
We could do a smarter grid search but iteratively perform a grid search and then reduce our domaine for the minimum output area we have found.
So if we do i iterations we will have $i * m^n$ evaluations od the objective.
\subsection{Directional Direct Search}
Global idea\newline
Test a sample of points in specified directions around the iterate.
If a point is better, select it as next iterate.\newline
This algorithm consiste of two steps:\newline
1)Poll step:\newline
We test the different possibilities and go to a better one(with our comparaison function)\newline
2)Mesh parameter update:\newline
We update the size of the step we want to perform.\newline
\subsubsection{Poll step}
We define a set D of positive bases that is going to define the direction we will explore.
Exemple:\newline
For a 3D space D is :\newline
$e_1$ = (1,0,0)\newline
$e_2$ = (0,1,0)\newline
$e_3$ = (0,0,1)\newline

How to build a new point:\newline
$x_(new) = x+\alpha*d (with d in D)$
\subsubsection{Mesh Parameter Update}
Two cases:\newline
1)Iteration declared sucessfull(we found a better solution with the comparaison function)\newline
2)


