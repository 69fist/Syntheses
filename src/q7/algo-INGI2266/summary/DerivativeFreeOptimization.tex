Usually when we try to optimize a function with a continuous domain we use derivative.
However it is not always possible because:\newline
1) The gradient is not computable\newline
2) We dont know the objective function\newline
3) The gradient is to expensive to compute\newline
4) We don't know how to compute the gradient\newline

We can use Derivative free optimization.
We dont know the objective function but we have comparison function that allows us to compare solutions.

\subsection{Grid Search}
Since the domain is continuious we can't compare every point. To solve this problem we will cut each dimension of the domain in m. We evaluate eaach one of these combination and we output the minimum.
We will always have $m^n$ evaluation(because we have n dimensions that we cut in m parts).
We could do a smarter grid search but iteratively perform a grid search and then reduce our domaine for the minimum output area we have found.
So if we do i iterations we will have $i * m^n$ evaluations od the objective.
\subsection{Directional Direct Search}
Global idea\newline
Test a sample of points in specified directions around the iterate.
If a point is better, select it as next iterate.\newline
This algorithm consiste of two steps:\newline
1)Poll step:\newline
We test the different possibilities and go to a better one(with our comparaison function)\newline
2)Mesh parameter update:\newline
We update the size of the step we want to perform.\newline
\subsubsection{Poll step}
We define a set D of positive bases that is going to define the direction we will explore.
Exemple:\newline
For a 3D space D is :\newline
$e_1$ = (1,0,0)\newline
$e_2$ = (0,1,0)\newline
$e_3$ = (0,0,1)\newline

How to build a new point:\newline
$x_(new) = x+\alpha*d $(with d in D)
\subsubsection{Mesh Parameter Update}
Two cases:\newline
1)Iteration declared sucessfull(we found a better solution with the comparaison function)\newline
We increase $\alpha$\newline
2)Iteration declared unsucessfull\newline
We decrease $\alpha$\newline

\subsubsection{Problem : Denis-Wood Coutours}
We can not go toward a better solution with contours like that.\newline
Two solutions:\newline
1)Rotate the bases after N unsuccessful iterations\newline
2)Run DDS N times with N different set of bases.\newline

\subsubsection{Complexity}
b is the number of bases.
Worst case : Nbeval = O(n)
Best case : O(1)

\subsection{The Nelder-Mead Algorithm}
Global idea\newline
Build a structure of points.\newline
At each iteration, replace the worst point in the structure.\newline
The point structure\newline
The structure of point is a polyhedron of dimension n + 1.(abusively called simplex).\newline
The simplex structure\newline
The simplex structure Y is a set of n + 1 vertices (where n is the
dimension of the search space) defined as follows:\newline
$Y = {y^0,y^1,...,y^n}$\newline
The vertice $y^0$ is a better solution than the vertice $y^1$ which is a better solution that then vertice $y^2$ ....
\subsubsection{Operations on the simplex}
1)Reflection\newline
2)Expansion\newline
3)Contration\newline
If no amelioration by doing those : Shrink.\newline

First we do a reflection with the worst point.From there we have 3 cases.\newline
1) $f^0 \leq f^r < f^{n-1}$ -> replace $y^n$ by $y^r$\newline
2) $f^r < f^0 $-> perform an expansion\newline
3) $f^r \geq f^{n-1}$ -> perform an contraction\newline

Expansion:\newline
1) if $f^e \leq f^r$ -> replace $y^n$ by $y^e$\newline
2) Else replace $y^n$ by $y^r$\newline

Contraction:\newline
1)$f^r < f^n$-> outside contraction
2)$f^r \geq f^n$ -> inside contraction

Shrink

\subsubsection{Complexity}
Reflection\newline
Nbevals = O(1)\newline
Expansion/Contraction\newline
Nbevals = O(1)\newline
Shrink\newline
Nbevals = O(n)\newline
\subsection{Multi-Directional Search}
Global idea\newline
Same point structure than in Nelder-Mead.\newline
Perform operations on all but one vertices of the simplex around
the best vertex.\newline
The point structure\newline
\newline
The structure of point is a polyhedron of dimension n + 1 (The
simplex of Nelder-Mead).\newline

\subsubsection{Operations on the simplex}
1)Rotation\newline
2)Expansion\newline

Rotation\newline
Evaluate $f^r = min{f(y^r_i)|i=,;...;n}$\newline
if $f^r$ < $f^0$ -> expansion\newline
else -> contract simplex \newline

Expansion\newline
Evaluate $f^r = min{f(y^r_i)|i=,;...;n}$\newline
if $f^e$ < $f^r$ -> The new simplex is the expanded simplex\newline
else -> cThe new simplex is the rotated simplex \newline

Shrink\newline

\subsubsection{Complexity}
Rotation\newline
O(n)\newline
Expansion\newline
O(n)\newline
Shrink\newline
O(n)

\subsection{Smart Random Algorithms}
Problem:\newline
Algorithm might converto to local minimum.\newline
Solution\newline
Restart from different points\newline

Idea:\newline
Run  N times the algirithm such as:\newline
1)For each run the algorithm begins from a random starting
point\newline
2)Random starting points following a uniform distribution\newline
3)Return the best result obtained over the N runs\newline

Problem is that those random sequence might contain gaps.\newline
To avoid that problem we use Halton Sequence.\newline
How to build a Halton sequence:\newline
Chose a base(integer) and successively decompose [0, 1] in subintervals according to the base.\newline

To build multi dimensional Halton sequence we build n 1D qequence with n different bases(prime two by two) and we mix the points.\newline
Problem with higher prime sequence (correlation problem).\newline
Solution:\newline
Shuffle one of the sequence before mixing the different sequence to build the points
