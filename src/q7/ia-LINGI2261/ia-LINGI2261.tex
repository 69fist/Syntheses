\input{../../lib.tex}

\hypertitle{en}{Artificial Intelligence}{7}{INGI}{2261}
{Nicolas Houtain\and Symeon Malengreau\and Gorby Nicolas Ndonda Kabasele}
{Yves Deville}

\textbf{ATTENTION: We should complete the content of this chapter with the content of the book}

\section{Chapitre 1 : Introduction}

\textbf{What is IA ?}

\subsection{Turing test approach}
A \textbf{AI} success this test if after ask question  a human don't know if this is
a other human or machine.

TODO

\section{Chapitre 2.1-2.3, Intelligent Agent}

\subsection{Agents and environments}
TODO

\subsection{Concept of rationality}
TODO

\subsection{Nature of environments}
TODO

\section{Chapitre 3 : Solving problems by searching}

\subsection{Key principles}

\begin{description}
\item[Search] process of looking for a (or the best) sequence of actions, that leads to a goal (specific state of the environment), starting from an initial state
\item[States] distinguishables stages during the problem solving process (representation of physical configuration)
\item[State space] is graph representation of the successor function with the cost
\item[Initial state] \textit{to complete}
\item[Action] an action transports the agent from one state to another one by applying an \textit{operator} to a state
\item[Operators] \textit{to complete}
\item[Goal test] \textit{to complete}
\item[Path cost] \textit{to complete}
\item[Solution] a solution is a sequence of actions leading from the initial state to a goal state
\item[Optimal solution] an optimal solution has the lowest path cost among all solutions
\item[Node] data structure constituting part of a search tree
\item[Frontier] set of generated nodes, which ancestors have been goal-tested (visited)
\end{description}

\subsection{Problem solving agents}
TODO

\subsection{Example}
TODO

\subsection{Searching for Solutions}

Searching for solutions is a traversal of some search space from the \textit{initial state} to the \textit{goal state} using a legal sequence of actions (as defined by operators).

\begin{enumerate}
\item Check if current state is a goal state
\item Expand the current state
\begin{enumerate}
\item Determine the set of reachable states
\item Return "failure" if the set is empty
\end{enumerate}
\item Select one state from the set of reachable states
\item Move to the selected state
\end{enumerate}

\subsubsection{Tree versus Graph Search}

There is two way to perform the search\footnote{the slides specifies an algorithm to search in a tree}.
\begin{itemize}
\item \textbf{A graph representing the state space}: you represent all the possible states as a graph, and you move between those states
\item \textbf{A search tree}: you list all the possibilities from the current states using the possible \textit{operators}
\end{itemize}


\subsubsection{Search strategies}
There is two types of search: \textbf{uninformed search} where the only information known by the agent is \textit{Am I on the goal?} and the \textbf{informed search} where agent has a background information about the problem.

We can evaluate a search strategy with 4 criterias:
\begin{itemize}
\item \textbf{Completeness}: it finds a solution if one exists
\item \textbf{Time complexity}: usually in terms of the number of nodes generated/expanded
\item \textbf{Space complexity}: maximum nodes in memory
\item \textbf{Optimality}: it finds a least cost solution?
\end{itemize}
And you use 3 differents variables:
\begin{itemize}
\item[$\textbf b$] maximum branching factor of the search tree (the maximum number of subnodes)
\item[$\textbf d$] depth (in the tree) of the least-cost solution
\item[$\textbf m$] maximum depth of he search tree (may be infinite)
\end{itemize}

\subsection{Uninformed search}

Here will be presented different algorithm to search in a tree.

\subsubsection{Algorithm: Breadth-First Search}
Breadth-First is an example of \textit{uninformed search}. The goal is to search in a tree level by level from "left" to "right".

You start from the root node and adds the subnodes at the end of a 
\textcolor{red}{FIFO queue} (the queue beeing the \textit{frontier}). 

\paragraph{Criteria}
This solution is \textit{complete} (if b is finite), and \textit{optimal} if the cost per
step is 1 (but in general use it ain't \textit{optimal}).

\paragraph{Complexity}
it has a \textit{time complexity} of $O(b^d)$\footnote{Time complexity: $1+b+b^2+...+b^d+b(b^d-1) = O(b^{d+2}) = O(b^d)$} and a \textit{space complexity} of $O(b^d)$. 

\begin{figure}[H]
\centering
\begin{tabular}{|llll|}
\hline
\textbf{Depth} & \textbf{Nodes} & \textbf{Time} & \textbf{Memory} \\
\hline
2 & 1100 & 0.11 seconds & 1 Mb \\
4 & 111100 & 11 seconds & 106 Mb \\
6 & $10^7$ & 19 minutes & 10 Gb \\
8 & $10^9$ & 31 hours & 1 Tb \\
10 & $10^{11}$ & 129 days & 101 Tb \\
12 & $10^{13}$ & 35 years & 10 Pb \\
14 & $10^{15}$ & 3523 years & 1 Eb \\
\hline
\end{tabular}
\caption{Breadth-First Search Evaluation}
\end{figure}

\subsubsection{Algorithm: Uniform-Cost search}

Uniform-Cost is another example of \textit{uninformed search}. This is similar to Breadth-First but with a cost adedd the the reachable nodes. Le \textit{path cost} is simply the sum of the individual edge costs to reache the current node. \textit{Frontier} is now a queue ordered by the path cost.

\paragraph{Criteria}
This algorithm is \textit{optimal} compare to Breadth-First, it is \textit{complete} if step cost is strictly positive.

\paragraph{Complexity}
\textit{time and space complexity} are difficult to establish (it depends on the tree), we evaluate it with $O(b^{C/\epsilon}$.\footnote{$C$ is the cost of the optimal solution and $\epsilon$ the step cost strictly positive}

\subsubsection{Algorithm: Depth-first Search}

Another exemple, where the \textit{frontier} is implemented as a \textcolor{red}{LIFO queue}. Concretly, we go to the last depth and then go back up.

\paragraph{Criteria}
This algorithm isn't \textit{complete}, as you can fall in infinite depth spaces. The \textit{time complexity} is $O(b^m)$ 

\paragraph{Complexity}
The \textit{space complexity} is $O(mb)$. \textbf{It is not an \textit{optimal} algorithm}.

\subsubsection{Algorithm: Depth-Limited Search}

It is the same as a \textit{Depth-first} but with a depth-limit. 

\subsubsection{Algorithm: Iterative Deepening}

Let $l$ be a limit. This algorithm is a Depth-Limited, but we increase the depth limit as we search. It combines advantage of Breadth-First and Depth-First methods. In this techniques many nodes are visited multiple times but it doesn't really matter beacause the number of those nodes is "small".

This algorithm is \textit{complete}, it has a \textit{time complexity} of $O(b^d)$ and a \textit{space complexity} of $O(bd)$. This is an \textit{optimal} algorithm.

\textbf{TODO: complete the explanation of why the space is linear now}

\subsubsection{Algorithm: Bidirectionnal Search}

We use Breadth-First search and we stop when two search trees intersects. There is a few difficulties with this type of search:
\begin{itemize}
\item Predecessors of a state must be generated\footnote{Quelqu'un peut-il expliquer?}
\item Search must be coordinated between the two search processes
\item Multiple goal states?
\item One serach must keep all nodes in memory
\end{itemize} 

This algorithm is \textit{complete}, it has a \textit{time complexity} of $O(b^{d/2})$ and a \textit{space complexity} of $O(b^{d/2})$. It's an optimal algorithm is step cost is 1 (like in Breadth-First). 

\begin{figure}[H]
\centering
\begin{tabular}{|l|cccccc|}
\hline
& \textbf{BF} & \textbf{UC} & \textbf{DF} & \textbf{DL} & \textbf{ID} & \textbf {BS}\\
\hline
\textbf{Completeness} & YES & YES & NO & YES if $l\geq d$ & YES & YES\\
\textbf{Time} & $b^{d+1}$ & $b^{C/\epsilon}$ & $b^m$ & $b^l$ & $b^d$ & $b^{d/2}$\\
\textbf{Space} & $b^{d+1}$ & $b^{C/\epsilon}$ & $bm$ & $bl$ & $bd$ & $b^{d/2}$ \\
\textbf{Optimallity} & YES & YES & NO & NO & YES & YES \\
\hline
\end{tabular}
\caption{Graph Search Algorithm}
\end{figure}

\subsection{Repeated States}

Failure to detect repeated states can turn a solvable problems into unsolvable ones. We are going to avoid visiting nodes that have already been visited. 

\subsection{Informed search}

\textbf{Attention, Attention: This synthesis must be completed with the content of the book}

On the previous algorithm, we didn't use any information about the solution, in \textit{informed search} we are going to exploit informations on the node to make a decision.

\begin{description}
\item[Informed search] we search using problem specific knowledge and find and/or deduce information about future states and future paths.
\end{description}

We are going to use an evaluation function $f(n)$ that evaluates if the node $n$ is a good candidate to expand (select the node that minimize $f(n)$). Take care that this is an \textbf{evaluation} function \textbf{not the exact} distance.\footnote{If it was the exact distance, we would already know if we are (or not) on the solution path, and we would never have to change direction at any point during the search.}

\subsubsection{Best-first search}

BFS is afamily of search methods on trees and graphes. It is the same algorithm that we saw earlier, but we add the \textit{evaluation function} in the \textcolor{red}{priority queue}.

Two example:
\begin{itemize}
\item Greedy best-first search
\item A*
\end{itemize}

\subsubsection{Heuristics functions}

A \textbf{heuristic function}, denoted $h(n)$, is the estimated cost of the node to the goal.\footnote{$f(n)=g(n)$} Obviously we don't know this cost, so we'll have to approximate it. You have to define the heuristic function for each problem you'll have to solve. 

\begin{description}
\item[Optimistic heuristic] an admissible heuristic
\item[Admissible heuristic] it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.\cite{wikiadmheur} \textbf{TODO: explain properly}
\item[Consistent heuristic] The estimation from a node $n$ to the goal, is lesser than the cost\footnote{Not an estimation, the exact cost} from $n$ to a new node $n'$ with the estimation from the node $n'$ to the goal ($h(n) \leq c(n,a,n')+h(n')$).
    Consistent heuristic is admissible.
\item[Contour] a countour is a set of nodes with $f(n) \leq$ some fixed value
\end{description}

\subsubsection{Algorithm: Greedy best-first search}

This algorithm calculate directly the heuristic using only the evaluated distance. It ain't good because you don't take into account the path cost and you may come to no solution at all. 

\begin{figure}[H]
\centering
\begin{tabular}{|llll|}
\hline
\textbf{Complete} & \textbf{Time complexity} & \textbf{Space complexity} & \textbf{Optimal} \\
\hline
No& $O(b^m)$ & $O(b^m)$ & No\\
\hline
\end{tabular}
\caption{Greedy best-first search analysis}
\end{figure}

\subsubsection{Algorithm: A* Search}

A* use the distance and add the current solution path cost. Let $g(n)$ be the cost of the solution going to the node $n$ and $h(n)$ a heuristic function.
$$f(n) = g(n) + h(n)$$
$h(n)$ must never overestimates the cost to reach a goal (and so $f(n)$ also never overestimates the cost). If $n$ is the goal node, then $h(n) = 0$.

\begin{figure}[H]
\centering
\begin{tabular}{|llll|}
\hline
\textbf{Complete} & \textbf{Time complexity} & \textbf{Space complexity} & \textbf{Optimal} \\
\hline
Yes & $O(b^m)$ & $O(b^m)$ & Yes\\
\hline
\end{tabular}
\caption{A* search analysis}
\end{figure}

This is a nice algorithm, but we have a space complexity issue, it takes a lot of memory.

\paragraph{Proof of A* optimality}

Let $G$ be a goal node in the fringe, but in a suboptimal path. Its path cost $g(G)=C$ is not the lowest one\footnote{$\exists C^* : C > C^*$}. We use the formula at line (1), then as $G$ is a goal node and $h$ is admissible we have line (2). 
\begin{eqnarray}
f(G) &=& g(G) + h(G)\\
h(G) &=& 0\\
f(G) &=& g(G)\\
&=& C > C^*\\
f(G) > C^*
\end{eqnarray}
Let $n$ be a node in the fringe, with $n$ in the path to the optimal soluation (cost $C^*$). As $h$ is admissible, we have $f(n) = g(n) + h(n) \leq C^*$. Then we have
$$f(n) \leq C^* < f(G)$$
In this scenario, $G$ will never be selected, so the hypothesis is absurd. In conclusion \textbf{A* expands no nodes with} $f(n) > C*$.
\paragraph{Properties of A*}

Here are the properties of A*:
\begin{itemize}
\item With $h(n)$ consistent, the sequence of nodes expanded by A* using \textbf{Graph-Search} is in non decreasing order of $f(n)$
\item A* (using \textbf{Graph-Search}) is optimal if $h(n)$ is consistent
\item A* expands all nodes with $f(n) < C^*$ ($C*$ beeing the optimal cost)
\item A* expands some (at least one) of the nodes on the $C^*$ countour before finding the goal
\item A* expands no nodes with $f(n) > C^*$ 
\end{itemize}

\subsubsection{Algorithm: Iterative deepening A* (IDA*)}

We combined the algorithm A* with the Iterative deepening. Let $l$ be a limit. \textbf{TODO}

\begin{figure}[H]
\centering
\begin{tabular}{|llll|}
\hline
\textbf{Complete} & \textbf{Time complexity} & \textbf{Space complexity} & \textbf{Optimal} \\
\hline
Yes & Exponetial & \textbf{\textit{linear}} & Yes\\
\hline
\end{tabular}
\caption{IDA* search analysis}
\end{figure}

It's a nice algorithm, but we can still improve the algorithm. With an infinite amount of memory, A* would be the best algorithm. If I agree to re-do some operations IDA* is the best. So we use A* and when we are short of memory we switch to IDA*.\footnote{We called this one \textit{Simple memory-bounded A*}}

\subsubsection{Monotonicity of $f(n)$}

If $h(n)$ is consistent (Let a be an action and n,n' two nodes we have $h(n) \leq c(n,a,n') + h(n')$) then $f(n)$ along any path is non decreasing.

\paragraph{Proof}

Suppose that $n'$ is successor of $n$. We have
\begin{eqnarray*}
g(n') &=& g(n) + c(n,a,n')\\
f(n') &=& g(n') + h(n')\\
&=& g(n) + c(n,a,n') + h(n')\\
c(n,a,n') +h(n') &\geq& h(n)\\
g(n) + c(n,a,n') + h(n') &\geq& g(n) + h(n)\\
&\geq& f(n) \\
f(n') &\geq& f(n)
\end{eqnarray*}

\subsubsection{Comparing two heuristics}

To compare two heuristics you may use:
\begin{itemize}
\item The number of generated nodes $N$
\item The effective branching factor $b^*$, with $N+1 = 1 + b^* + (b^*)^2 + ... + (b^*)^d$ (an ideal $b^*$ is close to one)
\end{itemize}

\subsubsection{Designing heuristics}

You must choose an \textit{admissible} and \textit{consistent} heuristic. You must choose the most dominant heuristics\footnote{The most dominant is the one with the most informations}. 


\subsection{Heuristic function}
TODO


\section{Chapitre 4.1 : Local search}

A search is the operation of looking for a solution where solution is a path from start to goal. We've seen two kind of search, an \textit{uninformed search} in which no prediction is available about the cost of the path. And an \textit{informed search}, where we can estamite the cost of the solution.

What if the path is not important, but only the goal? \textbf{Local search} will not keep track of paths. It has sereval advantages
\begin{itemize}
\item Use a small amount of memory,
\item They can find soluton in infinite search,
\item They find a reasonable (not optimal) solution.
\end{itemize}

\subsection{How does it work?}

You always improve the current solution (we always store the best solution at every iteration). Then we check in the neighbourhood if there is a better solution. If a best solution exists, you restart from this point.

\subsection{Optimization Problem}

We have an \textbf{objective function} that you can \textit{minimize} or \textit{maximize}. Ex: \textit{distance to position}, \textit{number of queen attacking in the 8 queens problem}, ...

The only problem of this algorithm are \textbf{local optimal}, so you may be stuck at some point in the search. 

\subsubsection{Neighbourhood size}

If we choose to take larger neighbourhood we will have \textit{shorter path} to the solution, but we will also need \textit{more time} to explore all possibilities. It's a \textbf{key design decision}.

\subsubsection{Neighbourhood connectivity}

In a \textbf{connected neighbourhood}, there is a path to an optimal solution, from each solution. It has two advantages
\begin{itemize}
\item Convergence property
\item No need of restarting strategy (you will never be block)
\end{itemize}

\subsubsection{Neighbourhood constraint}

\textit{How to combine faisability with optimality requirements?} Two approaches are possible:
\begin{itemize}
\item
	\begin{enumerate}
	\item Maintain feasibility at all times
	\item Explore only feasible solutions
	\end{enumerate}
\item 
	\begin{enumerate}
	\item Do not maintain feasibility at all time; relax a subset of the constraints
	\item Explore a larger search space
	\item Drive the search to high quality and feasible solutions
	\end{enumerate}
\end{itemize}

\subsection{Heuristics and metaheuristics}

\begin{description}
\item[Heuristics] Drive the search towards \textit{local optimum}, it's a memoryless solution
\item[Metaheuristics] Drive the search towards \textit{global optimality}, it includes memory or learning
\item[Systematic heuristics] Exploration (possibly partial) of the neighborhood to determine the next solution
\end{description}

\subsection{Hill climbing}

Select a starting state, and move in the direction of increasing value. Stop the iteration when it's impossible to go further. This algorithm has multiple issues:
\begin{itemize}
\item Local maximum
\item Plateau (a flat area)
\item Ridges
\end{itemize}

\subsubsection{Variants}

There is multiple variants of the hill climbing algorithm.
\begin{description}
\item[Stochastic hill climbing] randomly choose in the list of possibilites
\item[First-choice hill climbing] pick the first good successor you find (useful when there is a large amount of possibilites)
\item[Random restart] You start from multiple random points
\end{description}

\subsubsection{Analysis}

Hill climbing has a big Achilles hell, the local maximas. You cannot escape local maxima, considering that hill climbing \textbf{will never makes downhill moves}.

\subsection{Random walks}

You can consider another kind of heuristic, in which you select randomly an element of the neighborhood and decide whether to accept it as the next solution. There is two possibles approches: \textit{random improvement} and \textit{metropolis heuristic} \textbf{TODO Complete with the book}.

\subsection{Metaheuristic}

We've already defined what \textit{metaheuristic} consist of, in this subsection we are going to give some examples of \textit{metaheuristic}. Here are somme examples:
\begin{enumerate}
\item \textbf{Simulated annealing}: \textbf{TODO complete with algorithm}

	This technique use a analogy with metallurgy, in which you want molecules to find a stable location relative to neighbors, and to do so you heat the metal causing mollecules to take on undesirable locations. During cooling the molecules reduce their movement and settle into a more stable position. \textit{Annealing} is the process of heating metal and letting it cool slowly to lock in the stable locations of the molecules. So here is the principle of this technique:
	\begin{enumerate}
		\item Always move uphill if possible (hill climbing)
		\item Sometimes go downhill (like in metallurgy when temperature is high).
		\item Optimality is guaranteed with slow annealing schedule. 
	\end{enumerate}

\item \textbf{Local Beam Search}:
	
	Hill climbing and simulated annealing techniques keep one state in memory. In this technique you keep $k$ states in memory. \textbf{TODO explain the advantage ?!}

\item \textbf{Genetic Algorithms}:

	Algorithm that use the idea of evolution.
	\begin{enumerate}
		\item Start with $k$ initial guesses forming a population
		\item Each individual from the population has a DNA (fixed-length string) and a fitness score
		\item Produces the next generation by reproduction between the best individuals from current population
		\item Add a few random mutations (not too much, or risk to screw up a good solution)
	\end{enumerate}
	The only issues with this algorithm is that crossover is not applicable to all problems.

\item \textbf{Tabu search Metaheursitics}:
	
	You select the best neighbor that has not yet been visited and only maintain the recent visited node, not all of them. 
\end{enumerate}

\subsection{Intensification vs Diversification}

\begin{itemize}
\item \textbf{Intensification}:

	\begin{description}
		\item[Goal] increase search around promising areas
		\item[Risk] premature convergence
		\item[Mean] favor good solutions
	\end{description}

\item \textbf{Diversification}:
	 
	\begin{description}
		\item[Goal] explore new ares
		\item[Risk] convergence to optimality may be too long
		\item[Mean] probabilistic choice of solutions
	\end{description}

\subsection{Other local search Approaches}

There is a bunch of other local search:
\begin{description}
\item[Variable Neighborhood Search] sequence of neighborhood
\item[Guided local search] use a sequence of objective functions to drive away from local optima
\item[Adaptive local Search] The heuristics/metaheuristisc are dynamically adapted during the search
\item[Ant colony optimization] the selection function is updated
\item[Statistic Local Search] another name for Local search, stressing the stochastic aspect of the search
\end{description}

\end{itemize}


\section{Chapitre 5 : Adversial search}
TODO

\section{Chapitre 6 : }
TODO

\section{Chapitre 7 : }
TODO

\section{Chapitre 8 $\backslash$ 8.3.3 : }
TODO

\section{Chapitre 9 : }
TODO

\section{Chapitre 10 : }
TODO

\section{Chapitre 11 $\backslash$ 11.2, 11.4 : }
TODO

\section{Chapitre 18.1-3, 18.10 : }
TODO


\begin{thebibliography}{1}
\bibitem{wikiadmheur} http://en.wikipedia.org/wiki/Admissible\_heuristic, {\em Wikipedia}
%\bibitem{Propagation} P.G. Fontolliet, {\em Traité d'Electricité}, Volume XVIII, Ecole polytechnique fédéral de Lausanne, pp 72-73
\end{thebibliography}

\end{document}
