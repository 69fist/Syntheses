\input{../../lib.tex}

\hypertitle{en}{Artificial Intelligence}{7}{INGI}{2261}
{Nicolas Houtain\and Symeon Malengreau\and Gorby Nicolas Ndonda Kabasele}
{Yves Deville}

\textbf{ATTENTION: We should complete the content of this chapter with the content of the book}

\section{Chapitre 1 : Introduction}

\textbf{What is IA ?}

\subsection{Turing test approach}
A \textbf{AI} success this test if after ask question  a human don't know if this is
a other human or machine.

TODO

\section{Chapitre 2.1-2.3, Intelligent Agent}

\subsection{Agents and environments}
\begin{description}

    \item[Agent]  :  Anything  that  can be  viewed  as  perceiving  its
    environment through sensor and acting through actuators.

    \item[Percept] : Refer to the agent's perceptual inputs at any given
        instant

    \item[Percept sequence] : Complet history of percept

\end{description}

\subsection{Concept of rationality}
\begin{description}

    \item[Rationality] : Is do the right think, but what is the
        right think for computer? Need \textit{Performance measure}

    \item[Rationnal  agent]  : For  each  possible  percept sequence,  a
    rational agent should select an  action that is expected to maximize
    its performance measure, given the  evidence provided by its percept
    sequence and whatever build-in knowledge the agent has.

        \begin{enumerate}
            \item Performance measure that define criterion of success
            \item Knowledge of the environment
            \item Action that can perform
            \item Percept sequence to date
        \end{enumerate}

    \item Note : Rationnality $\neq$ omniscience

    \item[Omniscience agent] : know the \textbf{real} result of its action.

    \item[Autonomy] :  A agent learn what  ic can can to  compensate for
    partial or incorrec prior knowledge

\end{description}


\subsection{Nature of environments}
Task environnement (PEAS) is a specification to the \textbf{P}erformance, 
\textbf{E}environment, \textbf{A}ctuators ans \textbf{S}ensors.

\begin{figure}[h]
    \centering
    \begin{tabular}{|m{3cm}|m{3cm}|m{3cm}|m{3cm}|}
        \hline
        Perf measure & Environment & Actuators & Sensors \\
        \hline
        Safe, fast, legal, max profit  & Roard, other traffic, customer,
        pedestrians &  Accelerator, display,  brake, signal  & Camerass,
        sonar, speedometer, FPS,\ldots \\
        \hline

    \end{tabular}
    \caption{Example PEAS for taxi driver}
\end{figure}

TODO page 43 ( Tout les Vs )

\section{Chapitre 3 : Solving problems by searching}

\subsection{Key principles}

\begin{description}
    \item[Search] process of looking for a (or the best) sequence of actions, that leads to a goal (specific state of the environment), starting from an initial state
    \item[States] distinguishables stages during the problem solving process (representation of physical configuration)
    \item[State space] is graph representation of the successor function with the cost
    \item[Initial state] \textit{to complete}
    \item[Action] an action transports the agent from one state to another one by applying an \textit{operator} to a state
    \item[Operators] \textit{to complete}
    \item[Goal test] \textit{to complete}
    \item[Path cost] \textit{to complete}
    \item[Solution] a solution is a sequence of actions leading from the initial state to a goal state
    \item[Optimal solution] an optimal solution has the lowest path cost among all solutions
    \item[Node] data structure constituting part of a search tree
    \item[Frontier] set of generated nodes, which ancestors have been goal-tested (visited)
\end{description}

\subsection{Problem solving agents}

\textit{An  agent with  several immediate  options of  unknow value  can
decide what to do by first  exmining future actions that eventually lead
to states of know value}

\paragraph{Assumption} : Environment is  static, deterministic and fully
observable.

\subsubsection{Problem definition}
A problem can be defined :
\begin{enumerate}
    \item States and \textbf{initial state}
    \item A description of the  \textbf{possible action} available to the
    agent. 
    \item A description of what each action does, called \textbf{transition
        model}. 
    \item A \textbf{Goal test}
    \item A \textbf{Path cost} that use a step cost.
\end{enumerate}

\subsubsection{Solution} 
A  \textbf{solution}  is a  sequence  of actions  leading
from  the  initial state  to  a  goal state  A  \textcolor{red}{optimal}
\textbf{solution} has the lowest path cost amoung all solutions.

\subsubsection{State space}
Together,   initial  state,   actions   and   transition  model   define
\textbf{state space}  of the  problem. (\textit{Graph  representation of
the successor function with the cost})

\subsubsection{Abstraction}
Removing detail from a representation of the state or the action.

\subsection{Example}
TODO


\subsection{Searching for Solutions}

\begin{description}

    \item[Searching] for solutions  is a traversal of  some search space
    from the \textit{initial  state} to the \textit{goal  state} using a
    legal sequence of actions (as defined by operators).

    \item[Expanding]  :  apply  each   legal  action  to  current  state
    (=generating state)

    \item[Frontier] : set of all leaf nodes available for expansion. 
        (That correspond of all leaf nodes wich ancestor have visited)
\end{description}

\paragraph{Loopy path} A  complete search tree can be  infinite if there
is  looping. But  we  don't considere  this one  because  path cost  are
additive and  steo cost are  nonnegative. (\textit{Loppy path  is nerver
better than same path without loop})

\subparagraph{ } To avoid redundant path, we use \textbf{graph search}
by using \textit{explored set}

\subsubsection{Tree and Graph Search}

There is two way to  perform the search\footnote{the slides specifies an
algorithm to search in a tree}.

\begin{itemize}

    \item \textbf{A  graph representing the state  space}: you represent
all the possible states as a graph, and you move between those states

    \item \textbf{A  search tree}: you  list all the  possibilities from
the current states using the possible \textit{operators}

\end{itemize}

\subsubsection{States and Node}
\begin{itemize}
    \item A \textbf{state} is a representation of a physical
        configuration
    \item A \textbf{node} is a data structure contituting part
        of a search tree
\end{itemize}

So, there can be many node with same state!

\subsubsection{Repeated States}

Failure  to detect  repeated states  can turn  a solvable  problems into
unsolvable ones. We are going to  avoid visiting nodes that have already
been visited.

Use \textbf{graph  search} is like tree  search but do not  expand nodes
with a state appearing in some already visited node


\subsubsection{Algorithm description}

TODO

\subsubsection{Search strategies}

There is two types of  search: \textbf{uninformed search} where the only
information known  by the agent  is \textit{Am I  on the goal?}  and the
\textbf{informed search} where agent  has a background information about
the problem.

We can evaluate a search strategy with 4 criterias:
\begin{itemize}
    \item \textbf{Completeness}: it finds a solution if one exists
    \item \textbf{Time complexity}: usually in terms of the number of nodes generated/expanded
    \item \textbf{Space complexity}: maximum nodes in memory
    \item \textbf{Optimality}: it finds a least cost solution?
\end{itemize}
And you use 3 differents variables:
\begin{itemize}
    \item $\textbf b$  maximum branching factor of the search tree (the maximum number of subnodes)
    \item $\textbf d$  depth (in the tree) of the least-cost solution
    \item $\textbf m$  maximum depth of he search tree (may be infinite)
\end{itemize}



\subsection{Uninformed search}
Here will be presented different algorithm to search in a tree.

\begin{figure}[h]
\centering
\begin{tabular}{|l|m{2cm}|m{2cm}|m{2cm}|m{2cm}|m{2cm}|m{2cm}|}
\hline
& \textbf{BFS} & \textbf{UCS} & \textbf{DFS} & \textbf{DLS} & \textbf{IDS} & \textbf {BDS}\\
\hline
& level by level & cheap path & last depth & limited depth & iterative depth & bi direct  \\

\hline
\hline
\textbf{Complete} & \textcolor{red}{if} \textbf{b} finite & \textcolor{red}{if} \textbf{sp} > 0  & NO  & \textcolor{red}{if} $l\geq d$ & YES & YES\\
\hline
\textbf{Optimale} & \textcolor{red}{if} \textbf{sp} = 1 & YES & NO & \textcolor{red}{if} l==d & \textcolor{red}{if} += 1 & \textcolor{red}{if} \textbf{sp} = 1 \\
\hline
\textbf{Time} & $b^{d+1}$    & $b^{1 + C/\epsilon}$ & $b^m$ & $b^l$ & $b^d$ & $b^{d/2}$\\
\hline
\textbf{Space} & $b^{d+1}$ & $b^{1 + C/\epsilon}$ & $bm$ & $bl$ & $bd$ & $b^{d/2}$ \\
\hline
Frontier & FIFO & Priority & LIFO & LIFO & LIFO & FIFO \\
\hline

\end{tabular}
\caption{Uninformed Search Algorithm}
\end{figure}


\subsubsection{Algorithm: Breadth-First Search}
The goal is to search in a tree level by level from "left" to "right".

The memory use grow really fast! 

You start from the root node and adds the subnodes at the end of a 
\textcolor{red}{FIFO queue} (the queue beeing the \textit{frontier}). 

\paragraph{Criteria} \textit{Complete} if b is finite and \textit{optimal} if the cost per
step is 1 (but in general use it ain't \textit{optimal}).

\paragraph{Complexity} \textit{Time complexity} of $O(b^d)$\footnote{Time complexity: $1+b+b^2+...+b^d+b(b^d-1) = O(b^{d+2}) = O(b^d)$} and a \textit{space complexity} of $O(b^d)$. 

\subsubsection{Algorithm: Uniform-Cost search}

Like a BFS but this is not the short path but the \textbf{cheap} path.

This is similar to Breadth-First but with a cost adedd the the reachable
nodes. 

The \textit{path  cost} is simply the sum of  the individual edge
costs  to reache  the current  node.  \textit{Frontier} is  now a  queue
ordered by the path cost.

\paragraph{Criteria}
This algorithm is \textit{optimal} compare to Breadth-First, it is \textit{complete} if step cost is strictly positive to avoid the infinit path with zero-cost action.

\paragraph{Complexity}

\textit{time and  space complexity}  are difficult to  establish because
\textit{uniform cost search} are guided by path costes rather than depth.
(So he explore large trees of  small steps before
exploring paths involving large and perhaps useful step.)

We evaluate it with $O(b^{C/\epsilon}$.\footnote{$C$ is the cost of the optimal solution and $\epsilon$ the step cost strictly positive}

\subsubsection{Algorithm: Depth-first Search}

The \textit{frontier}  is implemented as a  \textcolor{red}{LIFO queue}.
Concretly, we go to the last depth and then go back up.

\paragraph{Criteria}
This algorithm isn't \textit{complete}, as you can fall in infinite depth spaces. The \textit{time complexity} is $O(b^m)$ 

\paragraph{Complexity}
The \textit{space complexity} is $O(mb)$. \textbf{It is not an \textit{optimal} algorithm}.

\subsubsection{Algorithm: Depth-Limited Search}

It is the same as a \textit{Depth-first} but with a depth-limit (l) and
then replace m by l.

Cannot find a solution if l < d !

\subsubsection{Algorithm: Iterative Deepening}

Let $l$ be  a limit. This algorithm is a  Depth-Limited, but we increase
the depth limit as we search. It combines advantage of Breadth-First and
Depth-First methods. In this techniques  many nodes are visited multiple
times but it doesn't really matter beacause the number of those nodes is
"small".

\paragraph{Criteria} This algorithm is  \textit{complete} and this is an
\textit{optimal} algorithm if we increase one by one.

\paragraph{Complexity} It has a \textit{time complexity} of $O(b^d)$ and
a \textit{space complexity} of $O(bd)$.


\subsubsection{Algorithm: Bidirectionnal Search}

We  use two Breadth-First search (start to goal and goal to start)  and   we  stop  when  two  search  trees
intersects. There is a few difficulties with this type of search:

\begin{itemize}
    \item Predecessors of a (goal) state must be generated. (Not always possible)
    \item Search must be coordinated between the two search processes
    \item What if multiple goal states?
    \item One serach must keep all nodes in memory
\end{itemize} 

\paragraph{Criteria}  This algorithm  is \textit{complete}  and it's  an
optimal algorithm is step cost is 1 (like in Breadth-First).

\paragraph{Complexity} It has a \textit{time complexity} of $O(b^{d/2})$
and a \textit{space complexity} of $O(b^{d/2})$.




\subsection{Informed search}

\begin{description}
    \item[Informed search] we search using problem specific knowledge and find and/or deduce information about future states and future paths. (\textit{exploit informations on the node
        to make better decision})
\end{description}


\begin{figure}[h]
    \centering
    \begin{tabular}{|l|m{2.5cm}|m{2.5cm}|m{2.5cm}|m{2.5cm}|m{2.5cm}|}
        \hline
        & \textbf{GBFS} & \textbf{A*} & \textbf{IDA*} & \textbf{RBFS} & \textbf{SMA*} \\
        \hline
        & best solution & total estimated & A* with iterative deepening & \\

        \hline
        \hline
        \textbf{Complete} & No (loop) & \textcolor{red}{if} $\epsilon$ > 0 & Yes  \\
        \hline
        \textbf{Optimale} & NO & \textcolor{red}{if} admissible, consistante & \textcolor{red}{if} admissible, consistante\\
        \hline
        \textbf{Time} & $b^m$ & $b^m$ & Exponential\\
        \hline
        \textbf{Space} &$b^m$ & $b^m$ & Linear\\
        \hline
        Frontier & Priority & Priority &Priority &\\
        \hline
    \end{tabular}
    \caption{Informed Search Algorithm}
\end{figure}


\paragraph{Best-first search}

is a general search algorithm (Tree or graph) in wich a node is selected
for  expansion based  on  a \textbf{evaluation  function} ($f(n$).  This
function is  construed as a  \textit{estimate cost}\ldots so  the lowest
evaluation is expanded first. So we use \textcolor{red}{priority queue}.

Take care that  this is an \textbf{evaluation}  function \textbf{not the
exact} distance.\footnote{If it was the exact distance, we would already
know if we are (or not) on the solution path, and we would never have to
change direction at any point during the search.}

\subsubsection{Heuristics functions}

A \textbf{heuristic function}, denoted $h(n)$, is the estimated cost of the node to the goal.\footnote{$f(n)=g(n)$} Obviously we don't know this cost, so we'll have to approximate it. You have to define the heuristic function for each problem you'll have to solve. 

\begin{description}
    \item[Contour] a countour is a set of nodes with $f(n) \leq$ some fixed value

    \item[Optimistic heuristic] an admissible heuristic because they think the cost of solving the problem is less thant it actually is

    \item[Admissible heuristic] it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.\cite{wikiadmheur} $h(n)$ = 0 if is a goal state.
        
    \item[Consistent heuristic] The estimation from a node $n$ to the goal, is lesser than the cost\footnote{Not an estimation, the exact cost} from $n$ to a new node $n'$ with the estimation from the node $n'$ to the goal $$h(n) \leq c(n,a,n')+h(n')$$
        Consistent heuristic is admissible.

    \item[Triangle inequality] :\\ 
        \begin{tabular}{m{8cm}m{7cm}}

            Each side of a triangle cannot be longen than te sum of
        the other two sides. 
        &
        \begin{tikzpicture}
            \node[circle, draw, minimum size=0.8cm,] (N) {n};
            \node[circle, draw, minimum size=0.8cm, below right = 0.5cm and 2cm of N] (NN) {n'};
            \node[circle, draw, minimum size=0.8cm, right = 4cm of N] (G) {G};

            \path (N) edge[->] node[below left] {$h(n)$} (NN) ;
            \path (NN) edge[->] node[below right] {$c(n, a, n')$} (G) ;

            \path (N) edge[->] node[above] {$h(n')$} (G) ;

        \end{tikzpicture}
    \end{tabular}

    \item[Monotonicity] If $h(n)$ is consistent (Let a be an action and n,n'
    two nodes we  have $h(n) \leq c(n,a,n') + h(n')$)  then $f(n)$ along any
    path is non decreasing.

        Suppose that $n'$ is successor of $n$. We have
        \begin{eqnarray*}
            g(n') &=& g(n) + c(n,a,n')\\
            f(n') &=& g(n') + h(n')\\
            &=& g(n) + c(n,a,n') + h(n')\\
            c(n,a,n') +h(n') &\geq& h(n)\\
            g(n) + c(n,a,n') + h(n') &\geq& g(n) + h(n)\\
            &\geq& f(n) \\
            f(n') &\geq& f(n)
        \end{eqnarray*}
\end{description}


\subsubsection{Algorithm: Greedy Best-first search}
GBFS tries to expand the node that is \textbf{closest} to the goal.
(\textit{For path finding, the straight-line distance})

\paragraph{Criteria} It's not a optimal solution and it's no complete
because you can loop.

\paragraph{Complexity} The \textit{space} and \textit{time} complexity
is $O(b^m)$ but with a good heuristic the complexity can be reduced
substantially.


\subsubsection{Algorithm: A* search}
Minimizing the total estimated solution cost (fusion Greedy and uniform-cost).

A* use the  distance and add the current solution  path cost. Let $g(n)$
be the cost of the solution going to the node $n$ and $h(n)$ a heuristic
function. $$f(n)  = g(n)  + h(n)$$ $h(n)$  must never  overestimates the
cost to reach a goal (and  so $f(n)$ also never overestimates the cost).
If $n$ is the goal node, then $h(n) = 0$.

\paragraph{Complexity}

\textit{space} and \textit{time} ar $O(b^d)$  This is a nice algorithm,.
but we have  a \textcolor{red}{space complexity issue}, it  takes a lot.
of memory!

\paragraph{Criteria} It's \textit{complete} if the is cost that be negative
to avoid infinite zero-cost action. 
It's optimal if heuristic is admissible (\textit{in tree search}) or consistante
(\textit{in graph search}).

\paragraph{Properties of A*}

Here are the properties of A*:
\begin{itemize}
    \item With $h(n)$ consistent, the sequence of nodes expanded by A* using \textbf{Graph-Search} is in non decreasing order of $f(n)$
    \item A* (using \textbf{Graph-Search}) is optimal if $h(n)$ is consistent
    \item A* expands all nodes with $f(n) \leq C^*$ ($C*$ beeing the optimal cost)
    \item A* expands some (at least one) of the nodes on the $C^*$ countour before finding the goal
    \item A* expands no nodes with $f(n) > C^*$ 
\end{itemize}

A*  is \textbf{optimaly  efficient} because  he don't  expand node  with
$f(n) > C*$. We  can't be more efficient because if  we don't expand all
node with $f(n) < C*$ we take the risk of missing the optimal solution!


\paragraph{To proof of A* optimality}
TODO (C'est pas bien fait.. recopiage simple des slides)

we juste pro thant A* expands no nodes with $f(n) > c*$ .
\begin{enumerate}

    \item Let  $G$ be  a goal node  in the fringe,  but in  a suboptimal
    path. Its path cost $g(G)=C$ is not the lowest one\footnote{$\exists
    C^* : C  > C^*$}. 
    
    We use the  formula at line (1), then as  $G$ is a
    goal node and $h$ is admissible we have line (2).

        \begin{eqnarray}
            f(G) &=& g(G) + h(G)\\
            \footnotemark\quad h(G) &=& 0 \\
            f(G) &=& g(G) = C > C^*\\
            \color{red}f(G) &\color{red}>&\color{red} C^*
        \end{eqnarray}
        \footnotetext{Because G is a goal state}

    \item Let $n$ be a node in the fringe, with $n$ in the path to the optimal soluation (cost $C^*$). As $h$ is admissible, we have $f(n) = g(n) + h(n) \leq C^*$. Then we have
    $$f(n) \leq C^* < f(G)$$
    In this scenario, $G$ will never be selected, so the hypothesis is absurd. 
\end{enumerate}

In conclusion \textbf{A* expands no nodes with} $f(n) > C*$.

\subsubsection{Memory-bounded heuristic search}


\paragraph{Algorithm: Iterative deepening A* (IDA*)}

We combined the algorithm A* with the Iterative deepening. Let $l$ be a limit.
IDA* use f-cost(g+h) rather than depth.

Each iteration the value is the lowest f-cost than exced cutoff on previous iteration.

Complet and  optimal with  complexity exponentiel for  for \textit{time}
and linéaire for \textit{space} 

\paragraph{Algorithm: Recursive best-first search}
TODO

\paragraph{Algorithm: Memory-bouded A* and simplified MA*}
TODO

It's a nice  algorithm, but we can still improve  the algorithm. With an
infinite amount of memory, A* would be the best algorithm. If I agree to
re-do  some operations  IDA* is  the  best. So  we  use A*  and when  we
are  short of  memory we  switch  to IDA*.\footnote{We  called this  one
\textit{Simple memory-bounded A*}}




\subsubsection{Learning to search better}
TODO

\subsection{Heuristic in practice}

\subsubsection{Comparing two heuristics}

To compare two heuristics you may use:
\begin{itemize}
\item The number of generated nodes $N$
\item The effective branching factor $b^*$, with $N+1 = 1 + b^* + (b^*)^2 + ... + (b^*)^d$ (an ideal $b^*$ is close to one)
\end{itemize}

\subsubsection{Designing heuristics}

You must choose an \textit{admissible} and \textit{consistent} heuristic. You must choose the most dominant heuristics\footnote{The most dominant is the one with the most informations}. 




\section{Chapitre 4.1 : Local search}

A search  is the operation of  looking for a solution  where solution is
a  path  from  start  to  goal.  We've  seen  two  kind  of  search,  an
\textit{uninformed search} in which no prediction is available about the
cost of the path and \textit{informed search}, where we can estamite
the cost of the solution.

\paragraph{ }
\textbf{Local search} will not keep track of paths because he juste wan't
to have a goal :
\begin{itemize}
    \item Use a small amount of memory,
    \item They can find soluton in infinite search,
    \item They find a reasonable (\textit{not optimal}) solution.
\end{itemize}

\subsection{How does it work?}

You always \textbf{improve the current solution} (\textit{Store one solution at each iteration})
The next solution is found in the \textbf{neghborhood} of the current solution.

Typically modify the value of a variable at each step.

\subsection{Optimization Problem}

We have an \textbf{objective function} that you can \textit{minimize} or
\textit{maximize}.  You want  to  find the  global maximum/minimum.  Ex:
\textit{distance to position}, \textit{number  of queen attacking in the
8 queens problem}, ...

The only  problem of this  algorithm are \textbf{local optimal},  so you
may be stuck at some point in the search.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{local.png}
    \caption{Objective function on the state space}
\end{figure}

\subsubsection{Neighbourhood}

\paragraph{Size} 
If we choose  to take larger neighbourhood we  will have \textit{shorter
path}  to the  solution, but  we will  also need  \textit{more time}  to
explore all possibilities. It's a \textbf{key design decision} (\textit{choice
between length of path and exploration})

\paragraph{Connectivity}

From each solution  there is a path  to an optimal solution.  It has two
advantages:
\begin{itemize}
    \item Required for convergence property
    \item No need of restarting strategy (you will never be block)
\end{itemize}

\paragraph{Constraint}

\textit{How to combine faisability with optimality requirements?} Two approaches are possible:
\begin{itemize}
\item
	\begin{enumerate}
	\item Maintain feasibility at all times
	\item Explore only feasible solutions
	\end{enumerate}
\item 
	\begin{enumerate}
	\item Do not maintain feasibility at all time; relax a subset of the constraints
	\item Explore a larger search space
	\item Drive the search to high quality and feasible solutions
	\end{enumerate}
\end{itemize}

\subsection{Heuristics and metaheuristics}

\begin{description}

    \item[Heuristics] focus on  chosinf the next solution  and drive the
    search  towards \textit{local  optimum} based  on local  information
    (\textit{current solution and its  neighborhood}). It's a memoryless
    solution.

        \begin{itemize}
            \item Hill-climbing
            \end{itemize}

    \item[Metaheuristics] Try to escape from  local and drive the search
    towards  \textit{global optimality}  based on  collected information
    on  the   execution  sequences.  It  includes   memory  or  learning

        \begin{itemize}
            \item Iterated local search
            \item Simulated annealing
            \item Guided local search
            \item \ldots
            \end{itemize}

    \item[Systematic heuristics]  Exploration (possibly partial)  of the
neighborhood to determine the next solution

\end{description}

\subsection{Algorithm: Hill climbing}

Move on  the direction of increasing  value and stop the  iteration when
it's impossible to go further.

This algorithm go quickly to a best solution but he have multiple issues :
\begin{itemize}
    \item Local maximum 
    \item Plateau : \textit{a maximum flat area} can stop the algorithm
    \item Ridges : sequence of local maxima where Hill climbing are very difficult to navigate
\end{itemize}


Hill climbing depend  very  much on  the  shape  of  the state-space  landscape!  Hill
climbing has a  big Achilles hell, the local maximas.  You cannot escape
local maxima,  considering that  hill climbing \textbf{will  never makes
downhill moves}.

\subsubsection{Variants}

There is multiple variants of the hill climbing algorithm:
\begin{description}
    \item[Stochastic hill climbing] randomly choose in the list of uphill moves
    \item[First-choice hill climbing] pick the first good successor you find 
        (\textit{useful when there is a large amount of possibilites})
    \item[Random restart] You start from multiple random points
\end{description}


\subsection{Random walks}

You can consider another kind of heuristic, in which you select randomly
an  element of  the  neighborhood and  decide whether  to  accept it  as
the  next solution.  There  is two  possibles approches:  \textit{random
improvement}  and  \textit{metropolis heuristic}  \textbf{TODO  Complete
with the book}.


\subsection{Simulated annealing}
Combine RW and HW to make a complete and efficient algorithm.
The algorithm start by randomize hard and then gradually reduce the intensity of randomized
and take more the best solution in neighboor.

(\textit{Annealing is the process of heating metal and letting it cool slowly to lock in the stable locations of the molecules} )

So here is the principle of this technique:
\begin{enumerate}
	\item Always move uphill if possible (hill climbing)
	\item Sometimes go downhill (like in metallurgy when temperature is high).
	\item Optimality is guaranteed with slow annealing schedule. 
\end{enumerate}


\subsection{Local beam search}

Hill climbing and simulated annealing techniques keep one state in memory. 
In this technique you keep $k$ states in memory and at each step all state 
generated successors. 

(\textit{Information betweek state can ba shared, like
``Com over here, the grass is greener''}).
In pratcice, local beam search suffer from a lack of diversity because
state quickly become concentrated in a small region.


\paragraph{ } A stochastic beam search (like stochastic hill climbin)
help to alleviate this problem


\subsection{Genetic algorithms}  

Based  onevolution theory  of  evolution, start  whith  k initial  state
(\textit{population})  and the  next  generation is  produce by  merging
between individuals from current population. 

A individual is represented by a fixed-length string and is \textit{fitness}
(score for each state, greater is better) is evaluate.

\paragraph{Cross over} A cross over point is choice at each reproduction.
The goal is to can produce s state that is long way from either parent state.

TODO page 131 paragra 2


\paragraph{Random mutation } can also perfom for explore new parts of
search space, but do this with a low probability to no break a good solution!


\paragraph{ } 
Good compromise between go to the better, random exploration and shared information
between many state.
The only issues with this algorithm is that crossover is not applicable to all problems.



\subsection{Tabu search Metaheursitics}:
You select  the best  neighbor that  has not yet  been visited  and only
maintain the recent visited node, not all of them.

TODO

\subsection{Intensification vs Diversification}
\begin{tabular}{m{8cm}|m{8cm}}
Intensification & Diversification \\
\hline

	\begin{description}
		\item[Goal] increase search around promising areas
		\item[Risk] premature convergence
		\item[Mean] favor good solutions
	\end{description}
&
	\begin{description}
		\item[Goal] explore new ares
		\item[Risk] convergence to optimality may be too long
		\item[Mean] probabilistic choice of solutions
	\end{description}
\end{tabular}

\subsection{Other local search Approaches}

There is a bunch of other local search:
\begin{description}
    \item[Variable Neighborhood Search] sequence of neighborhood
    \item[Guided local search] use a sequence of objective functions to drive away from local optima
    \item[Adaptive local Search] The heuristics/metaheuristisc are dynamically adapted during the search
    \item[Ant colony optimization] the selection function is updated
    \item[Statistic Local Search] another name for Local search, stressing the stochastic aspect of the search
\end{description}



\section{Chapitre 5 : Adversial search}

\subsection{Game}

\textbf{Game theory} view any multiagent environment as a game, provided that
the impact of each agent on the other is \textit{significant}.

In ia the most common games are deterministic (\textit{fully observable with utility value at
the end of the game are always equal en opposite}), turn-taking, two-player, zero-sum game of
perfect information.

There is also game with imperfect information and not deterministic value because of chance.


\textbf{A game} :
\begin{description}
    \item[Initial state] ($s_0$)
    \item[Player (s)] : wiche player has move
    \item[Actions (s)] : set of legal move
    \item[Result (s, a)] : \textit{transition model} wich define result of a move
    \item[Terminal set (s)] 
    \item[Utility (s,p)] : \textit{utility function} define the final numeric value for a game that ends in terminal state s for a player p.
        \textit{zero-sum game} defined as one where the total payoff to all players is the same for every instance of the game.
\end{description}

\textbf{Game tree} : tree where node are game state and edge are move.

\subsection{Optimal decision}
Given game tree, optimal strategyan be determined from the minmax value of each
node.. \textit{assuming that both players play optimally}.

Minmax is perfect play for deterministic game and if the opponent don't play
optimally, he is better.


\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{minmax.png}
    \caption{Minmax}
\end{figure}

\subsubsection{Algorithm: minmax}
Generate game tree to the terminal states and apply the utiliy function on these state.
After apply minmax to go on the top and value determine move.

Minmax is complete is tree is finite.

\paragraph{Complexity} 
Such as minmax perform a \textcolor{red}{DFS} on the game tree, the \textit{space} complexity is O($bm$) 
or O($m$) if action are genetare one at a time.

\textit{Time} complexity is O($b^m$).

\subsubsection{Optimal decisions in multiplayer games}
TODO

\subsection{Alpha-beta pruning}
\begin{figure}[h]
\begin{tabular}{m{10cm}m{6cm}}
    \centering
    \includegraphics[width=10cm]{minmax_ex.png}
&

\begin{description}
    \item $\mathbf{\alpha}$ : value of the best (\textit{higher}) choice found
        so far at any choice point along the parth for \textbf{MAX}
    \item $\mathbf{\beta}$ : value of the best (\textit{lowest}) choice found
    so far at any choice point along the parth for \textbf{MIN}
\end{description}
\end{tabular}
\end{figure}

Prining does not affect final result.

\paragraph{Complexity} if successors are ideally visited, \textit{time} complexity
is O($b^{(d/2)}$) and if visited randomly O($b^{3d/4}$)


\subsubsection{Move ordering}

Alpha beta prunning is highly dependent on the order in which the states are examined.

\paragraph{ }\textbf{Killer move} TODO


\paragraph{Game graph} There is repeated state in many games because of \textbf{transpositions}.
To be efficient, we can store the evaluation of state in hash table called \textbf{transposition
table}.

This table can doubling the reachable search depth in chess, but whith a million node per second,
it's no possible to keep all node in table\ldots To solve this, various strategie
choose wich node to keep and wich to discard.

\subsection{Imperfect real-time decisions}





\section{Chapitre 6 : }
TODO

\section{Chapitre 7 : }
TODO

\section{Chapitre 8 $\backslash$ 8.3.3 : }
TODO

\section{Chapitre 9 : }
TODO

\section{Chapitre 10 : }
TODO

\section{Chapitre 11 $\backslash$ 11.2, 11.4 : }
TODO

\section{Chapitre 18.1-3, 18.10 : }
TODO


\begin{thebibliography}{1}
\bibitem{wikiadmheur} http://en.wikipedia.org/wiki/Admissible\_heuristic, {\em Wikipedia}
%\bibitem{Propagation} P.G. Fontolliet, {\em Traité d'Electricité}, Volume XVIII, Ecole polytechnique fédéral de Lausanne, pp 72-73
\end{thebibliography}

\end{document}
