\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tensor}
\usepackage{array}
\usepackage{subfigure}
\usepackage{esint}
\usepackage{esvect}
\usepackage{cellspace}
\usepackage{tabularx}
\usepackage{graphicx}
\addparagraphcolumntypes{X}
\cellspacetoplimit=2pt
\cellspacebottomlimit=2pt
%\DeclareRobustCommand{\[}{\noindent\begin{equation*}}
%\DeclareRobustCommand{\]}{\end{equation*}}
\usepackage[pdfauthor={Benoît Legat, Nicolas Cognaux et Léopold Cambier},
pdftitle={Synthèse de Mécanique Q2 - LFSAB1202},
pdfsubject={Mathématique}]{hyperref}
\usepackage{appendix}

\theoremstyle{definition}
\newtheorem{mydef}{Définition}%[section]
\newtheorem{mynota}[mydef]{Notation}
\newtheorem{myprop}[mydef]{Propriétés}
\newtheorem{myrem}[mydef]{Remarque}
\newtheorem{myform}[mydef]{Formules}
\newtheorem{mycorr}[mydef]{Corrolaire}
\newtheorem{mytheo}[mydef]{Théorème}

\newcommand{\eqdef}{\stackrel{\Delta}{=}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rnn}{\mathbb{R}^{n \times n}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\newIm}{Im}
\DeclareMathOperator{\asin}{asin}
\DeclareMathOperator{\acos}{acos}
\DeclareMathOperator{\atan}{atan}
\DeclareMathOperator{\acot}{acot}
\DeclareMathOperator{\cis}{cis}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\newdet}{det}
\DeclareMathOperator{\p}{p}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\ind}{ind}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\kmath}{k}

\DeclareMathOperator{\newker}{Ker}
\DeclareMathOperator{\newim}{Im}
\DeclareMathOperator{\newdim}{dim}
\DeclareMathOperator{\newrang}{rang}
\DeclareMathOperator{\newnull}{null}
\DeclareMathOperator{\newint}{int}
\renewcommand{\div}{\mathrm{div}}
\newcommand{\rot}{\vv{\mathrm{rot}}\;}

\let\oldnabla\nabla
\renewcommand{\nabla}{\vv{\oldnabla}}

\renewcommand{\i}{\mathrm{i}}
\newcommand{\fl}{\rightarrow}
\newcommand{\pa}{\partial}
\newcommand{\C}{\mathbb{C}}

%% Pour liste à la ligne
\newcommand*\InsertTheoremBreak{%
	\begingroup % keep changes local
		\setlength\itemsep{0pt}%
		\setlength\parsep{0pt}%
		\item[\vbox{\null}]%
	\endgroup%
}%

\title{Synthèse de Mathématique Q2 - LFSAB1102}
\author{Benoit Legat \and Nicolas Cognaux \and Léopold Cambier}

\begin{document}
\maketitle

%       + <- x
%      /|
%     /è|
%    /gb|
%   /l r| <- x - P_V(x) \in V^\perp
%  /A  e|
% +-----+ <- P_V(x)
\part{Algèbre}

\section{Espaces euclidiens}
\begin{mydef}[Espace euclidien]
	Un espace euclidien $E$ est un espace vectoriel réel équipé d'un produit scalaire, c'est à dire d'une application
	$(-|-) : E \times E \to \R : (x, y) \mapsto (x|y)$ qui est
	\begin{description}
			\item[Bilineaire]
			$\forall x, y, z \in E, \alpha, \beta \in \R$
			\begin{eqnarray*}
				(\alpha x + \beta y | z) & = & \alpha (x | z) + \beta (y | z)\\
				(x | \alpha y + \beta z) & = & \alpha (x | y) + \beta (x | z)
			\end{eqnarray*}
			\item[Symetrique]
			$\forall x,y \in E$
			$$(x|y) = (y|x)$$
			\item[Defini positif]
			$\forall x \in E \setminus \{0\}$
			$$(x|x) > 0$$
	\end{description}
\end{mydef}

\begin{myrem}
	Notons que le fait d'être symétrique et linéaire à gauche (resp. à droite) entraine automatiquement la linéarité à droite (resp. à gauche).
\end{myrem}

\subsection{Produit scalaire}

\begin{mydef}
	Soient $E$ un espace euclidien, $V \subseteq E$ et $x,y \in E$.
	La norme de $x$ et la distance entre $x$ et $y$ sont définies respectivement comme suit
	\begin{eqnarray*}
		||x|| & \eqdef & \sqrt{(x|x)}\\
		\dist(x, y) & \eqdef & ||x - y||\\
	\end{eqnarray*}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien, $x,y \in E$ et $\alpha \in \R$.
	\begin{itemize}
		\item $(x|0) = 0 = (0|x)$;
		\item $||x|| \geq 0$;
		\item $\dist(x, y) \geq 0$;
		\item $x \neq 0 \iff ||x|| > 0$;
		\item $x \neq y \iff \dist(x, y) > 0$;
		\item $||\alpha x|| = |\alpha|||x||$;
		\item $|(x | y)| \leq ||x||\cdot||y||$ (Inégalite de Cauchy)
			\footnote{Le cas d'égalité se fait si et seulement si $x$ est parallèle à $y$.};
		\item $||x + y|| \leq ||x|| + ||y||$ (Inégalite triangulaire).
	\end{itemize}
\end{myprop}

\subsection{Orthogonalité}

\begin{mydef}
	Soient $E$ un espace euclidien, $V \subseteq E$ et $x,y \in E$.
	L'orthogonalité entre deux vecteurs et l'espace orthogonal sont définis respectivement comme suit
	\begin{eqnarray*}
		x \perp y & \stackrel{\Delta}{\Leftrightarrow} & (x|y) = 0\\
		V^{\perp} & \eqdef & \left\{x \in E | x \perp v, \forall v \in V\right\}
	\end{eqnarray*}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien et $V \subseteq E$.
	\begin{itemize}
		\item $V \cap V^{\perp} \subseteq \{0\}$ avec égalite $\iff 0 \in V$;
		\item $E^{\perp} = \{0\}$;
		\item $\{0\}^{\perp} = E$;
		\item $V^{\perp}$ est un sev de $E$;
		\item $V_1 \subseteq V_2 \Rightarrow V_1^{\perp} \supseteq V_2^{\perp}$.
		\item $V \subseteq \left( V^{\perp} \right) ^{\perp}$;
	\end{itemize}
	Si $V$ est un sev de $E$
	\begin{itemize}
		\item $V = \left(V^\perp \right)^\perp$;
		\item $E = V \oplus V^{\perp}$ et cette somme est une somme directe;
			\item
			Si $E$ est de dimension finie, alors
			$\dim E = \dim V + \dim V^{\perp}$.
	\end{itemize}
\end{myprop}


\begin{mydef}
	Soient $E$ un espace euclidien et $x_1, x_2,... ,x_n \in E$,
	\begin{enumerate}
		\item La famille $x_1, x_2,... ,x_n$ est ume famille orthogonale si
			\begin{itemize}
				\item $x_i \neq 0, \forall i$;
				\item $x_i \perp x_j, \forall i \neq j$.
			\end{itemize}

		\item La famille $x_1, x_2,... ,x_n$ est une famille orthonormée si
			\begin{itemize}
				\item $||x_i|| = 1, \forall i$;
				\item $x_i \perp x_j, \forall i \neq j$.
			\end{itemize}
	\end{enumerate}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien, et $u_1, \ldots, u_n$ une base orthonormée de $V$
	\footnote{Cette propriété est utilisée dans le cours LFSAB1202 pour dire que ${\bf u}.{\bf v} = uv$
	où $u$ et $v$ sont leurs coordonnées respectives dans la base orthonormée $[{\bf\hat{I}}]$.}.
	$\forall \alpha_i, \beta_i \in \R,$
	$$(\alpha_1u_1 + \ldots + \alpha_nu_n | \beta_1u_1 + \ldots + \beta_nu_n) = \alpha_1\beta_1 + \ldots + \alpha_n\beta_n$$
\end{myprop}

\begin{myprop}\InsertTheoremBreak
	\begin{itemize}
		\item Une famille orthonormée est une famille orthogonale;
		\item Une famille orthogonale est une famille libre.
	\end{itemize}
\end{myprop}

\begin{myprop}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ de dimension finie tel que $V \neq \{0\}$. $V$ admet une base orthonormée.
\end{myprop}

\begin{mycorr}
	Tout espace euclidien de dimension finie admet une base orthonormée.
\end{mycorr}

\begin{myprop}
	Soit $L : E \to F$ une application linéaire entre espaces euclidiens, avec $E$ de dimension finie.
	Il existe
	\footnote{On peut le démontrer sans trop de mal avec l'inégalité de Cauchy.}
	un réel $K \geq 0$ tel que, pour tout $x \in E$,
	\[ ||L(x)|| \leq K \cdot ||x|| \]
\end{myprop}

\subsection{Projection orthogonale}
\begin{mytheo}
	Soient $E$ un espace euclidien, $V$ un sev de dimension finie de $E$ et $x \in E$.
	Il existe un unique vecteur $P_V(x)$ tel que
	\[
	\left\{
	\begin{array}{l}
		P_V(x) \in V\\
		x - P_V(x) \in V^{\perp}
	\end{array}
	\right.
	\]
	On appelle ce vecteur $P_V(x)$ la projection orthogonale de $x$ sur $V$.

	Si $u_1, \ldots, u_n$ est une base orthonormée de $V$, alors
	\[ P_V(x) = (x|u_1)u_1 + \ldots + (x|u_n)u_n \]
\end{mytheo}

\begin{myprop}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ et $x \in E$.
	% La condition "sev" est elle inutile pour les autres myprop que l'existence de P_V(x) ?
	\begin{itemize}
		\item $y \neq P_V(x) \Rightarrow \dist(x, P_V(x)) < \dist(x, y)$;
		\item $P_V : E \to E$ est une application linéaire;
		\item $\Ker P_V = V^{\perp}$;
		\item $\newIm P_V = V = \{x \in E | P_V(x) = x\}$;
		\item $P_V \circ P_V \circ \cdots \circ P_V = P_V$.
	\end{itemize}
\end{myprop}

\subsection{Méthode de Gramm-Schmidt}

Supposons qu'on possède une base quelconque de $V$ : $(e_1, \dots , e_n)$. On peut construire une base orthonormée $(u_1, \dots, u_n)$ de la manière suivante
\begin{eqnarray*}
	e_1 &=& \frac{x_1}{||x_1||}\\
	e_2 &=& \frac{x_2 - e_1 \cdot (x_2|e_1)}{||x_2 - e_1 \cdot (x_2|e_1)||}\\
	e_3 &=& \frac{x_3 - e_1 \cdot (x_3|e_1) - e_2 \cdot (x_3|e_2)}{||x_3 - e_1 \cdot (x_3|e_1) - e_2 \cdot (x_3|e_2)||}\\
	\vdots &=& \vdots\\
	e_n &=& \frac{ x_n - \sum_{i=1}^{n-1} e_i \cdot (x_n|e_i) }{ || x_n - \sum_{i=1}^{n-1} e_i \cdot (x_n|e_i) || }
\end{eqnarray*}

\subsection{Application aux systèmes d'équations linéaires}
Soit
\[ S : A \cdot x = b, A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, b \in \mathbb{R}^m \]
un système d'équation linéaire.
Posons $b' \eqdef P_{\mathcal{C}(A)}(b)$.
Les solutions du système $S' : A \cdot x = b'$ sont dites solutions approchées de $S$.

Si $b \in \mathcal{C}(A)$, on aura $b' = b$ et les solutions seront les solutions exactes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Diagonalisation de matrices et d'opérateurs linéaires}

\subsection{Définitions}

\begin{mydef}[Opérateur linéaire]
	Soit $E$ un espace vectoriel sur un corps $\mathbb{K}$.
	Un opérateur linéaire sur $E$ est une application linéaire $L: V \to V$.
\end{mydef}

\begin{mydef}[Matrice diagonalisable]
	Une matrice $A \in \mathbb{K}^{n \times n}$ est diagonalisable si $\exists P, D \in \mathbb{K}^{n \times n}$, avec $P$ inversible et $D$ diagonale, telles que $A = P \cdot D \cdot P^{-1}$.
\end{mydef}

\begin{mydef}[Valeur propre, vecteur propre et espace propre] Soit $L : V \rightarrow V$ un opérateur linéaire
	\begin{itemize}
		\item $\lambda \in \mathbb{K}$ est \emph{valeur propre} si $\exists x \in V, x \neq 0$ tel que $L(x) = \lambda \cdot x$ ;
		\item un tel $x$ est dit \emph{vecteur propre} de $L$ associé à $\lambda$ ;
		\item si $\lambda \in \mathbb{K}$ est valeur propre de $L$, l'\emph{espace propre} associé à $\lambda$ est l'ensemble
			\[ E(\lambda) = \{ x \in V | L(x) = \lambda \cdot x \} \]
	\end{itemize}
\end{mydef}

\begin{myrem}
	\InsertTheoremBreak
	\begin{itemize}
		\item $E(\lambda) = \newker (L - \lambda \cdot I)$ donc $E(\lambda)$ est un sous-espace vectoriel de V ;
		\item $\lambda$ est valeur propre de L $\Leftrightarrow \newdim E(\lambda) > 0$.
	\end{itemize}
\end{myrem}

\begin{mydef}[Opérateur diagonalisable]
	Un opérateur linéaire $L$ est \emph{diagonalisable} si $V$ admet une base $f$ formée par les vecteurs propres de $L$. Dans ce cas, on a la relation
	\[ \tensor*[_e]{(L)}{_e}
	= \tensor*[_e]{(I)}{_f} \cdot \tensor*[_f]{(L)}{_f} \cdot \tensor*[_f]{(I)}{_e}
	= P \cdot D \cdot P^{-1} \]
\end{mydef}

\begin{mydef}[Multiplicité algébrique]
	Soit un opérateur linéaire $L$.
	La multiplicité algébrique d'une valeur propre $\lambda_i$ est la multiplicité de $\lambda_i$ en tant que racine du polynôme caractéristique en $\lambda$
	\[ P(\lambda) = \left| \tensor*[_e]{L}{_e} - \lambda I \right| \]
	On la note $m_a(\lambda_i)$.
\end{mydef}

\begin{mydef}[Multiplicité géométrique]
	La multiplicité géométrique d'une valeur propre $\lambda$ est définie ainsi
	\[ m_g(\lambda) \eqdef \newdim E(\lambda) \]
\end{mydef}

\begin{mydef}
	Soit $A \in \mathbb{K}^{n \times n}$,
	la trace de $A$ notée $\trace(A)$ est la somme des éléments de sa diagonale.
\end{mydef}

\subsection{Diagonalisation}

\begin{myprop}
	Soient $L : V \to V$ un opérateur linéaire et $e = (e_1, \dots, e_n)$ une base de $V$:
	$L$ est diagonalisable $\Leftrightarrow _e\!(L)_e$ est diagonalisable.
\end{myprop}

\begin{myrem}
	\InsertTheoremBreak
	\begin{itemize}
		\item Les valeurs propres et les vecteurs propres de $A \in \mathbb{K}^{n \times n}$ sont aussi les valeurs propres et les vecteurs propres de $L_A : \mathbb{K}^n \rightarrow \mathbb{K}^n : L_A(x) = A \cdot x$ ;
		\item Si $A$ est diagonalisable, alors
			$A = P\cdot D \cdot P^{-1}$ où
			\[
			D = \begin{pmatrix} \lambda_1 &  &  &  \\
				& \lambda_2 &  & \\
				& & \ddots & \\
				& & & \lambda_n \\
			\end{pmatrix}
			\]
			est la matrice diagonale des valeurs propres de $A$
			et $P$ est une matrice inversible dont les colonnes sont une base de $\mathbb{K}^n$ formée par les vecteurs propres de $A$.
			Les vecteurs propres en colonne dans $P$ doivent impérativement être dans le même ordre que leur valeur propre respective dans $D$.
		\item $\lambda_i \in \mathbb{K}$ est valeur propre de $A$ si et seulement si $\lambda_i$ est racine du polynôme caractéristique
			\[ \newdet (A - \lambda \cdot I) \in \mathbb{K}[\lambda]_n \]
	\end{itemize}
\end{myrem}

\begin{myprop}
	Soient $L: V \to V$ un opérateur linéaire,
	$\lambda_1, \dots, \lambda_r$ des valeurs propres de $L$ \emph{distinctes}
	et $x_1, \dots, x_r$ des vecteur propres relatifs, respectivement à $\lambda_1, \dots, \lambda_r$.
	La famille formée par les vecteurs $x_1, \dots, x_r$ est une famille \emph{orthogonale} et donc aussi une famille \emph{libre}.
\end{myprop}

\begin{myprop}[Multiplicité]
	Si $\lambda$ est l'une des valeurs propres d'un opérateur linéaire $L$, alors
	\[ 1 \leq m_g (\lambda) \leq m_a (\lambda) \]
\end{myprop}

\begin{myprop}[Conditions nécessaires et suffisantes pour qu'une matrice soit diagonalisable]
	Soit $\lambda_1 , \dots , \lambda_r$ l'ensemble des valeurs propres distinctes de $L : V \rightarrow V$ ($\newdim V = n$).
	Les conditions suivantes sont équivalentes:
	\begin{itemize}
		\item $L$ est diagonalisable
		\item $V = E(\lambda_1) \oplus \cdots \oplus E(\lambda_r)$ (c'est d'office en somme directe);
		\item $m_g(\lambda_1) + \cdots + m_g(\lambda_r) = n$ ;
		\item $m_a (\lambda_1) + \cdots + m_a (\lambda_r) = n$ et $m_g(\lambda_i) = m_a (\lambda_i)$ $\forall i = 1, \dots, r$.
	\end{itemize}
	\subparagraph{Remarque}
	Si $\mathbb{K} = \mathbb{C}$, la condition $m_a(\lambda_1) + \dots + m_a(\lambda_r)$ est tout le temps vraie.
	Sinon, la seule manière qu'elle soit fausse est que le polynôme caractéristique ait une racine complexe.
\end{myprop}

\begin{myprop}
	Soient $A \in \mathbb{K}^{n \times n}$ et $\lambda_1, \dots, \lambda_n \in \mathbb{K}$ les valeurs propres de $a$.
	\begin{eqnarray*}
		\det(A) &=& \prod_{i = 1}^{n} \lambda_i^{m_g(\lambda_i)}\\
		\trace(A) &=& \sum_{i = 1}^{n} \lambda_i m_g(\lambda_i) % FIXME: m_g ou m_a ?
	\end{eqnarray*}
\end{myprop}

\begin{myrem}
	Notons que
	\footnote{Rappel ({\bf Le théorème de la nullité et du rang}):
	Soit $L:A \to B$ une application linéaire, on a $\newnull L + \newrang L = \dim A$}
	\begin{align*}
		m_g(\lambda) & = \newnull (L - \lambda \cdot I) \\
		& = n - \newrang (L - \lambda \cdot I)
	\end{align*}
\end{myrem}

\subsection{Diagonalisation en pratique}
Soit une application linéaire $L$ avec $A = \tensor*[_e]{I}{_e}$.
Calculez le polynôme
\[ P(\lambda) = \left| A - \lambda I \right| \]
Calculez les $r$ différentes racines $\lambda_i$ de multiplicité algébrique $m_a(\lambda_i)$.
Vérifiez ici que $\sum_{i=1}^r m_a(\lambda_i) = n$. Si ce n'est pas le cas, $L$ et $A$ ne sont pas diagonalisables.
%TODO: cas où sum m_a != n ?

Pour chaque $\lambda_i$, vérifiez que $m_g(\lambda_i) = m_a(\lambda_i)$. Si ce n'est pas le cas, $L$ et $A$ ne sont pas diagonalisables.

Calculez une base de $E(\lambda_i)$ pour chaque $\lambda_i$ et ça devrait vous donner $n$ vecteurs $x_i$.
Prenez
\begin{eqnarray*}
	P &=& \begin{pmatrix}x_1 & \cdots & x_n\end{pmatrix}\\
	D &=&
	\begin{pmatrix}
		\lambda_1 & &\\
		&\ddots&\\
		&&\lambda_r
	\end{pmatrix}
\end{eqnarray*}
En plaçant $\lambda_i$ $m_g(\lambda_i)$ fois dans la diagonale de $D$.

Vous pouvez alors dire
\[ A = PDP^{-1} \]

\paragraph{Remarque}
Dans le cadre d'une évaluation, le calcul de l'inversion de $P$ ne sera sûrement pas requis, à moins que $n \leq 2$ ou que $P$ soit orthogonale.
En effet, rappelez vous que si $x_j$ et $x_k$ sont associé à les $\lambda_i$ différents, ils sont nécessairement orthogonaux.
Dès lors, si vous avec $m_g(\lambda_i) = 1$ pour tout $\lambda_i$, tous vos $x_i$ sont orthogonaux entre eux.
Vous avez donc
\[
P^{-1} =
\begin{pmatrix}
	\frac{x_1}{||x_1||} & \cdots & \frac{x_n}{||x_n||}
\end{pmatrix}
\]
Bien sûr si $P$ était orthogonale, $||x_i|| = 1$ pour tout $i$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Formes quadratiques}

\subsection{Théorème spectral}

\begin{mydef}\InsertTheoremBreak
	\begin{enumerate}
		\item Une matrice carrée $A \in \mathbb{R}^{n \times n}$ est symétrique si $A^T = A$.
		\item Une matrice carrée $Q \in \mathbb{R}^{n \times n}$ est orthogonale si $Q^T\cdot Q = I$.
	\end{enumerate}
\end{mydef}

\begin{mydef}[Matrice orthogonale]
	Une matrice $Q \in \R^{n \times n}$ est orthogonale si ses colonnes forment une base \emph{orthonormée} de $\R^n$ par rapport au produit scalaire canonique, ou encore si
	\[ Q \cdot Q^t = I_n \]
\end{mydef}

\begin{myrem}\InsertTheoremBreak
	\begin{enumerate}
		\item $Q$ est orthogonale précisément si ses colonnes sont une base orthonormée de $\mathbb{R}^n$ par rapport au produit scalaire canonique.
		\item Si $Q$ est orthogonale, alors elle est inversible et $Q^{-1} = Q^T$.
	\end{enumerate}
\end{myrem}

\begin{mytheo}[Théorème spectral]
	Soit $A \in \R^{n \times n}$.
	Les conditions suivantes sont équivalentes:
	\begin{itemize}
		\item $A$ est symétrique ;
		\item $\R^n$ admet une base orthonormée de vecteurs propres de $A$ ;
		\item $\exists Q \in \Rnn$ orthogonale, $D \in \Rnn$ diagonale, telles que
			\[ A = Q \cdot D \cdot Q^{-1} = Q \cdot D \cdot Q^T. \]
	\end{itemize}
	\subparagraph{Remarque}
	Le théorème spectral ne s'applique qu'au matrices réelles.
\end{mytheo}

\subsection{Définitions}

\begin{mynota}
	Soit $V$ un espace vectoriel réel de dimension finie $n$,
	et soit $e = \{e_1, \dots, e_n\}$ une base de $V$.
	Pour un vecteur $v = x_1e_1 + \dots + x_ne_n \in V$, nous écrivons
	\[ \tensor[_e]{v}{} = \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}. \]
\end{mynota}

Voici deux définitions équivalentes de ``Forme quadratique''.

\begin{mydef}[Forme quadratique]
	Soit $V$ un espace vectoriel \emph{réel} de dimension finie $n$,
	et soit $e = \{e_1, \dots, e_n\}$ une base de $V$.
	Une fonction $q: V \to \R$ est une forme quadratique s'il existe
	une matrice \emph{symétrique} $A \in \Rnn$ telle que, pour tout $v \in V$,
	on ait $q(v) = \tensor[_e]{v}{^T} \cdot A \cdot \tensor[_e]{v}{}$.
	Explicitement, si $v = x_1e_1 + \dots + x_ne_n$,
	\[ q(v) = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix} \cdot A \cdot \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} \]
	est un polynôme homogène de degré 2 par rapport aux $n$ indéterminées $x_1, \dots, x_n$.
	Nous notons $A = q(e)$ et nous l'appelons matrice associée à la forme $q$ par rapport à la base $e$.
	\subparagraph{Remarque}
	\begin{itemize}
		\item Une fois la base $e$ fixée, si la matrice $A$ existe, elle est unique;
		\item Le fait que $q: V \to \R$ soit une forme quadratique ne dépend pas du choix de la base.
	\end{itemize}
\end{mydef}

\begin{mydef}[Forme quadratique]
	Soit $V$ un espace vectoriel \emph{réel}.
	Une forme quadratique est une fonction
	\[ q : V \rightarrow \R \]
	satisfaisant à deux conditions :
	\begin{itemize}
		\item $\forall \lambda \in \R, \forall x \in V : q(\lambda x) = \lambda^2 q(x)$ ;
		\item $\bar{q} : V \times V \rightarrow \R$ est bilinéaire.
	\end{itemize}
	avec $\bar{q}(x,y) = \frac 12 (q(x+y) - q(x) - q(y) )$.
	\subparagraph{Remarque}
	$\bar{q}$ est \emph{symétrique} : $\bar{q}(x,y) = \bar{q}(y,x)$.
\end{mydef}

\subsection{Lien avec les matrices symétriques}

\begin{myprop} Soit $V$ un espace vectoriel réel, $e = (e_1, \dots , e_n)$ une base de $V$. Il y a une bijection entre
	\begin{itemize}
		\item Les formes quadratiques sur $V$ et
		\item les matrices symétriques $n \times n$ à coefficients réels.
	\end{itemize}
	De fait, par définition :
	\[ a_{i,j} = \bar{q}(e_i, e_j) \]
	\[ q_{A,e} : V \rightarrow \R : q(x)_{A,e} = _e\!x^t \cdot A \cdot _e\!x \]
\end{myprop}

\begin{mycorr}
	De là, on tire que $q(e)$ est l'\emph{unique} matrice symétrique ($q(e) \in \R^{n \times n}$) telle que
	\[ q(x) = \tensor[_e]{x}{^T} \cdot q(e) \cdot \tensor[_e]{x}{} \]
\end{mycorr}

\begin{mycorr} Soit $V$ un espace vectoriel réel, $q$ une forme quadratique, $e = (e_1, \dots , e_n)$ et $f = (f_1, \dots, f_n)$ deux bases de $V$, alors
	\[ q(f) = (\tensor[_e]{I}{_f})^T \cdot q(e) \cdot \tensor[_e]{I}{_f} \]
	Réciproquement, si $B = P^T \cdot q(e) P$, il existe une base $f$ telle que $q(f) = B$.
	Il suffit de prendre la base $f$ déterminée par $P = \tensor[_e]{I}{_f}$.
\end{mycorr}

\begin{myrem}
	Deux matrices symétriques $A, B \in \Rnn$ sont associée à une même forme quadratique $q: V \to \R$ si il existe une matrice inversible $P$ telle que $B = P^T \cdot A \cdot P$.
\end{myrem}

\subsection{Caractère}

\begin{mydef} Soit $V$ un espace vectoriel réel, $q : V \rightarrow \R$ forme quadratique. La forme quadratique est
	\begin{itemize}
		\item définie positive si $q(x) >0 \; \forall x \in V, x \neq 0$ ;
		\item semi-définie positive si $q(x) \geq 0 \; \forall x \in V$ ;
		\item définie négative si $q(x) < 0 \; \forall x \in V, x \neq 0$ ;
		\item semi-définie négative si $q(x) \leq 0 \; \forall x \in V$ ;
		\item indéfinie si $\exists x \in V$ tel que $q(x) > 0$ et $\exists y \in V$ tel que $q(x) < 0$.
	\end{itemize}
\end{mydef}

\begin{myprop}
	\InsertTheoremBreak
	\begin{itemize}
		\item Le caractère d'une matrice $A$ est par définition celui de la forme quadratique associée
			$q_A: \Rn \to \R: q(x) = \tensor[_e]{x}{^T} \cdot A \cdot \tensor[_e]{x}{}$;
		\item Le caractère de $A$ ne dépend pas de cette base $e$ de $\R^n$.
	\end{itemize}
\end{myprop}

\subsection{Caractère et valeurs propres}

\begin{myprop}[Caractère et valeurs propres]
	Soient $q: V \to \R$ une forme quadratique et $e$ une base de $V$.
	Comme $q(e)$ est symétrique, par le théorème spectral, elle est donc diagonalisable,
	le caractère de $q$ peut être déterminé par les valeurs propres de $q(e)$. $q$ est
	\begin{itemize}
		\item définie positive si et seulement si $\lambda_1, \dots, \lambda_n > 0$;
		\item semi-définie positive si et seulement si $\lambda_1, \dots, \lambda_n \geq 0$ ;
		\item définie négative si et seulement si $\lambda_1, \dots, \lambda_n < 0$ ;
		\item semi-définie négative si et seulement si $\lambda_1, \dots, \lambda_n \leq 0$ ;
		\item indéfinie si $\exists  \lambda_i > 0$ et $\exists \lambda_j < 0$.
	\end{itemize}
\end{myprop}

\begin{myprop}[Loi d'inertie de Sylvester]
	Soient $q: V \to \R$ une forme quadratique et $e$ et $f$ une base de $V$.
	\begin{itemize}
		\item L'indice de positivité de la forme $q$ est le nombre de valeurs propres strictement positives de $q(e)$.
			Il est noté $\ind_+q$;
		\item L'indice de négativité de la forme $q$ est le nombre de valeurs propres strictement négatives de $q(e)$.
			Il est noté $\ind_-q$;
	\end{itemize}
	On a aussi
	\[ \ind_+q + \ind_-q = \newrang q(e) \]
\end{myprop}

\begin{myrem}
	On voit que le caractère de $q$ dépend des valeurs propres de $q(e)$.
	Seulement, $q(e)$ et $q(f)$ n'ont pas spécialement les mêmes valeurs propres. %FIXME: why ?
	Mais ils sont lié par la loi d'inertie de Sylvester.
	Ils ont le même nombre de valeurs propres strictement positives,
	le même nombre de valeurs proptres strictement négatives
	et 0 a la même multiplicité pour $q(e)$ et $q(f)$ qui vaut
	\[ n - \newrang q(e) = n - \ind_+q - \ind_-q \]
\end{myrem}


% +--------------------------------------------------------------------+
% | d Équations différentielles                                        |
% | --------------------------- + Équations différentielles = e^{-x^2} |
% |            dx                                                      |
% +--------------------------------------------------------------------+
\part{Équations différentielles}

\section{Définition}

\begin{mydef}[Problème de Cauchy]
	Soit un problème de Cauchy, défini de la manière suivante :

	\[ P = \left\{
	\begin{array}{l}
		y^{(n)}(t) + a_{n-1}y^{(n-1)}(t) + \cdots + a_1 y'(t) + a_0 y(t) = f(t) \\
		y(t_0) = \alpha_1, y'(t_0) = \alpha_2, \dots , y^{(n-1)}(t_0) = \alpha_n \\
	\end{array} \right.
	\]
	avec $\alpha_1, \dots, \alpha_n \in \mathbb{K}, t_0 \in \R$ fixés.

\end{mydef}

\begin{mytheo}[Existence et unicité]
	Un tel problème de Cauchy admet une et une seule solution.
\end{mytheo}

\section{Solution}

Soit un problème de Cauchy $P$. On étudie plusieurs choses séparément.

\paragraph{Équation différentielle homogène associée}
La solution générale de cette équation est de type
\[ u(t) = \sum_{j=1}^{q} P_j(t) \cdot e^{r_j t} \]
avec :
\begin{itemize}
	\item $P_q \in \C[X]_{m_j -1}$ ;
	\item $r_1, \dots, r_q$ les racines du polynôme caractéristique ;
	\item $m_j = m_a(r_j)$.
\end{itemize}
En d'autres mots :
\begin{itemize}
	\item $P$ est un polynôme dont le degré est égal à la multiplicité algébrique de la racine correspondante moins un ;
	\item On multiplie $P$ par une exponentielle dont l'argument est une des racines du polynôme caractéristique
\end{itemize}
Les coefficients sont déterminés par les conditions initiales.

\paragraph{Équation différentielle non-homogène}
Dans le problème de Cauchy, $f(t) = P(t) \cdot e^{rt}$ avec $P \in \C[X]_{\leq p}$.

La solution particulière de cette équation est de type
\[ v(t) = t^m \cdot R(t) \cdot e^{rt} \]
avec
\begin{itemize}
	\item $m$ la multiplicité de $r$ comme racine du polynôme caractéristique. Si on ne retrouve pas $r$ comme racine de ce polynôme, $m = 0$ ;
	\item $R \in \C[X]_{\leq p}$ ;
	\item $r$ est le même que dans l'équation différentielle de départ
\end{itemize}
Pour déterminer les constantes, on remplace dans l'équation différentielle non-homogène les dérivées successives de $y$ par celles de $v$, ceci permettant de déterminer les inconnues.

\paragraph{Principe de superposition}
Si
\[ f(t) = \sum_{i=1}^s Q_i(t) e^{r_it} \]
alors on a la solution $v$ tel que
\[ v(t) = \sum_{i=1}^s t^{m_i} \cdot R_i(t) \cdot e^{r_it} \]
tel que
\begin{itemize}
	\item $m_i = m_a(r_i)$ ;
	\item $R_i \in \C[X]_ { d^{\circ} Q_i }$.
\end{itemize}

\paragraph{Solution générale}
La solution générale d'une équation différentielle est simplement la somme des solutions générales de l'équation homogène associée et de la solution particulière de l'équation homogène. Donc l'espace $S$ des solutions est
\[ S = \{y = v + u \} \]
avec $u$ et $v$ comme défini ci-dessus.

\begin{myrem}[Équation en sinus et cosinus]
	Un point important : si le terme indépendant est une fonction $\cos t$ ou $\sin t$, alors pour trouver la solution particulière, il suffit de prendre respectivement la partie réelle ou imaginaire de $e^{it}$.
\end{myrem}

\section{Marche à suivre}

\begin{enumerate}

	\item Déterminer l'ensemble des solutions de l'équation homogène associée. On obtient une solution générale avec des constantes inconnues qu'on déterminera à la fin si il y a lieu.
	\item Pour chaque terme de l'équation non-homogène associée :
		\begin{itemize}
			\item si le terme est de type ($\text{polynôme} \cdot \text{exponentielle}$) on travaille de manière classique ;
			\item si le terme est de type sinus ou cosinus (de type $\sin (\omega t)$) on résout d'abord en le remplaçant par $e^{\omega t \i}$ ;
			\item ensuite on remplace dans l'équation non-homogène $y$ et ses dérivées par la solution particulière trouvée, et ses dérivées, pour déterminer les constantes inconnues. Attention, ces-dernières ne peuvent \emph{pas} dépendre de $t$ .
			\item si on avait à faire avec un cosinus ou sinus, on prend respectivement la partie réelle (si c'est un cosinus) ou imaginaire (si c'est un sinus) de la solution;
		\end{itemize}
	\item On somme ensuite l'ensemble des ces solutions (l'homogène + la ou les particulière)
	\item Si on cherche une solution limitée aux réels, on pose que les constantes de l'équation homogène sont complexes ($A = a_1 + \i a_2$, etc), on remplace, puis on annule les termes complexes.
	\item Si on a des conditions initiales ($y(0) = y_0$, $y'(0) = y'_0$, etc.) on introduit ces dernières dans la solution (et ses dérivées) pour déterminer les constantes de la partie homogène
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Calcul différentiel à plusieurs variables}

\section{Définition}


\begin{mydef}[Fonction à plusieurs variables]
	Une fonction
	\[ f : A \subseteq \R^n \rightarrow \R \]
	est définie par l'ensemble des paires $(a,f(a))$ pour chaque $a \in A$. Ces paires forment un sous-ensemble de $\R^n \times \R$ appelé graphe de $f$. Ces fonctions sont des fonctions scalaires.

	Une fonction
	\[ f : A \subseteq \R^n \rightarrow \R^m \]
	est définie par l'ensemble des paires $(a,f(a))$ pour chaque $a \in A$. Ces paires forment un sous-ensemble de $\R^n \times \R^m$ appelé graphe de $f$. Ces fonctions sont des fonctions vectorielles. On peut toujours décomposer ces fonctions en plusieurs fonctions scalaires. En d'autres mots, on peut toujours écrire
	\[ f = (f_1, f_2, \dots, f_n) \]
	où chaque $f_i$ est scalaire.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Concepts de base}

\paragraph{Note} Dans cette section, certains concepts n'ont pas été repris

\subsection{Ensembles élémentaires}
Soit
\[ f : A \subseteq \R^n \rightarrow \R^m \]
Une fonctions est caractérisée par trois ensembles principaux :
\begin{mydef} [Graphe de $f$] Le graphe de $f$ est défini comme
	\[ \mathrm{graph} f = \{(a,f(a))| a \in A\} \subset \R^n \times \R^m \]
\end{mydef}

\begin{mydef} [Image de $f$] L'image de $f$ est l'ensemble
	\[ \mathrm{Im} f = \{ f(a) | a \in a\} \subset \R^m \]
\end{mydef}

\begin{mydef} [Ensemble de niveau k]
	L'ensemble de niveau $k$ de $f$ est
	\[ \text{ens. de niveau k} = \{ a | a \in A \;\text{et}\; f(a) = k \} \subset \R^n \]
\end{mydef}

\subsection{Points et ensembles particuliers}

\begin{mydef}[Point intérieur] $a$ est un point intérieur à $A \Leftrightarrow \exists r > 0 \in \R$ tel que
	\[ ||x - a|| < r \Rightarrow x \in A \]
	Cet ensemble se note $\mathrm{int} A$.
\end{mydef}

\begin{mydef} [Boules] Les voisinages se définissent à partir des notions de boules ouvertes et fermées. Une boule ouverte est un ensemble
	\[ B(a;r) = \{ x \in \R^n | ||x-a|| < r \} \]
	Et une boule fermée
	\[ B[a;r] = \{ x \in \R^n | ||x-a|| \leq r \} \]
\end{mydef}

\begin{mydef} [Intérieur]
	$a$ est un point intérieur à $A$ si et seulement si il existe une boule ouverte centrée en $a$ entièrement incluse dans $A$. On le note $\mathrm{int} A$.
\end{mydef}

\begin{mydef} [Point limite]
	$a$ est un point limite de $A$ si et seulement si toute boule ouverte centrée en $a$ intersecte $A$ autre part qu'en $a$.
\end{mydef}

\begin{mydef} [Point isolé]
	$a$ est un point isolé de $A$ si il existe une boule ouverte centrée en $a$ qui n'intersecte $A$ qu'en $a$.
\end{mydef}

\begin{mydef} [Fermeture]
	C'est l'ensemble des points limites et isolés. On la note $\bar{A}$.
\end{mydef}

\begin{myform}
	\[ \mathrm{int} A \subseteq A \subseteq \bar{A} \]
\end{myform}

\begin{mydef} [Frontière] La frontière de $A$ est égale à
	\[ \text{frontière de A} = \bar{A} \backslash \mathrm{int} A \]
\end{mydef}

\subsubsection{Résumé sous forme de schémas}

La figure \ref{fig:ensembles} présente un résumé des différents types de points et d'ensembles. Un trait plein signifie un bord fermé, des pointillés un bord ouvert, des hachures un ensemble <<~remplis~>>, et un point un point isolé.

\begin{figure}[!ht]
	\begin{center}
		\subfigure[$A$]{\includegraphics[height=1.8cm]{images/figuresasy-1}}\qquad
		\subfigure[int($A$) --- l'intérieur de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-2}}\qquad
		\subfigure[$\bar{A}$ --- la fermeture de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-3}}\\
		\subfigure[fr($A$) --- la frontière de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-4}}\qquad
		\subfigure[Les points limites ou d'accumulation de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-5}}\qquad
		\subfigure[Les points isolés de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-6}}\\
		\caption{Les divers ensembles}
		\label{fig:ensembles}
	\end{center}
\end{figure}

\subsection{Limites}

Dans cette partie, on considère $f$, une fonction telle que
\[ f : A \subseteq \R^n \rightarrow \R \]

\begin{mydef}[Limite d'une fonction à plusieurs variables]
	On défini la limite de $f$ uniquement en tout point de la \emph{fermeture} de son domaine.
	Si $a$ est un point isolé de $A$, alors
	\[ \lim_{x \fl a} f(x) = f(a) \]
	Si $a$ est un point limite de $A$, alors
	\[ \lim_{x \fl a} f(x) = L \]
	si et seulement si $\forall \epsilon >0, \exists \delta > 0$ tel que $x \in A$ et
	\[ 0 < || x - a || < \delta \Rightarrow |f(x) - L| < \epsilon \]
	Si la fonction est vectorielle et que $a$ est un point limite de $A$, alors
	\[ \lim_{x \fl a} f(x) = L \]
	si et seulement si $\forall \epsilon >0, \exists \delta > 0$ tel que $x \in A$ et
	\[ 0 < || x - a || < \delta \Rightarrow ||f(x) - L|| < \epsilon \]
	Si on écrit $f$ sous la forme $f = (f_1 , \dots , f_n)$ où chaque $f_i$ est une fonction scalaire, alors la limite de $f$ existe si et seulement si la limite de chaque $f_i$ existe
	\[ \lim_{x \fl a} f(x) = \left( \lim_{x \fl a} f_1(x), \lim_{x \fl a} f_2(x), \dots , \lim_{x \fl a} f_n(x) \right) \]
\end{mydef}

\begin{myprop}[Condition nécessaire\footnote{Mais pas suffisante !} à l'existence d'une limite] Soit $f_B$ une restriction de la fonction $f$ à un sous-ensemble $B \subseteq A$. Si $f$ admet une limite en $a$, alors
	\[ \lim_{x \fl a}f_B(x) \]
	existe et
	\[ \lim_{x \fl a}f(x) = \lim_{x \fl a}f_B(x) \]
\end{myprop}

\begin{myprop}[Recollement (condition suffisante)] Soit $A = A_1 \cup A_2$ ; soit $f_1$, $f_2$ les restrictions de $f$ à $A_1$ et $A_2$. Si $a \in \bar{A_1}$ et $a \in \bar{A_2}$, alors
	\[ \lim_{x \fl a}f(x) = L \Leftrightarrow \left\{
	\begin{array}{l}
		\lim_{x \fl a} f_1(x) = L \\
		\lim_{x \fl a}f_2(x) = L
	\end{array} \right. \]
\end{myprop}

\begin{myform}[Combinaisons algébriques]
	Les quatre opérations classiques ($+$, $-$, $\cdot$, $\div$) sont possibles, mais \emph{pas} la composition de limites.
\end{myform}

\begin{myprop}[Positivité] Si
	\begin{itemize}
		\item $f(x) \geq 0 \Rightarrow \lim_{x \fl a} f(x) \geq 0$ ;
		\item $f(x) > 0 \Rightarrow \lim_{x \fl a} f(x) \geq 0$ ;
		\item $f(x) \leq g(x) \Rightarrow \lim_{x \fl a} f(x) \leq \lim_{x \fl a} g(x)$.
	\end{itemize}
	NB : ces propriétés sont aussi valables dans un voisinage de $a$.
\end{myprop}

\begin{myprop}[Étau (condition suffisante)]
	Si $g \leq f \leq h$ et que $\lim_{x \fl a} g(x) = \lim_{x \fl a} h(x) = L$ alors
	\[ \lim_{x \fl a} f(x) = L \]
\end{myprop}

\begin{myform}[Application de l'étau]\InsertTheoremBreak
	\begin{itemize}
		\item si $f \rightarrow 0$, alors $|f| \rightarrow 0$ ;
		\item si $f \fl 0$, et que $g$ est bornée, alors $\lim_{x \fl a} (g \cdot f) = 0$.
	\end{itemize}
\end{myform}

\begin{myprop}[Continuité]
	Une fonction $f : A \subseteq \R^n \rightarrow \R^m$ est continue en $a \in A$ si et seulement si
	\[ \lim_{x \rightarrow a}f(x) = f(a) \]
\end{myprop}

\begin{myrem}
	Une fonction est toujours continue en un point isolé de $A$.
\end{myrem}
\begin{myrem}
	Les quatres opérations classiques \emph{ainsi que la composition} préservent la continuité.
\end{myrem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dérivées directionnelles et partielles}

\begin{mydef}[Dérivée directionnelle] Soit $f : A \subseteq \R^n \fl \R^m$, $a$ un point intérieur de $A$, $d \in \R^n$ une direction. La dérivée directionnelle de $f$ en $a$ dans la direction $d$ est notée $D_df(x)$ et vaut, si elle existe, la limite suivante
	\[ D_df(a) = \lim_{t \fl 0} \frac{f(a+td) - f(a)}{t} \]
\end{mydef}

\begin{myform}[Approximation linéaire dans une direction]
	\[ f(a+td) \approx f(a) + t D_df(a) \]
\end{myform}

\begin{mydef}[Dérivée partielle]
	C'est la dérivée directionnelle le long d'un axe.
	Soit une fonction $f : A\subseteq \R^n \fl \R^m$ et $a$ un point intérieur de $A$. La dérivée partielle de $f$ par raport à une de ses variables $x_i$ est la dérivée directionelle de $f$ au point $a$ dans la direction $e^{(i)}$ et se note
	\[ \frac{\pa f}{\pa x_i}(a) = D_{e^{(i)}} f(a) \]
\end{mydef}

\begin{myrem}[Continuité et dérivées directionnelles]
	L'existence de toute les dérivées directionnelles (et donc à fortiori partielle) en un point n'implique pas la continuité.
\end{myrem}

\begin{myform}[Calcul des dérivées partielles] Pour calculer les dérivées partielles de $f$ selon une de ses variables $x_i$, on fixe toutes les autres variables et on calcule la dérivée partielle comme la dérivée d'une fonction à une seule variable ($x_i$).
\end{myform}

\begin{mydef}[Dérivées partielles d'ordre supérieur]
	L'existence des $i$\ieme{} dérivées partielles défini une fonction
	\[ \frac{\pa^i f}{\pa x^i} : \mathrm{int} A \subseteq \R^n \fl \R : x \fl \frac{\pa^i f}{\pa x^i}(x) \]
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Différentielle}

\subsection{Définitions}

\begin{mydef}[Différentielle]
	Soit une fonction $f : A \subseteq \R^n \fl \R^m$ et $a \in \R^n$ un point intérieur de $A$. La fonction $f$ est différentiable en $a$ si et seulement si il existe une application linéaire $L : \R^n \fl \R^m$ telle que
	\[ \lim_{h \fl 0} \frac{f(a+h) - f(a) - L(h)}{||h||} = 0 \]

	Si cette application linéaire $L$ existe, alors on l'appelle différentielle de $f$ en $a$ on elle se note
	\[ L = \dif f_a = \dif f(a) \]
\end{mydef}

\begin{myprop}
	Soit $f : A \subseteq \R^n \fl \R^m$ et $a$ un point intérieur à $A$. $f$ est différentiable si et seulement si il existe une application linéaire $L : \R^n \fl \R^m$ et une fonction $r : \R^n \fl \R^m$ telles que
	\[ f(a+h) = f(a) + L(h) + ||h|| r(h) \]
	et
	\[ \lim_{h \fl 0}r(h) = 0 \]
\end{myprop}

\begin{myprop}[Continuité]
	La différentiabilité garanti la continuité.
\end{myprop}

\begin{myform}[Lien avec dérivées partielles] On a les formules
	\[ \dif f_a : (\dif x_1 , \dots , \dif x_n) \fl \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a) \dif x_i \]
	\[ \frac{\pa f}{\pa x_i}(a) = \dif f_a(e^{(i)}) \]
	\[ Df_d(a) = \dif f_a(d) \]
\end{myform}

\begin{myprop}[Condition suffisante pour la différentiabilité] Une fonction $f : A \subseteq \R^n \fl \R^m$ et $a$ un point intérieur à $A$. $f$ est différentiable si
	\begin{itemize}
		\item toutes les dérivées partielles de $f$ \emph{existent} en $a$ ;
		\item toutes les dérivées partielles \emph{sauf une} sont continues dans un voisinage de $a$ (dans \emph{une} boule ouverte centrée en $a$)
	\end{itemize}
\end{myprop}

\subsection{Formules de calcul}


\noindent
\begin{tabularx}{1.0\textwidth}{|S X|S X|}
	\hline
	Fonction & Différentielle (au point $a$, évaluée en $h$)\\
	\hline
	$ \displaystyle M \cdot x \;\text{avec}\; M \in \R^{m \times n} $ & $\displaystyle M \cdot h $\\
	\hline
	$ \displaystyle M \cdot x + p $ & $\displaystyle  M \cdot h $ \\
	\hline
	$ \lambda \cdot f(x) \;\text{avec}\; \lambda \in \R $ & $\displaystyle \lambda \cdot \dif f_a (h) $ \\
	\hline
	$ \displaystyle M \cdot f(x) \;\text{avec}\; M \in \R^{p \times m}$ & $\displaystyle M \cdot \dif f_a (h)$ \\
	\hline
	$ \displaystyle f(x) + g(x) $ & $ \displaystyle \dif f_a (h) + \dif g_a (h) $ \\
	\hline
	$ \displaystyle f(x)\cdot g(x) $ & $\displaystyle  g(a) \cdot \dif f_a(h) + f(a) \cdot \dif g_a(h) $ \\
	\hline
	$ \displaystyle \frac{f(x)}{g(x)} $ & $\displaystyle  \frac{g(a) \cdot \dif f_a(h) - f(a) \cdot \dif g_a(h) }{g(a)^2} $ \\
	\hline
	$ \displaystyle (f(x)|g(x)) $ & $\displaystyle  (\dif f_a(h) | g(a) ) + (f(a)|\dif g_a(h)) $ \\
	\hline
	$ \displaystyle (f \circ g)(x) $ & $\displaystyle  \dif f_{g(a)} \circ \dif g_a(h) $ \\
	\hline
\end{tabularx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Plans et vecteurs tangents, gradient}

\subsection{Courbes et surfaces paramétrées}

\begin{mydef}[Courbe paramétrée]
	Soit une courbe paramétrée dans $\R^m$ décrite par $f : A \subseteq \R \fl \R^m$
	\[ \mathcal{C} = \{ f(t) | t \in A \subseteq \R \} \subseteq \R^m \]
\end{mydef}

\begin{myform}[Droite tangente]
	Si $f$ est différentiable, alors la droite tangente à la courbe $\mathcal{C}$ en $f(a)$ a pour équation paramétrique
	\[ t \fl f(a) + t f'(a) \]
	Où $f'(a)$ est le vecteur tangent à $\mathcal{C}$ en $a$
	Dans ce cas,
	\[ f(a + t) \approx f(a) + \dif f_a(t) = f(a) + t f'(a) \]
\end{myform}

\begin{mydef}[Surface paramétrée]
	Une surface paramétrée de $\R^m$ décrite par $f : A \subseteq \R^2 \fl \R^m$ est
	\[ \mathcal{S} = \{ f(u,v) | (u,v) \in A \subseteq \R^2 \} \subseteq \R^m \]
\end{mydef}

\begin{myform}[Plan tangent]
	Si $f$ est différentiable en $a$, le plan tangent à $f$ au point $f(a,b)$ est paramétrée par
	\[ (u,v) \fl f(a,b) + u \frac{\pa f}{\pa x_1} (a,b) + v \frac{\pa f}{\pa x_2}(a,b) \]
\end{myform}

\subsection{Gradient}

\begin{mydef}[Gradient] Soit $f : A \subseteq \R^n \fl \R$ une fonction scalaire différentiable en $a$, le gradient est un vecteur ligne à $n$ composantes
	\[ (\mathrm{grad} f)(a) = \nabla f(a) = \left( \frac{\pa f}{\pa x_1}(a) , \dots, \frac{\pa f}{\pa x_n}(a) \right) \]
\end{mydef}

\begin{myform} On a que
	\[ D_df(a) = \dif f_a(d) = \nabla f(a) d \]
\end{myform}

\begin{myprop} Le gradient est parallèle à la direction de plus forte pente,
	celle où $D_df(a)$ est maximal
	\footnote{\c{C}a peut se prouver aisément en majorant $D_df(a)$ par l'inégalité de Cauchy et en disant qu'on doit être dans le cas d'égalité pour être maximum.}.
	Il est aussi orthogonal aux courbes de niveau de $f$ en $a$.
\end{myprop}

\subsection{Courbes et surfaces définies par un graphe}

\begin{myform}[Plan tangent à un graphe]
	Soit une fonction scalaire $f : A \subseteq \R^n \fl \R$ différentiable en $a$ un point intérieur de $A$. Le plan tangent au point $(a,f(a))$ est
	\begin{align*}
		z & = f(a) + \frac{\pa f}{\pa x_1}(a)(x_1 - a_1) + \cdots + \frac{\pa f}{\pa x_n}(a) (x_n - a_n) \\
		& = f(a) + \dif f_a(x-a) \\
		& = f(a) + \nabla f(a)(x-a)
	\end{align*}

	La normale de ce plan tangent est
	\[
	\begin{pmatrix}
		\nabla f(a) & -1
	\end{pmatrix}
	\]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Théorème des accroissement finis}

\begin{mytheo}[Théorème des accroissements finis] Soit une fonction scalaire $f : A \subseteq \R^n \fl \R$ continue sur l'intervalle fermé $[a,b]$ et différentiable sur l'intervalle $]a,b[$, $\exists c \in ]a,b[$ tel que
	\[ \dif f_c(b-a) = f(b) - f(a) \]
\end{mytheo}

\begin{mytheo}[Théorème des accroissements finis (formulation équivalente)] Soit $f$ une fonction scalaire. $\exists 0 < \theta < 1$ tel que
	\[ f(a+h) = f(a) + \dif f_{a + \theta h}(h) = f(a) + \nabla f(a + \theta h)h \]
\end{mytheo}

\begin{myprop}[Pour les fonctions vectorielles] On a seulement l'inégalité
	\[ || f(b) - f(a) || \leq || \dif f_c(b-a) || \]
\end{myprop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classe de fonction}

\begin{mydef}[Classe de fonctions $\mathcal{C}^k$]
	Une fonction $f$ est dite de classe $\mathcal{C}^k$ sur un ensemble ouvert $A$ si et seulement si toutes ses dérivées partielles $k$\ieme{} existent et sont continues sur $A$. On écrit alors
	\[ f \in \mathcal{C}^k(A) \]
\end{mydef}

\begin{myform}[Classe de fonction] On a les inclusions suivantes
	\[ \mathcal{C}^{\infty}(A) \subset \dots \subset \mathcal{C}^3(A) \subset \mathcal{C}^2(A) \subset \mathcal{C}^1(A) \subset \mathcal{C}^0(A) \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matrices hessiennes et jacobiennes}

\begin{mydef}[Matrice hessienne]
	Soit $f : A \subseteq \R^n \fl \R$ et $a$ un point intérieur de $A$. Si toutes les dérivées partielles secondes existent en $a$, la matrice hessienne est la matrice carrée de dimension $n$ suivante
	\[ Hf(a) = \nabla^2 f(a) = \begin{pmatrix}
		\frac{\pa^2 f}{\pa x_1^2}(a) & \frac{\pa^2 f}{\pa x_2 x_1}(a) & \cdots & \frac{\pa^2 f}{\pa x_n \pa x_1}(a) \\
		\frac{\pa^2 f}{\pa x_1 \pa x_2}(a) & \frac{\pa^2 f}{\pa x_2^2}(a) & \cdots & \frac{\pa^2 f}{\pa x_n \pa x_2}(a) \\
		\vdots & \vdots & \ddots & \vdots \\
		\frac{\pa^2 f}{\pa x_1 \pa x_n}(a) & \frac{\pa^2 f}{\pa x_2 \pa x_n}(a) & \cdots & \frac{\pa^2 f}{\pa x_n^2}(a) \\
	\end{pmatrix} \]
\end{mydef}

\begin{myprop}[Matrice symétrique]
	Soit $f : A \subseteq \R^n \fl \R$ de classe $\mathcal{C}^2(A)$. Alors
	\[ \frac{\pa^2 f}{\pa x_j \pa x_i}(a) = \frac{\pa^2 f}{\pa x_i \pa x_j}(a) \]
	pour tout $a \in A$ et pour tous $1 \leq i,j \leq n$. Dans ce cas, la matrice hesienne est symétrique.
\end{myprop}

\begin{myprop}[Ordres supérieurs]
	Si une fonction est de classe $\mathcal{C}^k$ alors on peut permuter les dérivées partielles d'ordre $k$.
\end{myprop}

\begin{mydef}[Matrice jacobienne]
	Soit une fonction $f : A \subseteq \R^n \fl \R^m$ différentiable en un point $a$ intérieur à $A$. La matrice jacobienne est une matrice $m \times n$ :
	\[ Jf(a) =
	\begin{pmatrix}
		\frac{\pa f_1}{\pa x_1}(a) & \frac{\pa f_1}{\pa x_2}(a) & \cdots & \frac{\pa f_1}{\pa x_n}(a) \\
		\frac{\pa f_2}{\pa x_1}(a) & \frac{\pa f_2}{\pa x_2}(a) & \cdots & \frac{\pa f_2}{\pa x_n}(a) \\
		\vdots & \vdots & \ddots & \vdots \\
		\frac{\pa f_m}{\pa x_1}(a) & \frac{\pa f_m}{\pa x_2}(a) & \cdots & \frac{\pa f_m}{\pa x_n}(a) \\
	\end{pmatrix} \text{avec } f =
	\begin{pmatrix}
		f_1 \\ f_2 \\ \vdots \\ f_m
	\end{pmatrix} \]
\end{mydef}

\begin{myform}[Lien avec gradient, dérivées partielles, ...]
	On a les formules suivantes :
	\[ Jf(a) =
	\begin{pmatrix} \nabla f_1(a) \\ \nabla f_2(a) \\ \vdots \\ \nabla f_m(a)
	\end{pmatrix} \]
	\[ Jf(a) =
	\begin{pmatrix} \frac{\pa f}{\pa x_1}(a) & \frac{\pa f}{\pa x_2}(a) & \cdots & \frac{\pa f}{\pa x_n}(a)
	\end{pmatrix} \]
	Pour chaque composante scalaire d'une fonction vectorielle
	\[ \dif f_a(h) = Jf(a) h \]
	La matrice jacobienne est donc la matrice associée à l'application linéaire $\dif f_a$.
	Pour une fonction à une seule variable représentant une courbe paramétrée de $\R^n$ :
	\[ Jf(a) = f'(a) \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Développement de Taylor}

\begin{myform}[Développement avec reste d'ordre 1]
	Soit $A \subseteq \R^n$ un ensemble ouvert, $f : A \subseteq \R^n \fl \R$ une fonction scalaire de classe $\mathcal{C}^1$, $a \in \R^n$ un point de $A$ et $h$ un vecteur de $\R^n$ tel que $[a, a+h] \subseteq A$. $\exists 0 < \theta < 1$ tel que
	\begin{align*} f(a+h) & = f(a) + \nabla f(a+\theta h)h \\
		& = f(a) + \sum_{i =1}^n \frac{\pa f}{\pa x_i} (a+\theta h)h_i
	\end{align*}
\end{myform}

\begin{myform}[Développement avec reste d'ordre 2]
	Soit $f$ une fonction de classe $\mathcal{C}^2$.
	\begin{align*} f(a+h) & = f(a) + \nabla f(a) h + \frac 12 h \nabla^2 f(a+\theta h) h \\
		& = f(a) + \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a)h_i + \frac 12 \sum_{i=1}^n \sum_{j=1}^n \frac{\pa^2 f}{\pa x_i \pa x_j} (a+ \theta h) h_i h_j
	\end{align*}
\end{myform}

\begin{myform}[Développement avec reste d'ordre 3]
	Soit $f$ une fonction de classe $\mathcal{C}^3$.
	\begin{align*} f(a+h) & = f(a) + \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a)h_i + \frac 12 \sum_{i=1}^n \sum_{j=1}^n \frac{\pa^2 f}{\pa x_i \pa x_j} (a) h_i h_j \\ & + \frac{1}{3!} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \frac{\pa^3 f}{\pa x_i \pa x_j \pa x_k}(a+\theta h)h_i h_j h_k
	\end{align*}
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Composition (\emph{chain rule})}

\begin{myform}[\emph{Chain rule}]
	Pour composer des applications linéaires, on a la formule
	\[ J(g \circ f)(a) = Jg(f(a)) \cdot Jf(a) \]
	Si on explicite les composantes $f_1, f_2, \dots , f_m$ de $f$ avec
	\[ g(f(x)) = g( f_1(x_1, \dots, x_n), f_2(x_1, \dots, x_n), \dots, f_m(x_1, \dots, x_n)) \]
	et $g$ définie de sorte qu'on ait $g : (y_1, y_2, \dots , y_m) \rightarrow g(y_1, y_2, \dots, y_m)$. Donc $x$ sont les variables naturelles de $f$ et $y$ celles de $g$.
	Alors on a la formule\footnote{Notez la différence par rapport aux slides du cours, où les $y_1, y_2, \dots, y_n$ remplacent les $x_1, x_2, \dots, x_n$ pour na pas les confondre avec les variables de $f$}
	\[ \frac{\pa (g \circ f)}{\pa x_i}(a) = \frac{ \pa g}{\pa y_1}(f(a)) \frac{\pa f_1}{\pa x_i}(a) + \frac{ \pa g}{\pa y_2}(f(a)) \frac{\pa f_2}{\pa x_i}(a) + \cdots + \frac{ \pa g}{\pa y_m}(f(a)) \frac{\pa f_m}{\pa x_i}(a) \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimisation}

\subsection{Définitions}

On ne parlera ici que de fonction scalaire, donc de
\[ f : A \subseteq \R ^n \fl \R \]

\begin{mydef}[Extrémum global] Un extremum global est un point où $f$ prend une valeur extrême ; c'est à dire un point $a$ tel que
	\[ f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)}
	\end{array} \]
\end{mydef}

\begin{mydef}[Extrémum local] Un extremum local est un point où $f$ prend une valeur extrême sur un voisinage; c'est à dire un point $a$ tel que
	\[ \exists r >0 \; \text{tel que} \; f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in B(a;r) \cap A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)}
	\end{array} \]
\end{mydef}

\begin{mydef}[Extréma locaux libres]
	Un extremum local libre de $f$ est un point $a \in \newint A$ tel que
	\[ \exists r > 0 \; \text{tel que} \; f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in B(a;r)\cap A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)}
	\end{array} \]
\end{mydef}

\begin{mydef}[Point de selle] $a \in \newint A$ est un point de selle si, $\forall r >0, \exists x_+ \in B(a;r) \; \text{et} \; x_- \in B(a;r)$ tels que
	\[ f(x_-) < f(a) < f(x_+) \]
\end{mydef}

\begin{mydef}[Point critique] Un point critique est un point où le gradient, s'il existe, est nul.
\end{mydef}

\begin{mydef}[Point singulier] Un point singulier est un point du domaine où le gradient n'existe pas.
\end{mydef}

\begin{mydef}[Ensemble compact] Un  ensemble est compact s'il est fermé et borné, c'est à dire qu'il ne <<~tend pas vers l'infini~>> et qu'il contient tous ses points frontières.
\end{mydef}

\subsection{Conditions pour trouver des extrema}

\begin{myprop}[Condition nécessaire]
	Soit $f \in \mathcal{C}^2$, $a \in \newint A$. Pour que $a$ soit un extremum, il est nécessaire que
	\[ \nabla f(a) = 0 \]
\end{myprop}

\begin{myprop}[Propriété de Fermat]
	Soit $a \in \newint A$, extrémum local libre. Soit $f$ une direction de $\R^n$ ; si $D_df(a)$ existe, elle est nulle. Donc, $\frac{\pa f}{\pa x_i}(a) = 0$, $\nabla f(a) = 0$ et $\dif f_a = 0$ (s'ils existent).
\end{myprop}

\begin{myrem}
	Cette condition n'est pas applicable aux points frontières où le calcul différentiel n'apporte pas d'informations.
\end{myrem}

\begin{myprop}[Conditions nécessaires à la détermination des minima et maxima]
	Soit $a$ un point critique. Pour que $a$ soit un minimum (resp. un maximum), il faut que $\nabla^2 f(a)$ soit semi-définie positive (resp. négative).
\end{myprop}

\begin{myprop}[Conditions suffisantes à la déterminations des minima, maxima et points de selle]
	Si la hessienne est définie (à un point critique)
	\begin{itemize}
		\item strictement positive, ceci garanti un minimum ;
		\item strictement négative, ceci garanti un maximum ;
		\item indéfinie, ceci garanti un point de selle.
	\end{itemize}
\end{myprop}

\begin{myprop}[Point singulier]
	Dans le cas d'un point singulier, la détermination doit se faire à la main. Il n'y a aucun outil particulier.
\end{myprop}

\begin{mytheo}[Existence d'un maximum et d'un minimum]
	Si $f$ est continue et que $A$ est compact, alors $f(A)$ est compact. Dans ce cas, $f$ admet un maximum et un minimum sur le domaine $A$.
\end{mytheo}

\subsubsection{Résumé --- méthode}

\begin{itemize}
	\item Calculer le gradient de $f$, trouver les points critiques et singuliers ;
	\item Pour les points singuliers, on ne peut conclure qu'avec un raisonnement \emph{ad hoc} ;
	\item Pour les points critiques, on calcule la hessienne de $f$ en ces points :
		\begin{itemize}
			\item Si elle est strictement positive, négative ou indéfinie, on a respectivement un minimum, un maximum ou un point de selle. Dans ce cas, on conclut ;
			\item Si on ne peut pas conclure par le point précédent, on peut utiliser le théorème des accroissements finis, ou tenter un raisonnement \emph{ad hoc}.
		\end{itemize}

\end{itemize}

\subsection{Optimisation sous contrainte}

Soit $f : A \subseteq \R^n \fl \R$ une fonction scalaire et $g : A \subseteq \R^n \fl \R$ les contraintes ($g(x) = 0$). On défini l'ensemble admissible

\begin{mydef}[Ensemble admissible] L'ensemble admissible est
	\[ \Phi = \{ x \in A | g(x) = 0 \} \]
\end{mydef}

\begin{mydef}[Lagrangien] Le lagrangien est la fonction
	\[ \mathcal{L} : A \times \R \fl \R : (x, \lambda) \fl L(x, \lambda) = f(x) - \lambda g(x) \]
\end{mydef}

\begin{myprop}[Condition nécessaire]
	Si $a$ est un extremum local de $f$ sur $\Phi$ et si
	\begin{itemize}
		\item $\exists r >0$ tel que $f, g$ sont différentiables
			sur $B(a;r)$ ;
		\item les dérivées partielles de $f$ et $g$ existent et sont continues en $a$ ;
		\item $\nabla g(a) \neq 0$ ;
	\end{itemize}
	alors $\exists \lambda$ tel que $(a,\lambda)$ est un point critique de $\mathcal{L}(x,\lambda)$ ou $\nabla \mathcal{L}(a,\lambda) = 0$.
\end{myprop}

\begin{myrem}[Hessienne et Lagrangien]
	Attention on ne peut absolument pas utiliser la hessienne pour conclure à propos des optimisations sous contraintes résolues avec l'aide du Lagrangien.
\end{myrem}

\begin{myprop}[Contraintes multiples]
	Dans ce cas, on applique plusieurs fois le Lagrangien d'affilé :
	\[ \mathcal{L} : A \times \R^m \fl \R : (x, \lambda) \fl L(x, \lambda) = f(x) - \lambda_1 g_1(x) - \lambda_2 g_2(x) - \cdots - \lambda_m g_m(x) \]
	Dans ce cas, il ne faut pas que le gradient (de la contrainte) soit non-nul, mais que
	\[ \newrang J_g(a) = m \]
	c'est à dire que la Jacobienne des contraintes doit être de rang plein (ou, intuitivement, que les contraintes doivent être indépendantes\footnote{\emph{cf.} cours de mécanique avec la technique des multiplicateurs de Lagrange}).
\end{myprop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Calcul intégral et intégrales multiples}

\section{Généralités}

\begin{mydef}[Fonction intégrable] $f$ est intégrable sur le rectangle $D$ et a pour intégrale double la valeur $I$
	\[ I = \iint_D f(x,y) \dif A \]
	si et seulement si $\forall \epsilon > 0, \exists \delta > 0$ tel que pour toute partition $P \subset D$ tel que $||P|| < \delta$ et pour tous choix des points $(x_{ij}^{*},y_{ij}^{*})$ dans $R_{ij} \subset P$ on a
	\[ |R(f,C,P) - I| < \epsilon \]
\end{mydef}

\begin{myrem}
	\[ \dif A = \dif x \dif y = \dif y \dif x \]
\end{myrem}

\begin{mytheo}[Fonction intégrable et continue]
	Si $f$ est continue sur $D$ alors $f$ est intégrable sur $D$
\end{mytheo}

\begin{myprop} $f$ est intégrable sur un domaine quelconque $D$ si et seulement si $\hat{f}$ est intégrable sur un rectangle $R$ avec
	\[ \hat{f} : (x,y) \fl \hat{f}(x,y) = \left\{
	\begin{array}{lll} f(x,y) & \text{si} & (x,y) \in D \\ 0 & \text{si} & (x,y) \notin D  \; \text{et} \; (x,y) \in R
	\end{array}
	\right.
	\]
	et dans ce cas
	\[ \iint_D f(x,y) \dif A = \iint_R \hat{f}(x,y) \dif A \]
\end{myprop}

\begin{myrem}
	Ces propriétés sont <<~extensibles~>> à $n$ dimensions.
\end{myrem}

\begin{myform}[Dépendance linéaire de l'intégrant]
	$\forall L, M \in \R$ on a
	\[ \iint_D (L f(x,y) + M g(x,y)) \dif A = L \iint_D f(x,y) \dif A + M \iint_D g(x,y) \dif A \]
\end{myform}

\begin{myform}[Autres formules] On a, si $f \leq g$ sur $D$ :
	\[ \iint_D f(x,y) \dif A \leq \iint_D g(x,y) \dif A \]

	Si $D = D_1 \cup D_2 \cup \dots \cup D_k$ avec $D_i \cap D_j = \varnothing \; \forall i \neq j$, si $f$ est intégrale sur $D_j$ $\forall j = 1, \dots, k$ alors
	\[ \iint_D f(x,y) \dif A = \sum_{j=1}^k \iint_{D_j} f(x,y) \dif A \]

	On a également que
	\[ \left| \iint_D f(x,y) \dif A \right| \leq \iint \left| f(x,y) \right| \dif A \]
\end{myform}

\begin{mytheo}[Théorème de Fubini] Soit $f$ définie sur un pavé $D = [a_1,b_1] \times [a_2,b_2] \times \cdots \times [a_n,b_n] \subset \R^n$, $f$ continue sur $D$. Alors pour toute permutation $\sigma$ de la suite des nombres entiers $(1,2,\dots, n)$, on a
	\[ \int_D f = \int_{\sigma(1)}^{\sigma(1)} \left[ \int_{\sigma(2)}^{\sigma(2)}   \left[ \dots \left[ \int_{\sigma(n)}^{\sigma(n)} f(x_1,x_2,\dots,x_m) \dif x_{\sigma(n)} \right]   \dots \right] \dif x_{\sigma(2)} \right] \dif x_{\sigma(1)} \]
\end{mytheo}

\section{Méthodes de calcul}

\subsection{Par inspection} Si la fonction présente une symétrie, soit sur elle même soit sur son domaine, on peut travailler par <<~morceaux~>>.

\subsection{Par itération}

\begin{mydef}[Domaine x ou y-simple]
	Le domaine d'une fonction est
	\begin{itemize}
		\item y-simple si $x \in [a;b]$ et $y \in [c(x);d(x)]$ ;
		\item x-simple si $y \in [c;d]$ et $x \in [a(y);b(y)]$.
	\end{itemize}
\end{mydef}

\begin{myprop} Si $f$ est définie sur un domaine x-simple ou y-simple, alors on peut calculer l'intégrale comme ceci : soit $D$ un domaine y-simple,
	\begin{align*} \iint_D f(x,y)\dif A & = \int_a^b \left[ \int_{c(x)}^{d(x)} f(x,y) \dif y \right] \dif x \\
		& = \int_a^b \dif x \int_{c(x)}^{d(x)} f(x,y) \dif y
	\end{align*}
	De la même manière, si le domaine $D$ est x-simple, on a
	\[ \iint_D f(x,y)\dif A  = \int_c^d \dif y \int_{a(y)}^{b(y)} f(x,y) \dif x \]
\end{myprop}

\subsection{Par changement de variable}
Soit une intégrale
\[ \iint_D f(x,y) \dif A \]
à calculer. On va faire effectuer une transformation \emph{bijective} (ou \emph{mapping}) et ce en deux étapes.
\begin{enumerate}
	\item on pose
		\[ f(x,y) = f(x(u,v),y(u,v)) = g(u,v) \]
	\item on exprime ensuite $\dif A$ en fonction de $\dif u$ et $\dif v$. Pour ce faire, on a la formule
		\[ \dif A = |\newdet(J)| \dif u \dif v \]
		avec $J$ la jacobienne suivante :
		\[ J =
		\begin{pmatrix}
			\frac{\pa x}{\pa u} & \frac{\pa x}{\pa v} \\ \frac{\pa y}{\pa u} & \frac{\pa y}{\pa v}
		\end{pmatrix}
		\overset{\Delta}{=} \frac{\pa(x,y)}{\pa (u,v)} \]
\end{enumerate}

\section{Intégrale de lignes et de surfaces}

\subsection{Définitions et théorèmes généraux}

\begin{mydef}[Champ de vecteurs conservatifs] $\vv{F}$ est un champ de vecteurs conservatifs si et seulement si il existe $\Phi$, une fonction scalaire, telle que
	\[ \vv{F} = \nabla \Phi \]
	On dit alors que $\Phi$ est un potentiel scalaire et que $F$ dérive d'un potentiel.
	Dans ce cas, on a que, si $C$ est une courbe dont les extrémités sont $P_1$ et $P_2$,
	\[ \int_C F \cdot \dif r = \Phi(P_2) - \Phi(P_1) \]
	De plus il y a indépendance des chemins choisis.
\end{mydef}

\begin{myprop}[Champ de vecteurs conservatifs]
	Si $P_1 = P_2$, alors
	\[ \oint F \cdot \dif r = 0 \]

	Si $C = C_1 - C_2$, alors
	\[ \int_C F \cdot \dif r = 0 \Leftrightarrow \int_{C_1} F \cdot \dif r - \int_{C_2} F \cdot \dif r = 0 \]
\end{myprop}

\begin{mytheo} Soit $D$ ouvert et connexe de $\R^3$ et $\vv{F}$ un champ vectoriel défini sur $D$. Il y a équivalence des trois énoncés suivants :
	\begin{itemize}
		\item $\vv{F}$ est conservatif sur $D$, $\exists \Phi$ tel que $\nabla \Phi = \vv{F}$ ;
		\item $\oint_C \vv{F} \cdot \dif r = 0$, pour toute courbe fermée dans $D$ ;
		\item pour tout points $P_0$, $P_1$ dans $D$, $\int_C F \dif r$ a la même valeur quelque soit le chemin $C$ entre $P_0$ et $P_1$.
	\end{itemize}
\end{mytheo}

\begin{mydef}[Ensemble simplement connexe] C'est un ensemble dans lequel toute courbe fermée qui ne s'intersecte pas peut être transformée de manière continue en un seul point de $D$ sans quitter $D$ (par exemple, $\R^2$ est simplement connexe, mais $\R^2\backslash (0;0)$ pas).
\end{mydef}

\begin{mytheo}[Théorème de Pointcarré] Soit $D$ \emph{simplement} connexe, $F$ un champ vectoriel défini sur $D$, alors on a une 4\ieme{} équivalence aux trois précédentes
	\[ \frac{\pa F_i}{\pa x_j} = \frac{\pa F_j}{\pa x_i} \]
	pour $i,j = 1,2,3$.
\end{mytheo}

\subsection{Intégrale de ligne d'un champ scalaire}

\begin{mytheo}[Existence de l'intégrale de ligne]
	Soit $C$ une courbe de classe $\mathcal{C}^1$ et $f$ une fonction continue, alors
	\[ \int_C f \dif r \]
	existe.
\end{mytheo}

\subsubsection{Méthode de calcul}

On paramétrise la courbe $C$ par une fonction $r : t \fl r(t)$ avec $t \in [a;b]$ selon les limites du problème. On calcule ensuite l'intégrale par la formule
\[ \int_C f(x,y,z) \dif r = \int_a^b f(r(t)) \left|\left| \frac{\dif r}{\dif t} \right|\right| \dif t \]

\subsection{Intégrale de surface d'un champ scalaire}

On veut donc calculer
\[ \iint_S f(x,y,z) \dif S \]

\subsubsection{Méthode de calcul}

On procède en deux étapes.
\begin{enumerate}
	\item Pour cela on doit paramétrer la surface : soit $r : (u,v) \fl r(u,v)$ avec $(u,v) \in R$, représentation paramétrique de la surface. On a donc
		\[ r(u,v) = x(u,v) \hat{\imath} + y(u,v) \hat{\jmath} + z(u,v) \hat{\kmath} \]
	\item On exprime ensuite $\dif S$ en fonction de $\dif u$ et $\dif v$ par la formule
		\begin{align*} \dif S & = \left| \left| \frac{\pa r}{\pa u} \dif u \times \frac{\pa r}{\pa v} \dif v \right| \right| \\
			& = \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right|  \dif u \dif v
		\end{align*}
\end{enumerate}
On a donc finalement la formule suivante :
\[ \iint_S f(x,y,z) \dif S = \iint_R f(r(u,v)) \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right|  \dif u \dif v \]

\subsection{Intégrale de ligne de vecteur le long d'une courbe}

On veut donc calculer la composante tangentielle de vecteur le long d'une courbe
\[ \int_C \vv{F} \cdot \dif \vv{r} \]

\subsubsection{Méthode de calcul}

Comme auparavant, on paramétrise la courbe par une fonction $r : t \fl r(t) = x(t) \hat{\imath} + y(t) \hat{\jmath} + z(t) \hat{\kmath}$ avec $t \in [a;b]$
Et on a donc à calculer
\[ \int_C \vv{F} \cdot \dif \vv{r} = \int_a^b \vv{F}(r(t)) \cdot \left(  \frac{\dif \vv{r}}{\dif t} \right) \dif t \]

\subsection{Intégrale de champ de vecteur à travers une surface}

On veut calculer l'intégrale de la composante normale d'un champ vectoriel à une surface, soit
\[ \iint_S \vv{F} \cdot \hat{N} \dif S = \iint_S \vv{F} \cdot \dif \vv{S} \]

\subsubsection{Méthode de calcul}

On paramétrise la surface par une fonction $r : (u,v) \fl r(u,v) = x(u,v) \hat{\imath} + y(u,v) \hat{\jmath} + z(u,v) \hat{\kmath}$ avec $(u,v) \in D$. Le flux est alors donné par
\[ \iint_S \vv{F} \cdot \dif \vv{S} = \pm \iint_D \vv{F}(r(u,v)) \cdot \left( \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right) \dif u \dif v \]

\subsection{Résumé}

\begin{center}
	\begin{tabular}{|S c|S c|}
		\hline
		\textbf{Type} & \textbf{Formule} \\
		\hline
		Scalaire sur une ligne & $\displaystyle \int_C f(x,y,z) \dif r = \int_a^b f(r(t)) \left|\left| \frac{\dif r}{\dif t} \right| \right| \dif t$ \\
		\hline
		Scalaire sur une surface & $\displaystyle \iint_S f(x,y,z) \dif S = \iint_R f(r(u,v)) \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right| \dif u \dif v $ \\
		\hline
		Vectoriel sur une ligne & $ \displaystyle \int_C \vv{F} \cdot \dif \vv{r} = \int_a^b \vv{F}(r(t)) \cdot \left(  \frac{\dif \vv{r}}{\dif t} \right) \dif t $ \\
		\hline
		Vectoriel sur une surface & $ \displaystyle \iint_S \vv{F} \cdot \dif \vv{S} = \pm \iint_D \vv{F}(r(u,v)) \cdot \left( \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right) \dif u \dif v $ \\
		\hline
	\end{tabular}
\end{center}

\section{Analyse vectorielle}

\begin{mydef}[gradient] On défini le gradient de $f$ comme
	\[ \nabla f = \left( \frac{\pa f}{\pa x} , \frac{\pa f}{\pa y} , \frac{\pa f}{\pa z} \right) \]
\end{mydef}

\begin{mydef}[Divergence] On défini la divergence de $\vv{F}$ comme
	\begin{align*} \div \vv{F} & = \nabla \cdot \vv{F} \\
		& = \left( \frac{\pa \cdot}{\pa x} \hat{\imath} + \frac{\pa \cdot}{\pa y} \hat{\jmath} + \frac{\pa \cdot}{\pa z} \hat{\kmath} \right) \cdot \left( F_1 \hat{\imath} + F_2 \hat{\jmath} + F_3 \hat{\kmath} \right) \\
		& = \frac{\pa F_1}{\pa x} + \frac{\pa F_2}{\pa y} + \frac{\pa F_3}{\pa z} \\
	\end{align*}
\end{mydef}

\begin{mydef}[Rotationnel] On défini le rotationnel de $\vv{F}$ comme
	\begin{align*} \rot \vv{F} & = \nabla \times \vv{F} \\
		& = \left( \frac{\pa F_3}{\pa y} -  \frac{\pa F_2}{\pa z} \right) \hat{\imath} + \left( \frac{\pa F_1}{\pa z} -  \frac{\pa F_3}{\pa x} \right) \hat{\jmath} + \left( \frac{\pa F_2}{\pa x} -  \frac{\pa F_1}{\pa y} \right) \hat{\kmath}
	\end{align*}
\end{mydef}

\begin{myrem}[Produit vectoriel]
	\[ \left( \frac{\pa F_3}{\pa y} -  \frac{\pa F_2}{\pa z} \right) \hat{\imath} + \left( \frac{\pa F_1}{\pa z} -  \frac{\pa F_3}{\pa x} \right) \hat{\jmath} + \left( \frac{\pa F_2}{\pa x} -  \frac{\pa F_1}{\pa y} \right) \hat{\kmath} =
	\left|
	\begin{array}{ccc}
		\hat{\imath} & \hat{\jmath} & \hat{\kmath} \\ \frac{\pa \cdot}{\pa x} & \frac{\pa \cdot}{\pa y} & \frac{\pa \cdot}{\pa z} \\ F_1 & F_2 & F_3
	\end{array}
	\right| \]
\end{myrem}

\begin{myrem}[En 2 dimensions]
	En deux dimensions, on a
	\[ \div \vv{F} = \frac{\pa F_1}{\pa x} + \frac{\pa F_2}{\pa y} \]
	\[ \rot \vv{F} = \left( \frac{\pa F_2}{\pa x} - \frac{\pa F_1}{\pa y} \right) \hat{k} \]
\end{myrem}

\begin{myform}[Formules générales]
	\[ \nabla (\vv{F} \times \vv{G}) = (\nabla \times \vv{F})\vv{G} - \vv{F} \cdot (\nabla \times \vv{G}) \]
	\[ \div ( \rot \vv{F}) = \nabla (\nabla \times \vv{F}) = 0 \]
	\[ \rot (\nabla \Phi) = \nabla \times (\nabla \Phi) = 0 \]
\end{myform}

\begin{mydef}[Champ de vecteurs solénoïdal ou incompressible] Soit $\vv{F}$ un champ de vecteurs tel que
	\[ \div \vv{F} = 0 \]
	dans $D$ est dit incompressible ou solénoïdal.
\end{mydef}

\begin{mydef}[Champ de vecteurs irrotationnel] Soit $\vv{F}$ un champ de vecteurs tel que
	\[ \rot \vv{F} = 0 \]
	dans $D$ est dit irrotationnel.
\end{mydef}

\begin{myprop}[Champ de vecteurs solénoïdal et irrotationnel] On a les implications suivantes :
	\begin{itemize}
		\item $ \vv{F} \; \text{irrotationnel} \Rightarrow \vv{F} \; \text{conservatif (sur un domaine simplement connexe et ouvert)} $ ;
		\item $ \vv{F} \;\text{conservatif} \Rightarrow \vv{F} \;\text{irrotationnel} $ ;
		\item $ \div (\rot \vv{G}) = 0  \Rightarrow \text{Le rotationnel d'un champ de vecteur est solénoïdal} $ ;
		\item $ \vv{F} \;\text{est un champ vect. solénoïdal} \Rightarrow \div (\rot \vv{G}) = 0 \;\text{(sur un domaine étoilé)} $, ou encore : il existe $\vv{G}$ tel que $\vv{F} = \rot \vv{G}$.
	\end{itemize}
\end{myprop}

\begin{mydef}[Domaine étoilé]
	Un domaine est dit étoile si il existe $P_0$ dans $D$ tel que $P_0 + t(P - P_0)$, reliant $P_0$ à tout $P \in D$ est entièrement contenue dans $D$.
\end{mydef}

\begin{myprop}[Potentiel vecteur]
	Si $\vv{F}$ est un champ vectoriel solénoïdal sur $D$, domaine étoilé, alors il existe un potentiel vecteur $\vv{G}$ tel que $\vv{F} = \rot \vv{G}$.
\end{myprop}

\begin{myform}[Fonction gradient]
	Une fonction $\vv{f}$ est dit être un gradient si il existe $\Phi$ telle que
	\[ \vv{f} = \nabla \Phi \]
	Si $\vv{f}$ est définie sur un domaine simplement connexe et que $\rot \vv{f} = 0$, alors c'est un gradient.
	Si c'est le cas, et si en plus $\vv{f}$ est définie sur un domaine étoilé, on peut déterminer $\Phi$ par
	\[ \Phi = \int_0^1 \left( x F_1(xt,yt,zt) + y F_2(xt,yt,zt) + z F_3(xt,yt,zt) \right) \dif t \]
\end{myform}

\section{Théorèmes intégraux}

\begin{mytheo}[Théorème de Green]
	On travaille dans $\R^2$. Soit $R$ une région du plan avec un bord $C$ orienté. Soit $\vv{F}$ un champ de vecteurs. On a l'égalité
	\[ \iint_R \rot \vv{F} \cdot \hat{k} \dif A = \oint_C \vv{F} \cdot \dif \vv{r} \]
\end{mytheo}

\begin{myrem}[Domaines adjacents]
	Si on a deux domaines adjacent, $R_1$ et $R_2$, alors le théorème est valable sur leur union, le bord adjacent étant <<~annulé~>>.
\end{myrem}

\begin{mytheo}[Théorème de Stokes]
	Soit $S$ une surface de $\R^3$ et soit $C$ le contour orienté de cette surface. Alors
	\[ \iint_S \rot \vv{F} \cdot \hat{N} \dif S = \oint_C \vv{F} \dif \vv{r} \]
\end{mytheo}

\begin{myprop}[Application du théorème de Stockes]
	On peut calculer l'intégrale compliquée du rotationnel d'un champ de vecteur sur une surface et changeant la surface. Soit $S_1$ et $S_2$ deux surfaces possédant le même contour fermé $C$. Alors on a
	\[ \iint_{S_1} \rot \vv{F} \cdot \hat{N} \dif S = \oint_C \vv{F} \cdot \dif \vv{r} = \iint_{S_2} \rot \vv{F} \cdot \hat{N} \dif S \]
\end{myprop}

\section{Rotationnel et divergence}

\begin{myprop}[Rotationnel]
	Le rotationnel c'est le circulation d'un vecteur par unité de surface. C'est donc la densité de circulation. On a
	\[ \rot \vv{P} \cdot \vv{N} = \lim_{\epsilon \fl 0} \frac 1{\pi \epsilon^2} \oint_{C_\epsilon} \vv{F} \dif \vv{r} \]
	avec $C_\epsilon$ un cercle de rayon $\epsilon$ centré en $P$.
\end{myprop}

\begin{mytheo}[Théorème de la divergence]

	Soit $D$ un volume de $\R^3$, $S$ (surface fermée) le bord de $D$, et $\hat{N}$ la normale à cette surface fermée. Alors
	\[ \iiint_D \div \vv{F} \dif V = \oiint_S \vv{F} \cdot \hat{N} \dif S \]

\end{mytheo}

\begin{myprop}[Divergence]
	La divergence, c'est la densité de flux par unité de volume. Soit $D_{\epsilon}$ la boule de rayon $\epsilon$ et $S_{\epsilon}$ la surface de cette boule. On a
	\[ \div \vv{F}(P) = \lim_{\epsilon \fl 0} \frac{3}{4 \pi \epsilon^3} \oiint_{S_{\epsilon}} \vv{F} \cdot \hat{N} \dif S \]
\end{myprop}

\end{document}
