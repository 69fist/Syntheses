\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{subfigure}
\usepackage{esint}
\usepackage{esvect}
\usepackage{cellspace}
\usepackage{tabularx}
\usepackage{graphicx}
\addparagraphcolumntypes{X}
\cellspacetoplimit=2pt
\cellspacebottomlimit=2pt
%\DeclareRobustCommand{\[}{\noindent\begin{equation*}}
%\DeclareRobustCommand{\]}{\end{equation*}}
\usepackage[pdfauthor={Benoît Legat, Nicolas Cognaux et Léopold Cambier},
pdftitle={Synthèse de Mécanique Q2 - LFSAB1202},
pdfsubject={Mathématique}]{hyperref}
\usepackage{appendix}

\theoremstyle{definition}
\newtheorem{mydef}{Définition}[section]
\newtheorem{mynota}[mydef]{Notation}
\newtheorem{myprop}[mydef]{Propriétés}
\newtheorem{myrem}[mydef]{Remarque}
\newtheorem{myform}[mydef]{Formules}
\newtheorem{mycorr}[mydef]{Corrolaire}
\newtheorem{mytheo}[mydef]{Théorème}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\newIm}{Im}
\DeclareMathOperator{\asin}{asin}
\DeclareMathOperator{\acos}{acos}
\DeclareMathOperator{\atan}{atan}
\DeclareMathOperator{\acot}{acot}
\DeclareMathOperator{\cis}{cis}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\newdet}{det}
\DeclareMathOperator{\p}{p}
\DeclareMathOperator{\dom}{dom}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\kmath}{k}

\DeclareMathOperator{\newker}{Ker}
\DeclareMathOperator{\newim}{Im}
\DeclareMathOperator{\newdim}{dim}
\DeclareMathOperator{\newrang}{rang}
\DeclareMathOperator{\newint}{int}
\renewcommand{\div}{\mathrm{div}}
\newcommand{\rot}{\vv{\mathrm{rot}}\;} 

\let\oldnabla\nabla
\renewcommand{\nabla}{\vv{\oldnabla}}

\renewcommand{\i}{\mathrm{i}}
\newcommand{\fl}{\rightarrow}
\newcommand{\pa}{\partial}
\newcommand{\C}{\mathbb{C}}

%% Pour liste à la ligne
\newcommand*\InsertTheoremBreak{%
  \begingroup % keep changes local
    \setlength\itemsep{0pt}%
    \setlength\parsep{0pt}%
    \item[\vbox{\null}]%
  \endgroup%
 }%

\title{Synthèse de Mathématique Q2 - LFSAB1102}
\author{Benoit Legat \and Nicolas Cognaux \and Léopold Cambier}

\begin{document}
\maketitle

%       + <- x
%      /|
%     /è|
%    /gb|
%   /l r| <- x - P_V(x) \in V^\perp
%  /A  e|
% +-----+ <- P_V(x)
\part{Algèbre}

\section{Espaces euclidiens}
\begin{mydef}[Espace euclidien]
	Un espace euclidien $E$ est un espace vectoriel réel muni d'une fonction
	$(-|-) : E \times E \to \mathbb{R} : (x, y) \mapsto (x|y)$ qui est
	\begin{description}
		\item[Bilineaire]
			$\forall x, y, z \in E, \alpha, \beta \in \mathbb{R}$
			\begin{eqnarray*}
				(\alpha x + \beta y | z) & = & \alpha (x | z) + \beta (y | z)\\
				(x | \alpha y + \beta z) & = & \alpha (x | y) + \beta (x | z)
			\end{eqnarray*}
		\item[Symetrique]
			$\forall x,y \in E$
			$$(x|y) = (y|x)$$
		\item[Defini positif]
			$\forall x \in E \setminus \{0\}$
			$$(x|x) > 0$$
	\end{description}
	C'est le produit scalaire.
\end{mydef}

\begin{myrem}
	Notons que le fait d'être symétrique et linéaire à gauche (resp. à droite) entraine automatiquement la linéarité à droite (resp. à gauche).
\end{myrem}


\subsection{Produit scalaire}

\begin{mydef}
	Soient $E$ un espace euclidien, $V \subseteq E$ et $x,y \in E$.
	La norme d'un vecteur et la distance entre deux points sont définis respectivement comme suit
	\begin{eqnarray*}
		||x|| & \stackrel{\Delta}{=} & \sqrt{(x|x)}\\
		\dist(x, y) & \stackrel{\Delta}{=} & ||x - y||\\
	\end{eqnarray*}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien et $x,y \in E$.
	\begin{itemize}
		\item $||x|| \geq 0$
		\item $\dist(x, y) \geq 0$
		\item $x \neq 0 \iff ||x|| > 0$
		\item $x \neq y \iff \dist(x, y) > 0$
		\item $||\alpha x|| = \alpha||x||$
		\item $|(x | y)| \leq ||x||\cdot||y||$ (Inégalite de Cauchy)
			\footnote{Le cas d'égalité se fait si et seulement si $x$ est parallèle à $y$.}
		\item $||x + y|| \leq ||x|| + ||y||$ (Inégalite triangulaire)
	\end{itemize}
\end{myprop}

\subsection{Orthogonalité}

\begin{mydef}
	Soient $E$ un espace euclidien, $V \subseteq E$ et $x,y \in E$.
	L'orthogonalité entre deux vecteurs et l'espace orthogonal sont définis respectivement comme suit
	\begin{eqnarray*}
		x \perp y & \stackrel{\Delta}{\Leftrightarrow} & (x|y) = 0\\
		V^{\perp} & \stackrel{\Delta}{=} & \left\{x \in E | x \perp v, \forall v \in V\right\}
	\end{eqnarray*}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien et $V \subseteq E$.
	\begin{itemize}
		\item $V \cap V^{\perp} \subseteq \{0\}$ avec égalite $\iff 0 \in V$;
		\item $E^{\perp} = \{0\}$;
		\item $\{0\}^{\perp} = E$;
		\item $V^{\perp}$ est un sev de $E$;
		\item $V \subseteq \left( V^{\perp} \right) ^{\perp}$;
		\item $V_1 \subseteq V_2 \Rightarrow V_1^{\perp} \supseteq V_2^{\perp}$.
	\end{itemize}
	Si $V$ est un sev de $E$
	\begin{itemize}
		\item $E = V + V^{\perp}$ et cette somme est une somme directe;
		\item
			Si $E$ est de dimension finie
			\begin{itemize}
				\item $V = \left( V^\perp \right)^\perp$;
				\item $\dim E = \dim V + \dim V^{\perp}$.
			\end{itemize}
	\end{itemize}
\end{myprop}


\begin{mydef}
  Soient $E$ un espace euclidien et $x_1, x_2,... ,x_n \in E$,
  \begin{enumerate}
  \item La famille $x_1, x_2,... ,x_n$ est ume famille orthogonale si
    \begin{itemize}
    \item $x_i \neq 0, \forall i = 1, 2, \ldots, n$;
    \item $x_i \perp x_j, \forall i \neq j$.
    \end{itemize}

  \item La famille $x_1, x_2,... ,x_n$ est une famille orthonormée si
    \begin{itemize}
    \item $||x_i|| = 1, \forall i = 1, 2, \ldots, n$;
    \item $x_i \perp x_j, \forall i \neq j$.
    \end{itemize}
  \end{enumerate}
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien, et $u_1, \ldots, u_n$ une base orthonormée de $V$
	\footnote{Cette propriété est utilisée dans le cours LFSAB1202 pour dire que ${\bf u}.{\bf v} = uv$
	où $u$ et $v$ sont leurs coordonnées respectives dans la base orthonormée $[{\bf\hat{I}}]$.}.
	$\forall \alpha_i, \beta_i \in \mathbb{R},$
	$$(\alpha_1u_1 + \ldots + \alpha_nu_n | \beta_1u_1 + \ldots + \beta_nu_n) = \alpha_1\beta_1 + \ldots + \alpha_n\beta_n$$
\end{myprop}

\begin{myprop}\InsertTheoremBreak
	\begin{itemize}
		\item Une famille orthonormée est une famille orthogonale;
		\item Une famille orthogonale est une famille libre.
	\end{itemize}
\end{myprop}

\begin{myprop}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ de dimension finie tel que $V \neq \{0\}$. $V$ admet une base orthonormée.
\end{myprop}

\subsection{Projection orthogonale}
\begin{mydef}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ et $x \in E$.
	La projection orthogonale de $x$ sur $V$ est un vecteur $P_V(x)$ tel que
	\[
	\left\{
	\begin{array}{l}
		P_V(x) \in V\\
		x - P_V(x) \in V^{\perp}
	\end{array}
	\right.
	\]
\end{mydef}

\begin{myprop}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ et $u_1, \ldots, u_n$ une base orthonormée de $V$
	\[ \forall x \in E, \exists! P_V(x)\text{ et }P_V(x) = (x|u_1)u_1 + \ldots + (x|u_n)u_n \]
	\subparagraph{Commentaires}
	\begin{itemize}
		\item $P_V : E \to E$ est donc fonctionnelle.
		\item L'hypothèse (base: $u_1, u_2,..., u_n$) doit être une base de $V$, pas spécialement de $E$ qui est parfois de dimension trop importante.
	\end{itemize}
\end{myprop}

\begin{myprop}
	Soient $E$ un espace euclidien, $V$ un sev de $E$ et $x \in E$.
% La condition "sev" est elle inutile pour les autres myprop que l'existence de P_V(x) ?
	\begin{itemize}
		\item $y \neq P_V(x) \Rightarrow \dist(x, P_V(x)) < \dist(x, y)$;
		\item $P_V : E \to E$ est une application lineaire;
		\item $\Ker P_V = V^{\perp}$;
		\item $\newIm P_V = V = \{x \in E | P_V(x) = x\}$;
		\item $P_V \circ P_V \circ \cdots \circ P_V = P_V$.
	\end{itemize}
\end{myprop}

\subsection{Méthode de Gramm-Schmidt}

Supposons qu'on possède une base quelconque de $V$ : $( x_1, x_2, \dots , x_n)$. On peut construire une base orthonormée $(e_1, e_2, \dots, e_n)$ de la manière suivante
\[ e_1 = \frac{x_1}{||x_1||} \]
\[ e_2 = \frac{x_2 - e_1 \cdot (x_2|e_1)}{||x_2 - e_1 \cdot (x_2|e_1)||} \]
\[ e_3 = \frac{x_3 - e_1 \cdot (x_3|e_1) - e_2 \cdot (x_3|e_2)}{||x_3 - e_1 \cdot (x_3|e_1) - e_2 \cdot (x_3|e_2)||} \]
\[ \dots \]
\[ e_n = \frac{ x_n - \sum_{i=1}^{n-1} e_i \cdot (x_n|e_i) }{ || x_n - \sum_{i=1}^{n-1} e_i \cdot (x_n|e_i) || } \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Diagonalisation de matrices et d'opérateurs linéaires}

\subsection{Définitions}

\begin{mydef}[Matrice diagonalisable]
Une matrice $A \in K^{n \times n}$ est diagonalisable si $\exists P, D \in K^{n \times n}$, avec $P$ inversible et $D$ diagonale, telles que $A = P \cdot D \cdot P^{-1}$.
\end{mydef}

\begin{mydef}[Valeur propre, vecteur propre et espace propre] Soit $L : V \rightarrow V$ un opérateur linéaire
\begin{itemize}
\item $\lambda \in K$ est \emph{valeur propre} si $\exists x \in V, x \neq 0$ tel que $L(x) = \lambda \cdot x$ ;
\item un tel $x$ est dit \emph{vecteur propre} de $L$ associé à $\lambda$ ;
\item si $\lambda \in K$, l'\emph{espace propre} associé à $\lambda$ est l'ensemble
\[ E(\lambda) = \{ x \in V | L(x) = \lambda \cdot x \} \]
\end{itemize}
\end{mydef}

\begin{myrem}
\InsertTheoremBreak
\begin{itemize}
\item $E(\lambda) = \newker (L - \lambda \cdot I) \Rightarrow E(\lambda)$ est un sous-espace vectoriel de V ;
\item $\lambda$ est valeur propre de L $\Leftrightarrow \newdim E(\lambda) > 0$.
\end{itemize}
\end{myrem}

\begin{mydef}[Opérateur diagonalisable]
Un opérateur linéaire $L$ est \emph{diagonalisable} si $V$ admet une base formée par les vecteurs propres de $L$. Dans ce cas, on a la relation
\[ _f(L)_f = _f\!(I)_e \cdot _e\!(L)_e \cdot _e\!(I)_f = P \cdot D \cdot P^{-1} \]
\end{mydef}

\subsection{Diagonalisation}

\begin{myprop} Soit $L : V \rightarrow V$ un opérateur linéaire, $e = (e_1, \dots, e_n)$ une base de $V$ : $L$ est diagonalisable $\Leftrightarrow _e\!(L)_e$ est diagonalisable.
\end{myprop}

\begin{myrem}
\InsertTheoremBreak
\begin{itemize}
\item Les valeurs propres et les vecteurs propres de $A \in K^{n \times n}$ sont aussi les valeurs propres et les vecteurs propres de $L_A : K^n \rightarrow K^n : L_A(x) = A \cdot x$ ;
\item Si $A = P\cdot D \cdot P^{-1}$, alors \[ D = \begin{pmatrix} \lambda_1 &  &  &  \\
																																   & \lambda_2 &  & \\
																																  & & \ddots & \\
																																  & & & \lambda_n \\
																																  \end{pmatrix} \]
est la matrice diagonale des valeurs propres de $A$. $P$ est une matrice inversible dont les colonnes sont une base de $K^n$ formée par les vecteurs propres de $A$.
\item $\lambda_i \in K$ est valeur propre de $A$ si et seulement si $\lambda_i$ est racine du polynôme caractéristique
\[ \newdet (A - \lambda \cdot I) \in K[\lambda]_n \]
\end{itemize}
\end{myrem}

\begin{myprop}[Multiplicité(s)]
Si $\lambda$ est l'une des valeurs propres d'un opérateur linéaire $L$, alors
\[ 1 \leq \newdim E(\lambda) = m_g (\lambda) \leq m_a (\lambda) \]
\end{myprop}

\begin{myprop}[Conditions nécessaires et suffisantes pour qu'une matrice soit diagonalisable]
Soit $\lambda_1 , \dots , \lambda_r$ l'ensemble des valeurs propres distinctes de $L : V \rightarrow V$ ($\newdim V = n$). Pour que $L$ soit diagonalisable, il faut et il suffit que (les trois sont équivalents)
\begin{itemize}
\item $V = E(\lambda_1) + \cdots + E(\lambda_r)$ ;
\item $ \newdim E(\lambda_1) + \cdots + \newdim E(\lambda_r) = n$ ;
\item $m_a (\lambda_1) + \cdots + m_a (\lambda_r) = n$ et $\newdim E(\lambda_i) = m_a (\lambda_i) \forall i = 1, \dots, r$.
\end{itemize}
\end{myprop}

\begin{myrem}
Notons que
 \begin{align*} \newdim E(\lambda) & = \newdim \newker (L - \lambda \cdot I) \\ & = n - \newrang (A - \lambda \cdot I) \end{align*}
avec $A = _e\!(L)_e$.
\end{myrem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Formes quadratiques}

\subsection{Définitions}

\begin{mydef}[Forme quadratique]
Soit $V$ un espace vectoriel \emph{réel}. Une forme quadratique est une fonction
\[ q : V \rightarrow \R \]
satisfaisant à deux conditions :
\begin{itemize}
\item $\forall \lambda \in \R, \forall x \in V : q(\lambda x) = \lambda^2 q(x)$ ;
\item $\bar{q} : V \times V \rightarrow \R$ est bilinéaire.
\end{itemize}
avec $\bar{q}(x,y) = \frac 12 (q(x+y) - q(x) - q(y) )$.
\end{mydef}

\begin{myrem} $\bar{q}$ est symétrique : $\bar{q}(x,y) = \bar{q}(y,x)$. \end{myrem}

\subsection{Lien avec les matrices symétriques}

\begin{myrem} $A$ est symétrique si et seulement si $A^t = A$ \end{myrem}

\begin{myprop} Soit $V$ un espace vectoriel réel, $e = (e_1, \dots , e_n)$ une base de $V$. Il y a une bijection entre
\begin{itemize}
\item Les formes quadratiques sur $V$ et
\item les matrices symétriques $n \times n$ à coefficients réels.
\end{itemize}
De fait, par définition :
\[ a_{i,j} = \bar{q}(e_i, e_j) \]
\[ q_{A,e} : V \rightarrow \R : q(x)_{A,e} = _e\!x^t \cdot A \cdot _e\!x \]
\end{myprop}

\begin{mycorr} De là, on tire que $A_{q,e}$ est l'\emph{unique} matrice symétrique ($A \in \R^{n \times n}$) telle que
\[ q(x) = _e\!x^t \cdot A_{q,e} \cdot _e\!x \] \end{mycorr}

\begin{mycorr} Soit $V$ un espace vectoriel réel, $q$ une forme quadratique, $e = (e_1, \dots , e_n)$ et $f = (f_1, \dots, f_n)$ deux bases de $V$, alors
\[ A_{q,f} = (_eI_f)^t \cdot A_{q,e} \cdot _e\!I_f \]\end{mycorr}

\subsection{Caractère}

\begin{mydef} Soit $V$ un espace vectoriel réel, $q : V \rightarrow \R$ forme quadratique. La forme quadratique est
\begin{itemize}
\item définie positive si $q(x) >0 \; \forall x \in V, x \neq 0$ ;
\item semi-définie positive si $q(x) \geq 0 \; \forall x \in V$ ;
\item définie négative si $q(x) < 0 \; \forall x \in V, x \neq 0$ ;
\item semi-définie négative si $q(x) \leq 0 \; \forall x \in V$ ;
\item indéfinie si $\exists x \in V$ tel que $q(x) > 0$ et $\exists y \in V$ tel que $q(x) < 0$.
\end{itemize}
\end{mydef}

\begin{myprop}
\InsertTheoremBreak
\begin{itemize}
\item Le caractère d'une matrice est par définition celui de la forme quadratique associée ;
\item le caractère d'une matrice ne dépend pas de la base de $\R^n$.
\end{itemize}\end{myprop}

\subsection{Théorème spectral}

\begin{mydef}[Matrice orthogonale]
Une matrice $Q \in \R^{n \times n}$ est orthogonale si ses colonnes forment une base \emph{orthonormée} de $\R^n$ par rapport au produit scalaire canonique, ou encore si
\[ Q \cdot Q^t = I_n \]
\end{mydef}

\begin{myrem}
Si $Q$ est orthogonale, alors $Q$ est inversible, et l'inverse est égale à la transposée :
\[ Q^t = Q^{-1} \]
\end{myrem}

\begin{mytheo}[Théorème spectral]
Sont équivalents (avec $A \in \R^{n \times n}$, matrice \emph{réelle}) :
\begin{itemize}
\item $A$ est symétrique ;
\item $\R^n$ admet une base orthonormée de vecteurs propres de $A$ ;
\item $\exists Q$ orthogonale, $D$ diagonale, telles que
\[ A = Q \cdot D \cdot Q^{-1} = Q \cdot D \cdot Q^t \]
\end{itemize}
\end{mytheo}

\subsection{Caractère et valeurs propres}

\begin{myprop}[Caractère et valeurs propres]
Pour déterminer le caractère d'une forme quadratique ou d'une matrice, il suffit d'observer les valeurs propres de cette dernière :
\begin{itemize}
\item définie positive si et seulement si $\lambda_1, \dots, \lambda_n > 0$;
\item semi-définie positive si et seulement si $\lambda_1, \dots, \lambda_n \geq 0$ ;
\item définie négative si et seulement si $\lambda_1, \dots, \lambda_n < 0$ ;
\item semi-définie négative si et seulement si $\lambda_1, \dots, \lambda_n \leq 0$ ;
\item indéfinie si $\exists  \lambda_i > 0$ et $\exists \lambda_j < 0$ ($i \neq j$).
\end{itemize}
\end{myprop}



% +--------------------------------------------------------------------+
% | d Équations différentielles                                        |
% | --------------------------- + Équations différentielles = e^{-x^2} |
% |            dx                                                      |
% +--------------------------------------------------------------------+
\part{Équations différentielles}

\section{Définition}

\begin{mydef}[Problème de Cauchy]
Soit un problème de Cauchy, défini de la manière suivante :

\[ P = \left\{ \begin{array}{l}
y^{(n)}(t) + a_{n-1}y^{(n-1)}(t) + \cdots + a_1 y'(t) + a_0 y(t) = f(t) \\
y(t_0) = \alpha_1, y'(t_0) = \alpha_2, \dots , y^{(n-1)}(t_0) = \alpha_n \\ \end{array} \right. \]
avec $\alpha_1, \dots, \alpha_n \in K, t_0 \in \R$ fixés.

\end{mydef}

\begin{mytheo}[Existence et unicité]
Un tel problème de Cauchy admet une et une seule solution.
\end{mytheo}

\section{Solution}

Soit un problème de Cauchy $P$. On étudie plusieurs choses séparément.

\paragraph{Équation différentielle homogène associée}
La solution générale de cette équation est de type
\[ u(t) = \sum_{j=1}^{q} P_j(t) \cdot e^{r_j t} \]
avec :
\begin{itemize}
\item $P_q \in \C[X]_{m_j -1}$ ;
\item $r_1, \dots, r_q$ les racines du polynôme caractéristique ;
\item $m_j = m_a(r_j)$.
\end{itemize}
En d'autres mots :
\begin{itemize}
\item $P$ est un polynôme dont le degré est égal à la multiplicité algébrique de la racine correspondante moins un ;
\item On multiplie $P$ par une exponentielle dont l'argument est une des racines du polynôme caractéristique
\end{itemize}
Les coefficients sont déterminés par les conditions initiales.

\paragraph{Équation différentielle non-homogène}
Dans le problème de Cauchy, $f(t) = P(t) \cdot e^{rt}$ avec $P \in \C[X]_{\leq p}$.

La solution particulière de cette équation est de type
\[ v(t) = t^m \cdot R(t) \cdot e^{rt} \]
avec
\begin{itemize}
\item $m$ la multiplicité de $r$ comme racine du polynôme caractéristique. Si on ne retrouve pas $r$ comme racine de ce polynôme, $m = 0$ ;
\item $R \in \C[X]_{\leq p}$ ;
\item $r$ est le même que dans l'équation différentielle de départ
\end{itemize}
Pour déterminer les constantes, on remplace dans l'équation différentielle non-homogène les dérivées successives de $y$ par celles de $v$, ceci permettant de déterminer les inconnues.

\paragraph{Principe de superposition}
Si
\[ f(t) = \sum_{i=1}^s Q_i(t) e^{r_it} \]
alors on a la solution $v$ tel que
\[ v(t) = \sum_{i=1}^s t^{m_i} \cdot R_i(t) \cdot e^{r_it} \]
tel que
\begin{itemize}
\item $m_i = m_a(r_i)$ ;
\item $R_i \in \C[X]_ { d^{\circ} Q_i }$.
\end{itemize}

\paragraph{Solution générale}
La solution générale d'une équation différentielle est simplement la somme des solutions générales de l'équation homogène associée et de la solution particulière de l'équation homogène. Donc l'espace $S$ des solutions est
\[ S = \{y = v + u \} \]
avec $u$ et $v$ comme défini ci-dessus.

\begin{myrem}[Équation en sinus et cosinus]
Un point important : si le terme indépendant est une fonction $\cos t$ ou $\sin t$, alors pour trouver la solution particulière, il suffit de prendre respectivement la partie réelle ou imaginaire de $e^{it}$.
\end{myrem}

\section{Marche à suivre}

\begin{enumerate}

\item Déterminer l'ensemble des solutions de l'équation homogène associée. On obtient une solution générale avec des constantes inconnues qu'on déterminera à la fin si il y a lieu.
\item Pour chaque terme de l'équation non-homogène associée :
	\begin{itemize}
	\item si le terme est de type ($\text{polynôme} \cdot \text{exponentielle}$) on travaille de manière classique ;
	\item si le terme est de type sinus ou cosinus (de type $\sin (\omega t)$) on résout d'abord en le remplaçant par $e^{\omega t \i}$ ;
	\item ensuite on remplace dans l'équation non-homogène $y$ et ses dérivées par la solution particulière trouvée, et ses dérivées, pour déterminer les constantes inconnues. Attention, ces-dernières ne peuvent \emph{pas} dépendre de $t$ .
	\item si on avait à faire avec un cosinus ou sinus, on prend respectivement la partie réelle (si c'est un cosinus) ou imaginaire (si c'est un sinus) de la solution;
	\end{itemize}
\item On somme ensuite l'ensemble des ces solutions (l'homogène + la ou les particulière)
\item Si on cherche une solution limitée aux réels, on pose que les constantes de l'équation homogène sont complexes ($A = a_1 + \i a_2$, etc), on remplace, puis on annule les termes complexes.
\item Si on a des conditions initiales ($y(0) = y_0$, $y'(0) = y'_0$, etc.) on introduit ces dernières dans la solution (et ses dérivées) pour déterminer les constantes de la partie homogène
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Calcul différentiel à plusieurs variables}

\section{Définition}


\begin{mydef}[Fonction à plusieurs variables]
Une fonction
\[ f : A \subseteq \R^n \rightarrow \R \]
est définie par l'ensemble des paires $(a,f(a))$ pour chaque $a \in A$. Ces paires forment un sous-ensemble de $\R^n \times \R$ appelé graphe de $f$. Ces fonctions sont des fonctions scalaires.

Une fonction
\[ f : A \subseteq \R^n \rightarrow \R^m \]
est définie par l'ensemble des paires $(a,f(a))$ pour chaque $a \in A$. Ces paires forment un sous-ensemble de $\R^n \times \R^m$ appelé graphe de $f$. Ces fonctions sont des fonctions vectorielles. On peut toujours décomposer ces fonctions en plusieurs fonctions scalaires. En d'autres mots, on peut toujours écrire
\[ f = (f_1, f_2, \dots, f_n) \]
où chaque $f_i$ est scalaire.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Concepts de base}

\paragraph{Note} Dans cette section, certains concepts n'ont pas été repris

\subsection{Ensembles élémentaires}
Soit
\[ f : A \subseteq \R^n \rightarrow \R^m \]
Une fonctions est caractérisée par trois ensembles principaux :
\begin{mydef} [Graphe de $f$] Le graphe de $f$ est défini comme
\[ \mathrm{graph} f = \{(a,f(a))| a \in A\} \subset \R^n \times \R^m \]
\end{mydef}

\begin{mydef} [Image de $f$] L'image de $f$ est l'ensemble
\[ \mathrm{Im} f = \{ f(a) | a \in a\} \subset \R^m \]
\end{mydef}

\begin{mydef} [Ensemble de niveau k] L'ensemble de niveau $k$ de $f$ est
\[ \text{ens. de niveau k} = \{ a | a \in A \;\text{et}\; f(a) = k \} \subset \R^n \]
\end{mydef}

\subsection{Points et ensembles particuliers}

\begin{mydef}[Point intérieur] $a$ est un point intérieur à $A \Leftrightarrow \exists r > 0 \in \R$ tel que
\[ ||x - a|| < r \Rightarrow x \in A \]
Cet ensemble se note $\mathrm{int} A$.
\end{mydef}

\begin{mydef} [Boules] Les voisinages se définissent à partir des notions de boules ouvertes et fermées. Une boule ouverte est un ensemble
\[ B(a;r) = \{ x \in \R^n | ||x-a|| < r \} \]
Et une boule fermée
\[ B[a;r] = \{ x \in \R^n | ||x-a|| \leq r \} \]
\end{mydef}

\begin{mydef} [Intérieur] $a$ est un point intérieur à $A$ si et seulement si il existe une boule ouverte centrée en $a$ entièrement incluse dans $A$. On le note $\mathrm{int} A$. \end{mydef}

\begin{mydef} [Point limite] $a$ est un point limite de $A$ si et seulement si toute boule ouverte centrée en $a$ intersecte $A$ autre part qu'en $a$. \end{mydef}

\begin{mydef} [Point isolé] $a$ est un point isolé de $A$ si il existe une boule ouverte centrée en $a$ qui n'intersecte $A$ qu'en $a$. \end{mydef}

\begin{mydef} [Fermeture] C'est l'ensemble des points limites et isolés. On la note $\bar{A}$. \end{mydef}

\begin{myform}
\[ \mathrm{int} A \subseteq A \subseteq \bar{A} \]
\end{myform}

\begin{mydef} [Frontière] La frontière de $A$ est égale à
\[ \text{frontière de A} = \bar{A} \backslash \mathrm{int} A \]
\end{mydef}

\subsubsection{Résumé sous forme de schémas}

La figure \ref{fig:ensembles} présente un résumé des différents types de points et d'ensembles. Un trait plein signifie un bord fermé, des pointillés un bord ouvert, des hachures un ensemble <<~remplis~>>, et un point un point isolé.

\begin{figure}[!ht]
	\begin{center}
		\subfigure[$A$]{\includegraphics[height=1.8cm]{images/figuresasy-1}}\qquad
		\subfigure[int($A$) --- l'intérieur de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-2}}\qquad
		\subfigure[$\bar{A}$ --- la fermeture de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-3}}\\
		\subfigure[fr($A$) --- la frontière de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-4}}\qquad
		\subfigure[Les points limites ou d'accumulation de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-5}}\qquad
		\subfigure[Les points isolés de $A$]{\includegraphics[height=1.8cm]{images/figuresasy-6}}\\
		\caption{Les divers ensembles}
		\label{fig:ensembles}
	\end{center}
\end{figure}

\subsection{Limites}

Dans cette partie, on considère $f$, une fonction telle que
\[ f : A \subseteq \R^n \rightarrow \R \]

\begin{mydef}[Limite d'une fonction à plusieurs variables]
On défini la limite de $f$ uniquement en tout point de la \emph{fermeture} de son domaine.
Si $a$ est un point isolé de $A$, alors
\[ \lim_{x \fl a} f(x) = f(a) \]
Si $a$ est un point limite de $A$, alors
\[ \lim_{x \fl a} f(x) = L \]
si et seulement si $\forall \epsilon >0, \exists \delta > 0$ tel que $x \in A$ et
\[ 0 < || x - a || < \delta \Rightarrow |f(x) - L| < \epsilon \]
Si la fonction est vectorielle et que $a$ est un point limite de $A$, alors
\[ \lim_{x \fl a} f(x) = L \]
si et seulement si $\forall \epsilon >0, \exists \delta > 0$ tel que $x \in A$ et
\[ 0 < || x - a || < \delta \Rightarrow ||f(x) - L|| < \epsilon \]
Si on écrit $f$ sous la forme $f = (f_1 , \dots , f_n)$ où chaque $f_i$ est une fonction scalaire, alors la limite de $f$ existe si et seulement si la limite de chaque $f_i$ existe
\[ \lim_{x \fl a} f(x) = \left( \lim_{x \fl a} f_1(x), \lim_{x \fl a} f_2(x), \dots , \lim_{x \fl a} f_n(x) \right) \]
\end{mydef}

\begin{myprop}[Condition nécessaire\footnote{Mais pas suffisante !} à l'existence d'une limite] Soit $f_B$ une restriction de la fonction $f$ à un sous-ensemble $B \subseteq A$. Si $f$ admet une limite en $a$, alors
\[ \lim_{x \fl a}f_B(x) \]
existe et
\[ \lim_{x \fl a}f(x) = \lim_{x \fl a}f_B(x) \]
\end{myprop}

\begin{myprop}[Recollement (condition suffisante)] Soit $A = A_1 \cup A_2$ ; soit $f_1$, $f_2$ les restrictions de $f$ à $A_1$ et $A_2$. Si $a \in \bar{A_1}$ et $a \in \bar{A_2}$, alors
\[ \lim_{x \fl a}f(x) = L \Leftrightarrow \left\{ \begin{array}{l} \lim_{x \fl a} f_1(x) = L \\ \lim_{x \fl a}f_2(x) = L \end{array} \right. \]
\end{myprop}

\begin{myform}[Combinaisons algébriques] Les quatre opérations classiques ($+$, $-$, $\cdot$, $\div$) sont possibles, mais \emph{pas} la composition de limites. \end{myform}

\begin{myprop}[Positivité] Si
\begin{itemize}
\item $f(x) \geq 0 \Rightarrow \lim_{x \fl a} f(x) \geq 0$ ;
\item $f(x) > 0 \Rightarrow \lim_{x \fl a} f(x) \geq 0$ ;
\item $f(x) \leq g(x) \Rightarrow \lim_{x \fl a} f(x) \leq \lim_{x \fl a} g(x)$.
\end{itemize}
NB : ces propriétés sont aussi valables dans un voisinage de $a$.
\end{myprop}

\begin{myprop}[Étau (condition suffisante)]
Si $g \leq f \leq h$ et que $\lim_{x \fl a} g(x) = \lim_{x \fl a} h(x) = L$ alors
\[ \lim_{x \fl a} f(x) = L \]
\end{myprop}

\begin{myform}[Application de l'étau]\InsertTheoremBreak
\begin{itemize}
\item si $f \rightarrow 0$, alors $|f| \rightarrow 0$ ;
\item si $f \fl 0$, et que $g$ est bornée, alors $\lim_{x \fl a} (g \cdot f) = 0$.
\end{itemize}
\end{myform}

\begin{myprop}[Continuité] Une fonction $f : A \subseteq \R^n \rightarrow \R^m$ est continue en $a \in A$ si et seulement si
\[ \lim_{x \rightarrow a}f(x) = f(a) \]
\end{myprop}

\begin{myrem} Une fonction est toujours continue en un point isolé de $A$. \end{myrem}
\begin{myrem} Les quatres opérations classiques \emph{ainsi que la composition} préservent la continuité. \end{myrem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dérivées directionnelles et partielles}

\begin{mydef}[Dérivée directionnelle] Soit $f : A \subseteq \R^n \fl \R^m$, $a$ un point intérieur de $A$, $d \in \R^n$ une direction. La dérivée directionnelle de $f$ en $a$ dans la direction $d$ est notée $D_df(x)$ et vaut, si elle existe, la limite suivante
\[ D_df(a) = \lim_{t \fl 0} \frac{f(a+td) - f(a)}{t} \]
\end{mydef}

\begin{myform}[Approximation linéaire dans une direction]
\[ f(a+td) \approx f(a) + t D_df(a) \]
\end{myform}

\begin{mydef}[Dérivée partielle]
C'est la dérivée directionnelle le long d'un axe.
Soit une fonction $f : A\subseteq \R^n \fl \R^m$ et $a$ un point intérieur de $A$. La dérivée partielle de $f$ par raport à une de ses variables $x_i$ est la dérivée directionelle de $f$ au point $a$ dans la direction $e^{(i)}$ et se note
\[ \frac{\pa f}{\pa x_i}(a) = D_{e^{(i)}} f(a) \]
\end{mydef}

\begin{myrem}[Continuité et dérivées directionnelles]
L'existence de toute les dérivées directionnelles (et donc à fortiori partielle) en un point n'implique pas la continuité.
\end{myrem}

\begin{myform}[Calcul des dérivées partielles] Pour calculer les dérivées partielles de $f$ selon une de ses variables $x_i$, on fixe toutes les autres variables et on calcule la dérivée partielle comme la dérivée d'une fonction à une seule variable ($x_i$).
\end{myform}

\begin{mydef}[Dérivées partielles d'ordre supérieur]
L'existence des $i$\ieme{} dérivées partielles défini une fonction
\[ \frac{\pa^i f}{\pa x^i} : \mathrm{int} A \subseteq \R^n \fl \R : x \fl \frac{\pa^i f}{\pa x^i}(x) \]
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Différentielle}

\subsection{Définitions}

\begin{mydef}[Différentielle] Soit une fonction $f : A \subseteq \R^n \fl \R^m$ et $a \in \R^n$ un point intérieur de $A$. La fonction $f$ est différentiable en $a$ si et seulement si il existe une application linéaire $L : \R^n \fl \R^m$ telle que
\[ \lim_{h \fl 0} \frac{f(a+h) - f(a) - L(h)}{||h||} = 0 \]

Si cette application linéaire $L$ existe, alors on l'appelle différentielle de $f$ en $a$ on elle se note
\[ L = \dif f_a = \dif f(a) \]
\end{mydef}

\begin{myprop} Soit $f : A \subseteq \R^n \fl \R^m$ et $a$ un point intérieur à $A$. $f$ est différentiable si et seulement si il existe une application linéaire $L : \R^n \fl \R^m$ et une fonction $r : \R^n \fl \R^m$ telles que
\[ f(a+h) = f(a) + L(h) + ||h|| r(h) \]
et
\[ \lim_{h \fl 0}r(h) = 0 \]
\end{myprop}

\begin{myprop}[Continuité] La différentiabilité garanti la continuité. \end{myprop}

\begin{myform}[Lien avec dérivées partielles] On a les formules
\[ \dif f_a : (\dif x_1 , \dots , \dif x_n) \fl \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a) \dif x_i \]
\[ \frac{\pa f}{\pa x_i}(a) = \dif f_a(e^{(i)}) \]
\[ Df_d(a) = \dif f_a(d) \]
\end{myform}

\begin{myprop}[Condition suffisante pour la différentiabilité] Une fonction $f : A \subseteq \R^n \fl \R^m$ et $a$ un point intérieur à $A$. $f$ est différentiable si
\begin{itemize}
\item toutes les dérivées partielles de $f$ \emph{existent} en $a$ ;
\item toutes les dérivées partielles \emph{sauf une} sont continues dans un voisinage de $a$ (dans \emph{une} boule ouverte centrée en $a$)
\end{itemize}
\end{myprop}

\subsection{Formules de calcul}


\noindent
\begin{tabularx}{1.0\textwidth}{|S X|S X|}
\hline
Fonction & Différentielle (au point $a$, évaluée en $h$)\\
\hline
$ \displaystyle M \cdot x \;\text{avec}\; M \in \R^{m \times n} $ & $\displaystyle M \cdot h $\\
\hline
$ \displaystyle M \cdot x + p $ & $\displaystyle  M \cdot h $ \\
\hline
$ \lambda \cdot f(x) \;\text{avec}\; \lambda \in \R $ & $\displaystyle \lambda \cdot \dif f_a (h) $ \\
\hline
$ \displaystyle M \cdot f(x) \;\text{avec}\; M \in \R^{p \times m}$ & $\displaystyle M \cdot \dif f_a (h)$ \\
\hline
$ \displaystyle f(x) + g(x) $ & $ \displaystyle \dif f_a (h) + \dif g_a (h) $ \\
\hline
$ \displaystyle f(x)\cdot g(x) $ & $\displaystyle  g(a) \cdot \dif f_a(h) + f(a) \cdot \dif g_a(h) $ \\
\hline
$ \displaystyle \frac{f(x)}{g(x)} $ & $\displaystyle  \frac{g(a) \cdot \dif f_a(h) - f(a) \cdot \dif g_a(h) }{g(a)^2} $ \\
\hline
$ \displaystyle (f(x)|g(x)) $ & $\displaystyle  (\dif f_a(h) | g(a) ) + (f(a)|\dif g_a(h)) $ \\
\hline
$ \displaystyle (f \circ g)(x) $ & $\displaystyle  \dif f_{g(a)} \circ \dif g_a(h) $ \\
\hline
\end{tabularx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Plans et vecteurs tangents, gradient}

\subsection{Courbes et surfaces paramétrées}

\begin{mydef}[Courbe paramétrée]
Soit une courbe paramétrée dans $\R^m$ décrite par $f : A \subseteq \R \fl \R^m$
\[ \mathcal{C} = \{ f(t) | t \in A \subseteq \R \} \subseteq \R^m \]
\end{mydef}

\begin{myform}[Droite tangente]
Si $f$ est différentiable, alors la droite tangente à la courbe $\mathcal{C}$ en $f(a)$ a pour équation paramétrique
\[ t \fl f(a) + t f'(a) \]
Où $f'(a)$ est le vecteur tangent à $\mathcal{C}$ en $a$
Dans ce cas,
\[ f(a + t) \approx f(a) + \dif f_a(t) = f(a) + t f'(a) \]
\end{myform}

\begin{mydef}[Surface paramétrée]
Une surface paramétrée de $\R^m$ décrite par $f : A \subseteq \R^2 \fl \R^m$ est
\[ \mathcal{S} = \{ f(u,v) | (u,v) \in A \subseteq \R^2 \} \subseteq \R^m \]
\end{mydef}

\begin{myform}[Plan tangent]
Si $f$ est différentiable en $a$, le plan tangent à $f$ au point $f(a,b)$ est paramétrée par
\[ (u,v) \fl f(a,b) + u \frac{\pa f}{\pa x_1} (a,b) + v \frac{\pa f}{\pa x_2}(a,b) \]
\end{myform}

\subsection{Gradient}

\begin{mydef}[Gradient] Soit $f : A \subseteq \R^n \fl \R$ une fonction scalaire différentiable en $a$, le gradient est un vecteur ligne à $n$ composantes
\[ (\mathrm{grad} f)(a) = \nabla f(a) = \left( \frac{\pa f}{\pa x_1}(a) , \dots, \frac{\pa f}{\pa x_n}(a) \right) \]
\end{mydef}

\begin{myform} On a que
\[ D_df(a) = \dif f_a(d) = \nabla f(a) d \]
\end{myform}

\begin{myprop} Le gradient est parallèle à la direction de plus forte pente,
	celle où $D_df(a)$ est maximal
	\footnote{\c{C}a peut se prouver aisément en majorant $D_df(a)$ par l'inégalité de Cauchy et en disant qu'on doit être dans le cas d'égalité pour être maximum.}.
	Il est aussi orthogonal aux courbes de niveau de $f$ en $a$. \end{myprop}

\subsection{Courbes et surfaces définies par un graphe}

\begin{myform}[Plan tangent à un graphe]
Soit une fonction scalaire $f : A \subseteq \R^n \fl \R$ différentiable en $a$ un point intérieur de $A$. Le plan tangent au point $(a,f(a))$ est
\begin{align*} z & = f(a) + \frac{\pa f}{\pa x_1}(a)(x_1 - a_1) + \cdots + \frac{\pa f}{\pa x_n}(a) (x_n - a_n) \\
                   & = f(a) + \dif f_a(x-a) \\
                   & = f(a) + \nabla f(a)(x-a) \end{align*}

La normale de ce plan tangent est
\[ \begin{pmatrix} \nabla f(a) & -1 \end{pmatrix} \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Théorème des accroissement finis}

\begin{mytheo}[Théorème des accroissements finis] Soit une fonction scalaire $f : A \subseteq \R^n \fl \R$ continue sur l'intervalle fermé $[a,b]$ et différentiable sur l'intervalle $]a,b[$, $\exists c \in ]a,b[$ tel que
\[ \dif f_c(b-a) = f(b) - f(a) \]
\end{mytheo}

\begin{mytheo}[Théorème des accroissements finis (formulation équivalente)] Soit $f$ une fonction scalaire. $\exists 0 < \theta < 1$ tel que
\[ f(a+h) = f(a) + \dif f_{a + \theta h}(h) = f(a) + \nabla f(a + \theta h)h \]
\end{mytheo}

\begin{myprop}[Pour les fonctions vectorielles] On a seulement l'inégalité
\[ || f(b) - f(a) || \leq || \dif f_c(b-a) || \]
\end{myprop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classe de fonction}

\begin{mydef}[Classe de fonctions $\mathcal{C}^k$]
Une fonction $f$ est dite de classe $\mathcal{C}^k$ sur un ensemble ouvert $A$ si et seulement si toutes ses dérivées partielles $k$\ieme{} existent et sont continues sur $A$. On écrit alors
\[ f \in \mathcal{C}^k(A) \]
\end{mydef}

\begin{myform}[Classe de fonction] On a les inclusions suivantes
 \[ \mathcal{C}^{\infty}(A) \subset \dots \subset \mathcal{C}^3(A) \subset \mathcal{C}^2(A) \subset \mathcal{C}^1(A) \subset \mathcal{C}^0(A) \]
\end{myform}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matrices hessiennes et jacobiennes}

\begin{mydef}[Matrice hessienne]
Soit $f : A \subseteq \R^n \fl \R$ et $a$ un point intérieur de $A$. Si toutes les dérivées partielles secondes existent en $a$, la matrice hessienne est la matrice carrée de dimension $n$ suivante
\[ Hf(a) = \nabla^2 f(a) = \begin{pmatrix}
\frac{\pa^2 f}{\pa x_1^2}(a) & \frac{\pa^2 f}{\pa x_2 x_1}(a) & \cdots & \frac{\pa^2 f}{\pa x_n \pa x_1}(a) \\
\frac{\pa^2 f}{\pa x_1 \pa x_2}(a) & \frac{\pa^2 f}{\pa x_2^2}(a) & \cdots & \frac{\pa^2 f}{\pa x_n \pa x_2}(a) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\pa^2 f}{\pa x_1 \pa x_n}(a) & \frac{\pa^2 f}{\pa x_2 \pa x_n}(a) & \cdots & \frac{\pa^2 f}{\pa x_n^2}(a) \\
\end{pmatrix} \]
\end{mydef}

\begin{myprop}[Matrice symétrique]
Soit $f : A \subseteq \R^n \fl \R$ de classe $\mathcal{C}^2(A)$. Alors
\[ \frac{\pa^2 f}{\pa x_j \pa x_i}(a) = \frac{\pa^2 f}{\pa x_i \pa x_j}(a) \]
pour tout $a \in A$ et pour tous $1 \leq i,j \leq n$. Dans ce cas, la matrice hesienne est symétrique.
\end{myprop}

\begin{myprop}[Ordres supérieurs]
Si une fonction est de classe $\mathcal{C}^k$ alors on peut permuter les dérivées partielles d'ordre $k$.
\end{myprop}

\begin{mydef}[Matrice jacobienne]
Soit une fonction $f : A \subseteq \R^n \fl \R^m$ différentiable en un point $a$ intérieur à $A$. La matrice jacobienne est une matrice $m \times n$ :
\[ Jf(a) = \begin{pmatrix}
\frac{\pa f_1}{\pa x_1}(a) & \frac{\pa f_1}{\pa x_2}(a) & \cdots & \frac{\pa f_1}{\pa x_n}(a) \\
\frac{\pa f_2}{\pa x_1}(a) & \frac{\pa f_2}{\pa x_2}(a) & \cdots & \frac{\pa f_2}{\pa x_n}(a) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\pa f_m}{\pa x_1}(a) & \frac{\pa f_m}{\pa x_2}(a) & \cdots & \frac{\pa f_m}{\pa x_n}(a) \\
\end{pmatrix} \text{avec } f = \begin{pmatrix} f_1 \\ f_2 \\ \vdots \\ f_m \end{pmatrix} \]
\end{mydef}

\begin{myform}[Lien avec gradient, dérivées partielles, ...]
On a les formules suivantes :
\[ Jf(a) = \begin{pmatrix} \nabla f_1(a) \\ \nabla f_2(a) \\ \vdots \\ \nabla f_m(a) \end{pmatrix} \]
\[ Jf(a) = \begin{pmatrix} \frac{\pa f}{\pa x_1}(a) & \frac{\pa f}{\pa x_2}(a) & \cdots & \frac{\pa f}{\pa x_n}(a) \end{pmatrix} \]
Pour chaque composante scalaire d'une fonction vectorielle
\[ \dif f_a(h) = Jf(a) h \]
La matrice jacobienne est donc la matrice associée à l'application linéaire $\dif f_a$.
Pour une fonction à une seule variable représentant une courbe paramétrée de $\R^n$ :
\[ Jf(a) = f'(a) \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Développement de Taylor}

\begin{myform}[Développement avec reste d'ordre 1]
Soit $A \subseteq \R^n$ un ensemble ouvert, $f : A \subseteq \R^n \fl \R$ une fonction scalaire de classe $\mathcal{C}^1$, $a \in \R^n$ un point de $A$ et $h$ un vecteur de $\R^n$ tel que $[a, a+h] \subseteq A$. $\exists 0 < \theta < 1$ tel que
 \begin{align*} f(a+h) & = f(a) + \nabla f(a+\theta h)h \\
                        & = f(a) + \sum_{i =1}^n \frac{\pa f}{\pa x_i} (a+\theta h)h_i \end{align*}
\end{myform}

\begin{myform}[Développement avec reste d'ordre 2]
Soit $f$ une fonction de classe $\mathcal{C}^2$.
 \begin{align*} f(a+h) & = f(a) + \nabla f(a) h + \frac 12 h \nabla^2 f(a+\theta h) h \\
                        & = f(a) + \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a)h_i + \frac 12 \sum_{i=1}^n \sum_{j=1}^n \frac{\pa^2 f}{\pa x_i \pa x_j} (a+ \theta h) h_i h_j \end{align*}
\end{myform}

\begin{myform}[Développement avec reste d'ordre 3]
Soit $f$ une fonction de classe $\mathcal{C}^3$.
 \begin{align*} f(a+h) & = f(a) + \sum_{i=1}^n \frac{\pa f}{\pa x_i}(a)h_i + \frac 12 \sum_{i=1}^n \sum_{j=1}^n \frac{\pa^2 f}{\pa x_i \pa x_j} (a) h_i h_j \\ & + \frac{1}{3!} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \frac{\pa^3 f}{\pa x_i \pa x_j \pa x_k}(a+\theta h)h_i h_j h_k \end{align*}
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Composition (\emph{chain rule})}

\begin{myform}[\emph{Chain rule}]
Pour composer des applications linéaires, on a la formule
\[ J(g \circ f)(a) = Jg(f(a)) \cdot Jf(a) \]
Si on explicite les composantes $f_1, f_2, \dots , f_m$ de $f$ avec
\[ g(f(x)) = g( f_1(x_1, \dots, x_n), f_2(x_1, \dots, x_n), \dots, f_m(x_1, \dots, x_n)) \]
et $g$ définie de sorte qu'on ait $g : (y_1, y_2, \dots , y_m) \rightarrow g(y_1, y_2, \dots, y_m)$. Donc $x$ sont les variables naturelles de $f$ et $y$ celles de $g$.
Alors on a la formule\footnote{Notez la différence par rapport aux slides du cours, où les $y_1, y_2, \dots, y_n$ remplacent les $x_1, x_2, \dots, x_n$ pour na pas les confondre avec les variables de $f$}
\[ \frac{\pa (g \circ f)}{\pa x_i}(a) = \frac{ \pa g}{\pa y_1}(f(a)) \frac{\pa f_1}{\pa x_i}(a) + \frac{ \pa g}{\pa y_2}(f(a)) \frac{\pa f_2}{\pa x_i}(a) + \cdots + \frac{ \pa g}{\pa y_m}(f(a)) \frac{\pa f_m}{\pa x_i}(a) \]
\end{myform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimisation}

\subsection{Définitions}

On ne parlera ici que de fonction scalaire, donc de
\[ f : A \subseteq \R ^n \fl \R \]

\begin{mydef}[Extrémum global] Un extremum global est un point où $f$ prend une valeur extrême ; c'est à dire un point $a$ tel que
\[ f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)} \end{array} \]
\end{mydef}

\begin{mydef}[Extrémum local] Un extremum local est un point où $f$ prend une valeur extrême sur un voisinage; c'est à dire un point $a$ tel que
\[ \exists r >0 \; \text{tel que} \; f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in B(a;r) \cap A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)} \end{array} \]
\end{mydef}

\begin{mydef}[Extréma locaux libres]
Un extremum local libre de $f$ est un point $a \in \newint A$ tel que
\[ \exists r > 0 \; \text{tel que} \; f(a) \begin{array}{l} \geq \\ \leq \end{array} f(x) \; \forall x \in B(a;r)\cap A \begin{array}{l} \text{(maximum)} \\ \text{(minimum)} \end{array} \]
\end{mydef}

\begin{mydef}[Point de selle] $a \in \newint A$ est un point de selle si, $\forall r >0, \exists x_+ \in B(a;r) \; \text{et} \; x_- \in B(a;r)$ tels que
\[ f(x_-) < f(a) < f(x_+) \]
\end{mydef}

\begin{mydef}[Point critique] Un point critique est un point où le gradient, s'il existe, est nul. \end{mydef}

\begin{mydef}[Point singulier] Un point singulier est un point du domaine où le gradient n'existe pas. \end{mydef}

\begin{mydef}[Ensemble compact] Un  ensemble est compact s'il est fermé et borné, c'est à dire qu'il ne <<~tend pas vers l'infini~>> et qu'il contient tous ses points frontières. \end{mydef}

\subsection{Conditions pour trouver des extrema}

\begin{myprop}[Condition nécessaire]
Soit $f \in \mathcal{C}^2$, $a \in \newint A$. Pour que $a$ soit un extremum, il est nécessaire que
\[ \nabla f(a) = 0 \]
\end{myprop}

\begin{myprop}[Propriété de Fermat]
Soit $a \in \newint A$, extrémum local libre. Soit $f$ une direction de $\R^n$ ; si $D_df(a)$ existe, elle est nulle. Donc, $\frac{\pa f}{\pa x_i}(a) = 0$, $\nabla f(a) = 0$ et $\dif f_a = 0$ (s'ils existent).
\end{myprop}

\begin{myrem} Cette condition n'est pas applicable aux points frontières où le calcul différentiel n'apporte pas d'informations.\end{myrem}

\begin{myprop}[Conditions nécessaires à la détermination des minima et maxima]
Soit $a$ un point critique. Pour que $a$ soit un minimum (resp. un maximum), il faut que $\nabla^2 f(a)$ soit semi-définie positive (resp. négative).
\end{myprop}

\begin{myprop}[Conditions suffisantes à la déterminations des minima, maxima et points de selle]
Si la hessienne est définie (à un point critique)
\begin{itemize}
\item strictement positive, ceci garanti un minimum ;
\item strictement négative, ceci garanti un maximum ;
\item indéfinie, ceci garanti un point de selle.
\end{itemize}
\end{myprop}

\begin{myprop}[Point singulier] Dans le cas d'un point singulier, la détermination doit se faire à la main. Il n'y a aucun outil particulier. \end{myprop}

\begin{mytheo}[Existence d'un maximum et d'un minimum] Si $f$ est continue et que $A$ est compact, alors $f(A)$ est compact. Dans ce cas, $f$ admet un maximum et un minimum sur le domaine $A$. \end{mytheo}

\subsubsection{Résumé --- méthode}

\begin{itemize}
\item Calculer le gradient de $f$, trouver les points critiques et singuliers ;
\item Pour les points singuliers, on ne peut conclure qu'avec un raisonnement \emph{ad hoc} ;
\item Pour les points critiques, on calcule la hessienne de $f$ en ces points :
\begin{itemize}
\item Si elle est strictement positive, négative ou indéfinie, on a respectivement un minimum, un maximum ou un point de selle. Dans ce cas, on conclut ;
\item Si on ne peut pas conclure par le point précédent, on peut utiliser le théorème des accroissements finis, ou tenter un raisonnement \emph{ad hoc}.
\end{itemize}

\end{itemize}

\subsection{Optimisation sous contrainte}

Soit $f : A \subseteq \R^n \fl \R$ une fonction scalaire et $g : A \subseteq \R^n \fl \R$ les contraintes ($g(x) = 0$). On défini l'ensemble admissible

\begin{mydef}[Ensemble admissible] L'ensemble admissible est
\[ \Phi = \{ x \in A | g(x) = 0 \} \]
\end{mydef}

\begin{mydef}[Lagrangien] Le lagrangien est la fonction
\[ \mathcal{L} : A \times \R \fl \R : (x, \lambda) \fl L(x, \lambda) = f(x) - \lambda g(x) \]
\end{mydef}

\begin{myprop}[Condition nécessaire]
Si $a$ est un extremum local de $f$ sur $\Phi$ et si
\begin{itemize}
\item $\exists r >0$ tel que $f, g$ sont différentiables
 sur $B(a;r)$ ;
\item les dérivées partielles de $f$ et $g$ existent et sont continues en $a$ ;
\item $\nabla g(a) \neq 0$ ;
\end{itemize}
alors $\exists \lambda$ tel que $(a,\lambda)$ est un point critique de $\mathcal{L}(x,\lambda)$ ou $\nabla \mathcal{L}(a,\lambda) = 0$.
\end{myprop}

\begin{myrem}[Hessienne et Lagrangien] Attention on ne peut absolument pas utiliser la hessienne pour conclure à propos des optimisations sous contraintes résolues avec l'aide du Lagrangien. \end{myrem}

\begin{myprop}[Contraintes multiples] Dans ce cas, on applique plusieurs fois le Lagrangien d'affilé :
\[ \mathcal{L} : A \times \R^m \fl \R : (x, \lambda) \fl L(x, \lambda) = f(x) - \lambda_1 g_1(x) - \lambda_2 g_2(x) - \cdots - \lambda_m g_m(x) \]
Dans ce cas, il ne faut pas que le gradient (de la contrainte) soit non-nul, mais que
\[ \newrang J_g(a) = m \]
c'est à dire que la Jacobienne des contraintes doit être de rang plein (ou, intuitivement, que les contraintes doivent être indépendantes\footnote{\emph{cf.} cours de mécanique avec la technique des multiplicateurs de Lagrange}). \end{myprop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Calcul intégral et intégrales multiples}

\section{Généralités}

\begin{mydef}[Fonction intégrable] $f$ est intégrable sur le rectangle $D$ et a pour intégrale double la valeur $I$
\[ I = \iint_D f(x,y) \dif A \]
si et seulement si $\forall \epsilon > 0, \exists \delta > 0$ tel que pour toute partition $P \subset D$ tel que $||P|| < \delta$ et pour tous choix des points $(x_{ij}^{*},y_{ij}^{*})$ dans $R_{ij} \subset P$ on a
\[ |R(f,C,P) - I| < \epsilon \]
\end{mydef}

\begin{myrem}
\[ \dif A = \dif x \dif y = \dif y \dif x \]
\end{myrem}

\begin{mytheo}[Fonction intégrable et continue]
Si $f$ est continue sur $D$ alors $f$ est intégrable sur $D$
\end{mytheo}

\begin{myprop} $f$ est intégrable sur un domaine quelconque $D$ si et seulement si $\hat{f}$ est intégrable sur un rectangle $R$ avec
\[ \hat{f} : (x,y) \fl \hat{f}(x,y) = \left\{ \begin{array}{lll} f(x,y) & \text{si} & (x,y) \in D \\ 0 & \text{si} & (x,y) \notin D  \; \text{et} \; (x,y) \in R \end{array} \right. \]
et dans ce cas
\[ \iint_D f(x,y) \dif A = \iint_R \hat{f}(x,y) \dif A \]
\end{myprop}

\begin{myrem} Ces propriétés sont <<~extensibles~>> à $n$ dimensions. \end{myrem}

\begin{myform}[Dépendance linéaire de l'intégrant] $\forall L, M \in \R$ on a
\[ \iint_D (L f(x,y) + M g(x,y)) \dif A = L \iint_D f(x,y) \dif A + M \iint_D g(x,y) \dif A \]
\end{myform}

\begin{myform}[Autres formules] On a, si $f \leq g$ sur $D$ :
\[ \iint_D f(x,y) \dif A \leq \iint_D g(x,y) \dif A \]

Si $D = D_1 \cup D_2 \cup \dots \cup D_k$ avec $D_i \cap D_j = \varnothing \; \forall i \neq j$, si $f$ est intégrale sur $D_j$ $\forall j = 1, \dots, k$ alors
\[ \iint_D f(x,y) \dif A = \sum_{j=1}^k \iint_{D_j} f(x,y) \dif A \]

On a également que
\[ \left| \iint_D f(x,y) \dif A \right| \leq \iint \left| f(x,y) \right| \dif A \]
\end{myform}

\begin{mytheo}[Théorème de Fubini] Soit $f$ définie sur un pavé $D = [a_1,b_1] \times [a_2,b_2] \times \cdots \times [a_n,b_n] \subset \R^n$, $f$ continue sur $D$. Alors pour toute permutation $\sigma$ de la suite des nombres entiers $(1,2,\dots, n)$, on a
\[ \int_D f = \int_{\sigma(1)}^{\sigma(1)} \left[ \int_{\sigma(2)}^{\sigma(2)}   \left[ \dots \left[ \int_{\sigma(n)}^{\sigma(n)} f(x_1,x_2,\dots,x_m) \dif x_{\sigma(n)} \right]   \dots \right] \dif x_{\sigma(2)} \right] \dif x_{\sigma(1)} \]
\end{mytheo}

\section{Méthodes de calcul}

\subsection{Par inspection} Si la fonction présente une symétrie, soit sur elle même soit sur son domaine, on peut travailler par <<~morceaux~>>.

\subsection{Par itération}

\begin{mydef}[Domaine x ou y-simple]
Le domaine d'une fonction est
\begin{itemize}
\item y-simple si $x \in [a;b]$ et $y \in [c(x);d(x)]$ ;
\item x-simple si $y \in [c;d]$ et $x \in [a(y);b(y)]$.
\end{itemize}
\end{mydef}

\begin{myprop} Si $f$ est définie sur un domaine x-simple ou y-simple, alors on peut calculer l'intégrale comme ceci : soit $D$ un domaine y-simple,
\begin{align*} \iint_D f(x,y)\dif A & = \int_a^b \left[ \int_{c(x)}^{d(x)} f(x,y) \dif y \right] \dif x \\
																		& = \int_a^b \dif x \int_{c(x)}^{d(x)} f(x,y) \dif y \end{align*}
De la même manière, si le domaine $D$ est x-simple, on a
\[ \iint_D f(x,y)\dif A  = \int_c^d \dif y \int_{a(y)}^{b(y)} f(x,y) \dif x \]
\end{myprop}

\subsection{Par changement de variable}
Soit une intégrale
\[ \iint_D f(x,y) \dif A \]
à calculer. On va faire effectuer une transformation \emph{bijective} (ou \emph{mapping}) et ce en deux étapes.
\begin{enumerate}
\item on pose
\[ f(x,y) = f(x(u,v),y(u,v)) = g(u,v) \]
\item on exprime ensuite $\dif A$ en fonction de $\dif u$ et $\dif v$. Pour ce faire, on a la formule
\[ \dif A = |\newdet(J)| \dif u \dif v \]
avec $J$ la jacobienne suivante :
\[ J = \begin{pmatrix} \frac{\pa x}{\pa u} & \frac{\pa x}{\pa v} \\ \frac{\pa y}{\pa u} & \frac{\pa y}{\pa v} \end{pmatrix} \overset{\Delta}{=} \frac{\pa(x,y)}{\pa (u,v)} \]
\end{enumerate}

\section{Intégrale de lignes et de surfaces}

\subsection{Définitions et théorèmes généraux}

\begin{mydef}[Champ de vecteurs conservatifs] $\vv{F}$ est un champ de vecteurs conservatifs si et seulement si il existe $\Phi$, une fonction scalaire, telle que
\[ \vv{F} = \nabla \Phi \]
On dit alors que $\Phi$ est un potentiel scalaire et que $F$ dérive d'un potentiel.
Dans ce cas, on a que, si $C$ est une courbe dont les extrémités sont $P_1$ et $P_2$,
\[ \int_C F \cdot \dif r = \Phi(P_2) - \Phi(P_1) \]
De plus il y a indépendance des chemins choisis.
\end{mydef}

\begin{myprop}[Champ de vecteurs conservatifs]
Si $P_1 = P_2$, alors
\[ \oint F \cdot \dif r = 0 \]

Si $C = C_1 - C_2$, alors
\[ \int_C F \cdot \dif r = 0 \Leftrightarrow \int_{C_1} F \cdot \dif r - \int_{C_2} F \cdot \dif r = 0 \]
\end{myprop}

\begin{mytheo} Soit $D$ ouvert et connexe de $\R^3$ et $\vv{F}$ un champ vectoriel défini sur $D$. Il y a équivalence des trois énoncés suivants :
\begin{itemize}
\item $\vv{F}$ est conservatif sur $D$, $\exists \Phi$ tel que $\nabla \Phi = \vv{F}$ ;
\item $\oint_C \vv{F} \cdot \dif r = 0$, pour toute courbe fermée dans $D$ ;
\item pour tout points $P_0$, $P_1$ dans $D$, $\int_C F \dif r$ a la même valeur quelque soit le chemin $C$ entre $P_0$ et $P_1$.
\end{itemize}
\end{mytheo}

\begin{mydef}[Ensemble simplement connexe] C'est un ensemble dans lequel toute courbe fermée qui ne s'intersecte pas peut être transformée de manière continue en un seul point de $D$ sans quitter $D$ (par exemple, $\R^2$ est simplement connexe, mais $\R^2\backslash (0;0)$ pas).
\end{mydef}

\begin{mytheo}[Théorème de Pointcarré] Soit $D$ \emph{simplement} connexe, $F$ un champ vectoriel défini sur $D$, alors on a une 4\ieme{} équivalence aux trois précédentes
\[ \frac{\pa F_i}{\pa x_j} = \frac{\pa F_j}{\pa x_i} \]
pour $i,j = 1,2,3$.
\end{mytheo}

\subsection{Intégrale de ligne d'un champ scalaire}

\begin{mytheo}[Existence de l'intégrale de ligne]
Soit $C$ une courbe de classe $\mathcal{C}^1$ et $f$ une fonction continue, alors
\[ \int_C f \dif r \]
existe.
\end{mytheo}

\subsubsection{Méthode de calcul}

On paramétrise la courbe $C$ par une fonction $r : t \fl r(t)$ avec $t \in [a;b]$ selon les limites du problème. On calcule ensuite l'intégrale par la formule
\[ \int_C f(x,y,z) \dif r = \int_a^b f(r(t)) \left|\left| \frac{\dif r}{\dif t} \right|\right| \dif t \]

\subsection{Intégrale de surface d'un champ scalaire}

On veut donc calculer
\[ \iint_S f(x,y,z) \dif S \]

\subsubsection{Méthode de calcul}

On procède en deux étapes.
\begin{enumerate}
\item Pour cela on doit paramétrer la surface : soit $r : (u,v) \fl r(u,v)$ avec $(u,v) \in R$, représentation paramétrique de la surface. On a donc
\[ r(u,v) = x(u,v) \hat{\imath} + y(u,v) \hat{\jmath} + z(u,v) \hat{\kmath} \]
\item On exprime ensuite $\dif S$ en fonction de $\dif u$ et $\dif v$ par la formule
\begin{align*} \dif S & = \left| \left| \frac{\pa r}{\pa u} \dif u \times \frac{\pa r}{\pa v} \dif v \right| \right| \\
 											& = \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right|  \dif u \dif v \end{align*}
\end{enumerate}
On a donc finalement la formule suivante :
\[ \iint_S f(x,y,z) \dif S = \iint_R f(r(u,v)) \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right|  \dif u \dif v \]

\subsection{Intégrale de ligne de vecteur le long d'une courbe}

On veut donc calculer la composante tangentielle de vecteur le long d'une courbe
\[ \int_C \vv{F} \cdot \dif \vv{r} \]

\subsubsection{Méthode de calcul}

Comme auparavant, on paramétrise la courbe par une fonction $r : t \fl r(t) = x(t) \hat{\imath} + y(t) \hat{\jmath} + z(t) \hat{\kmath}$ avec $t \in [a;b]$
Et on a donc à calculer
\[ \int_C \vv{F} \cdot \dif \vv{r} = \int_a^b \vv{F}(r(t)) \cdot \left(  \frac{\dif \vv{r}}{\dif t} \right) \dif t \]

\subsection{Intégrale de champ de vecteur à travers une surface}

On veut calculer l'intégrale de la composante normale d'un champ vectoriel à une surface, soit
\[ \iint_S \vv{F} \cdot \hat{N} \dif S = \iint_S \vv{F} \cdot \dif \vv{S} \]

\subsubsection{Méthode de calcul}

On paramétrise la surface par une fonction $r : (u,v) \fl r(u,v) = x(u,v) \hat{\imath} + y(u,v) \hat{\jmath} + z(u,v) \hat{\kmath}$ avec $(u,v) \in D$. Le flux est alors donné par
\[ \iint_S \vv{F} \cdot \dif \vv{S} = \pm \iint_D \vv{F}(r(u,v)) \cdot \left( \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right) \dif u \dif v \]

\subsection{Résumé}

\begin{center}
\begin{tabular}{|S c|S c|}
\hline
\textbf{Type} & \textbf{Formule} \\
\hline
Scalaire sur une ligne & $\displaystyle \int_C f(x,y,z) \dif r = \int_a^b f(r(t)) \left|\left| \frac{\dif r}{\dif t} \right| \right| \dif t$ \\
\hline
Scalaire sur une surface & $\displaystyle \iint_S f(x,y,z) \dif S = \iint_R f(r(u,v)) \left| \left| \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right| \right| \dif u \dif v $ \\
\hline
Vectoriel sur une ligne & $ \displaystyle \int_C \vv{F} \cdot \dif \vv{r} = \int_a^b \vv{F}(r(t)) \cdot \left(  \frac{\dif \vv{r}}{\dif t} \right) \dif t $ \\
\hline
Vectoriel sur une surface & $ \displaystyle \iint_S \vv{F} \cdot \dif \vv{S} = \pm \iint_D \vv{F}(r(u,v)) \cdot \left( \frac{\pa r}{\pa u} \times \frac{\pa r}{\pa v} \right) \dif u \dif v $ \\
\hline
\end{tabular}
\end{center}

\section{Analyse vectorielle}

\begin{mydef}[gradient] On défini le gradient de $f$ comme
\[ \nabla f = \left( \frac{\pa f}{\pa x} , \frac{\pa f}{\pa y} , \frac{\pa f}{\pa z} \right) \]
\end{mydef}

\begin{mydef}[Divergence] On défini la divergence de $\vv{F}$ comme
\begin{align*} \div \vv{F} & = \nabla \cdot \vv{F} \\
											& = \left( \frac{\pa \cdot}{\pa x} \hat{\imath} + \frac{\pa \cdot}{\pa y} \hat{\jmath} + \frac{\pa \cdot}{\pa z} \hat{\kmath} \right) \cdot \left( F_1 \hat{\imath} + F_2 \hat{\jmath} + F_3 \hat{\kmath} \right) \\
											& = \frac{\pa F_1}{\pa x} + \frac{\pa F_2}{\pa y} + \frac{\pa F_3}{\pa z} \\
											\end{align*}
\end{mydef}

\begin{mydef}[Rotationnel] On défini le rotationnel de $\vv{F}$ comme
\begin{align*} \rot \vv{F} & = \nabla \times \vv{F} \\
														& = \left( \frac{\pa F_3}{\pa y} -  \frac{\pa F_2}{\pa z} \right) \hat{\imath} + \left( \frac{\pa F_1}{\pa z} -  \frac{\pa F_3}{\pa x} \right) \hat{\jmath} + \left( \frac{\pa F_2}{\pa x} -  \frac{\pa F_1}{\pa y} \right) \hat{\kmath} \end{align*}
\end{mydef}

\begin{myrem}[Produit vectoriel]
\[ \left( \frac{\pa F_3}{\pa y} -  \frac{\pa F_2}{\pa z} \right) \hat{\imath} + \left( \frac{\pa F_1}{\pa z} -  \frac{\pa F_3}{\pa x} \right) \hat{\jmath} + \left( \frac{\pa F_2}{\pa x} -  \frac{\pa F_1}{\pa y} \right) \hat{\kmath} =
\left| \begin{array}{ccc} \hat{\imath} & \hat{\jmath} & \hat{\kmath} \\ \frac{\pa \cdot}{\pa x} & \frac{\pa \cdot}{\pa y} & \frac{\pa \cdot}{\pa z} \\ F_1 & F_2 & F_3 \end{array}  \right| \]
\end{myrem}

\begin{myrem}[En 2 dimensions]
En deux dimensions, on a
\[ \div \vv{F} = \frac{\pa F_1}{\pa x} + \frac{\pa F_2}{\pa y} \]
\[ \rot \vv{F} = \left( \frac{\pa F_2}{\pa x} - \frac{\pa F_1}{\pa y} \right) \hat{k} \]
\end{myrem}

\begin{myform}[Formules générales]
\[ \nabla (\vv{F} \times \vv{G}) = (\nabla \times \vv{F})\vv{G} - \vv{F} \cdot (\nabla \times \vv{G}) \]
\[ \div ( \rot \vv{F}) = \nabla (\nabla \times \vv{F}) = 0 \]
\[ \rot (\nabla \Phi) = \nabla \times (\nabla \Phi) = 0 \]
\end{myform}

\begin{mydef}[Champ de vecteurs solénoïdal ou incompressible] Soit $\vv{F}$ un champ de vecteurs tel que
\[ \div \vv{F} = 0 \]
dans $D$ est dit incompressible ou solénoïdal.
\end{mydef}

\begin{mydef}[Champ de vecteurs irrotationnel] Soit $\vv{F}$ un champ de vecteurs tel que
\[ \rot \vv{F} = 0 \]
dans $D$ est dit irrotationnel.
\end{mydef}

\begin{myprop}[Champ de vecteurs solénoïdal et irrotationnel] On a les implications suivantes :
\begin{itemize}
\item $ \vv{F} \; \text{irrotationnel} \Rightarrow \vv{F} \; \text{conservatif (sur un domaine simplement connexe et ouvert)} $ ;
\item $ \vv{F} \;\text{conservatif} \Rightarrow \vv{F} \;\text{irrotationnel} $ ;
\item $ \div (\rot \vv{G}) = 0  \Rightarrow \text{Le rotationnel d'un champ de vecteur est solénoïdal} $ ;
\item $ \vv{F} \;\text{est un champ vect. solénoïdal} \Rightarrow \div (\rot \vv{G}) = 0 \;\text{(sur un domaine étoilé)} $, ou encore : il existe $\vv{G}$ tel que $\vv{F} = \rot \vv{G}$.
\end{itemize}
\end{myprop}

\begin{mydef}[Domaine étoilé] Un domaine est dit étoile si il existe $P_0$ dans $D$ tel que $P_0 + t(P - P_0)$, reliant $P_0$ à tout $P \in D$ est entièrement contenue dans $D$. \end{mydef}

\begin{myprop}[Potentiel vecteur] Si $\vv{F}$ est un champ vectoriel solénoïdal sur $D$, domaine étoilé, alors il existe un potentiel vecteur $\vv{G}$ tel que $\vv{F} = \rot \vv{G}$.
\end{myprop}

\begin{myform}[Fonction gradient]
Une fonction $\vv{f}$ est dit être un gradient si il existe $\Phi$ telle que
\[ \vv{f} = \nabla \Phi \]
Si $\vv{f}$ est définie sur un domaine simplement connexe et que $\rot \vv{f} = 0$, alors c'est un gradient.
Si c'est le cas, et si en plus $\vv{f}$ est définie sur un domaine étoilé, on peut déterminer $\Phi$ par
\[ \Phi = \int_0^1 \left( x F_1(xt,yt,zt) + y F_2(xt,yt,zt) + z F_3(xt,yt,zt) \right) \dif t \]
\end{myform}

\section{Théorèmes intégraux}

\begin{mytheo}[Théorème de Green]
On travaille dans $\R^2$. Soit $R$ une région du plan avec un bord $C$ orienté. Soit $\vv{F}$ un champ de vecteurs. On a l'égalité
\[ \iint_R \rot \vv{F} \cdot \hat{k} \dif A = \oint_C \vv{F} \cdot \dif \vv{r} \]
\end{mytheo}

\begin{myrem}[Domaines adjacents]
Si on a deux domaines adjacent, $R_1$ et $R_2$, alors le théorème est valable sur leur union, le bord adjacent étant <<~annulé~>>.
\end{myrem}

\begin{mytheo}[Théorème de Stokes]
Soit $S$ une surface de $\R^3$ et soit $C$ le contour orienté de cette surface. Alors
\[ \iint_S \rot \vv{F} \cdot \hat{N} \dif S = \oint_C \vv{F} \dif \vv{r} \]
\end{mytheo}

\begin{myprop}[Application du théorème de Stockes]
On peut calculer l'intégrale compliquée du rotationnel d'un champ de vecteur sur une surface et changeant la surface. Soit $S_1$ et $S_2$ deux surfaces possédant le même contour fermé $C$. Alors on a
\[ \iint_{S_1} \rot \vv{F} \cdot \hat{N} \dif S = \oint_C \vv{F} \cdot \dif \vv{r} = \iint_{S_2} \rot \vv{F} \cdot \hat{N} \dif S \]
\end{myprop}

\section{Rotationnel et divergence}

\begin{myprop}[Rotationnel]
Le rotationnel c'est le circulation d'un vecteur par unité de surface. C'est donc la densité de circulation. On a
\[ \rot \vv{P} \cdot \vv{N} = \lim_{\epsilon \fl 0} \frac 1{\pi \epsilon^2} \oint_{C_\epsilon} \vv{F} \dif \vv{r} \]
avec $C_\epsilon$ un cercle de rayon $\epsilon$ centré en $P$.
\end{myprop}

\begin{mytheo}[Théorème de la divergence]

Soit $D$ un volume de $\R^3$, $S$ (surface fermée) le bord de $D$, et $\hat{N}$ la normale à cette surface fermée. Alors
\[ \iiint_D \div \vv{F} \dif V = \oiint_S \vv{F} \cdot \hat{N} \dif S \]

\end{mytheo}

\begin{myprop}[Divergence]
La divergence, c'est la densité de flux par unité de volume. Soit $D_{\epsilon}$ la boule de rayon $\epsilon$ et $S_{\epsilon}$ la surface de cette boule. On a
\[ \div \vv{F}(P) = \lim_{\epsilon \fl 0} \frac{3}{4 \pi \epsilon^3} \oiint_{S_{\epsilon}} \vv{F} \cdot \hat{N} \dif S \]
\end{myprop}

\end{document}
