\input{../../lib.tex}

%\theoremstyle{definition}
%\newtheorem{mydef}{Définition}
%\newtheorem{mynota}[mydef]{Notation}
%\newtheorem{myprop}[mydef]{Propriétés}
%\newtheorem{myrem}[mydef]{Remarque}
%\newtheorem{myform}[mydef]{Formules}
%\newtheorem{mycorr}[mydef]{Corrolaire}
%\newtheorem{mytheo}[mydef]{Théorème}

\DeclareMathOperator{\res}{Res}

\hypertitle{Math\'ematique}{3}{FSAB}{1103}
{Beno\^it Legat}
{Jean-François Remacle et Grégoire Winckelmans}

\part{Equation aux dérivées partielles (EDP)}
\begin{myrem}
  \label{rem:inf}
  Ce cours traite des fonction qui ont un sens physique donc
  on a souvent besoin d'écrire que
  \[ \lim_{x\to\infty} u = \lim_{y\to\infty} u = 0. \]
  Ce n'est néanmoins pas tout le temps vrai.
  Par exemple, si $u$ est solution d'une équation de diffusion,
  vous ne pouvez pas dire que
  \[ \lim_{t\to\infty} u = 0 \]
  car l'intégrale de $u$ doit être conservée au cours du temps.

  Cependant, on peut tout le temps supposer que $u$ n'est jamais
  infini, même quand $t$ ou $x$ tendent vers l'infini.
  Sauf dans le cas d'un Dirac (voir annexe~\ref{app:dirac}), bien entendu.
\end{myrem}
\section{EDP du premier ordre}
Une EPD du premier ordre est une équation de la forme
\[ P(x, y)\fpart{u(x, y)}{x}
  + Q(x, y)\fpart{u(x, y)}{y}
  = R(x, y, u) \]
avec $P, Q, R$ des fonctions continues. % FIXME do I need it ?
$P$ et $Q$ peuvent aussi dépendre de $u$. 
Dans ce cas,
on dit que l'EDP est \emph{quasi-linéaire} et
la méthode présentée ici ne marche pas toujours.
On va traiter un cas d'EDP \emph{quasi-linéaire}
dans la section~\ref{sec:burgers} où on s'en sort tout de même
car on a $R = 0$.

On remarque que l'équation se réécrit comme suit
\begin{equation}
  \label{eq:pqrn}
  \left(P, Q, R\right) \cdot \left(
  \fpart{u}{x}, \fpart{u}{y}, -1\right) = 0.
\end{equation}

Où $\left(\fpart{u}{x}, \fpart{u}{y}, -1\right)$ est connu pour être normal
à $u$ au point $\left(x, y, u(x, y)\right)$.
Par \eqref{eq:pqrn}, on a donc que $(P, Q, R)$ est perpendiculaire à la normale
c'est à dire qu'il est tangent au graphe de $u$.

$(P, Q, R)$ forme donc un champ de vecteur tangent au graphe de $u$.
Mais comment construire $u$ à partir de ce champ ?

On remarque que comme $P$, $Q$ et $R$ sont continues, deux vecteurs proches
seront presque parallèles. Le champ est donc composé de courbes l'une à côté
de l'autre.

Pour déterminer $u$, la technique est de commencer par trouver ces courbes
dans le plan $xy$ puis de leur ajouter leur hauteur $u$.
On appelle ces courbes les \emph{courbes caractéristiques}.

\subsection{Nécessité de conditions initiales}
On connait les tangentes à $u$, on sait donc construire une
sorte de coque mais à quelle hauteur mettre cette coque ?
Aussi, comment agencer ces courbes caractéristiques l'une à côté de l'autre.
En effet, le champ de vecteurs formé par $(P, Q, R)$ est formé de vecteurs
qui sont parallèles lorsqu'ils sont très proches.
Si on a nos courbes caractéristiques, notre coque est encore maléable,
on peut déplacer ces courbes de haut en bas par rapport à une autre.

En donnant une condition initiale qui coupe une et une seule fois chaque
courbe caractéristique, on détermine tout cela.
Notons $\Gamma$, cette courbe parametrée en fonction de $s$
\[ \Gamma \equiv (x(s), y(s)). \]
On connait le long de cette courbe $u(x(s), y(s)) = f(s)$.

\subsection{Détermination des courbes caractéristiques}
\label{sec:detcara}
On cherche une courbe dont $(P, Q)$ est tangent c'est à dire que
\[ \fdif{y}{x} = \frac{Q}{P} \]
Le long de ces courbes, on a donc
\[ P(x, y)\dif y = Q(x, y) \dif x. \]
Il peut être judicieux ici de se retrouver avec que du $y$ à gauche
et que du $x$ à droite
\begin{align*}
  P'(y) \dif y & = Q'(x) \dif x & \frac{P'(y)}{Q'(x)} = \frac{P(x,y)}{Q(x,y)}.
\end{align*}
Pour déterminer ces courbes, il nous faut utiliser la condition initiale, donc
\[ \int_{y' = y(s)}^{y' = y} P'(y') \dif y'
= \int_{x' = x(s)}^{x' = x} Q'(x') \dif x' \]
où $x'$ et $y'$ sont des variables introduites juste pour l'intégration
car on a déjà $x$ et $y$ aux bornes donc on ne peut pas les utiliser pour
intégrer.

On arrive donc à une équation $g_s(x, y) = 0$ que $x$ et $y$ doivent
respecter pour être sur la courbe caractéristique passant par $(x(s), y(s))$.

\subsection{Détermination de $u$ le long des courbes caractéristiques}
On doit maintenant déterminer $u$ le long de ces courbes.
Pour ce faire, on va réutiliser le champ de vecteur $(P, Q, R)$ et sa
tangence au graphe de $u$.
On va intégrer \emph{le long d'une caractéristique} où on a
une équation $g_s(x, y) = 0$ qu'on a trouvé au point précédent
qui est respectée
\begin{align*}
  P \dif u & = R \dif x\\
  Q \dif u & = R \dif y.
\end{align*}
Ces deux équations sont équivalentes car, comme on est le long des
caractéristiques où $P \dif y = Q \dif x$.

On peut maintenant choisir la plus simple.
Sans perte de généralité, disons que c'est la première et
on va faire la même manipulation qu'au point précédent
\[ P'(u) \dif u = R'(x, y) \dif x. \]
On peut maintenant utiliser la condition initiale $u(s)$ pour
déterminer la hauteur des courbes caractéristiques
\[ \int_{u' = f(s)}^{u' = u(x,y)} P'(u') \dif u'
= \int_{x' = x(s)}^{x' = x} R'(x', y_s(x')) \dif x'. \]
où $y_s(x')$ est la valeur de $y$ lorsque $x = x'$
sur la courbe caractéristique respectant $g_s(x, y) = 0$.

\begin{myrem}[$s$ est constant]
  Il est important de bien réaliser que, lors de l'intégration,
  $s$ est bien constant et ne dépend pas de $x'$.
  On intègre sur \emph{une} caractéristique
  (celle qui passe par $(x(s), y(s))$ et qui respectent donc
  $g_s(x', y') = 0$), c'est d'ailleurs pour insister sur cela que je mets
  $s$ en indice de $g$ et pas en argument.
  Il est d'ailleurs inutile de développer $f(s)$ ici si on nous donne
  son équation.
\end{myrem}

On arrive à une équation de $u(x, y)$ dépendant de $x$ et $s$.
Il faut maintenant isoler $s$ dans la condition $g_s(x, y) = 0$ et trouver
$s(x, y)$.
On remplace alors, dans l'expression de $u(x, y)$, $s$ par $s(x, y)$.

\subsection{Equation de transport (ou de convection)}
On peut écrire de manière générale l'équation de transport de la façon
suivante

$$c\fpart{u}{x} + \fpart{u}{t} = 0$$

Où $c$, dans le cas d'un problème physique, est une vitesse. $c$ peut être
une constante, une fonction de $x$ (cas du transport dans un milieu non-homogène),
ou une fonction de $t$. $c$ peut même être une fonction de $u$, dans ce cas
l'EDP n'est plus linéaire, nous traiterons un exemple d'équation de transport
non-linéaire dans la section \ref{sec:burgers}. Comme l'équation est homogène,
$u$ est conservée le long des caractéristiques et ce genre d'équation est assez
simple à résoudre.

\subsubsection{Equation de transport sous-forme conservative}
Une autre forme de l'équation de transport est la forme conservative :

$$\fpart{}{x}(cu) + \fpart{u}{t} = 0$$

où $c = c(x,t,u)$ (on remarque que dans le cas $c$ constant ou $c = c(t)$,
on revient au cas précédent). L'équation sous forme conservative est telle
que l'intégrale de $u$ est conservée au cours du temps. C'est donc souvent 
ce type d'équation qu'on rencontre en physique. Pour résoudre cette équation
par la méthode des caractéristiques, il faut d'abord la développer 

$$c\fpart{u}{x} + \fpart{u}{t} = -\fpart{c}{x}u.$$

\subsubsection{Equation de transport non-linéaire}
\label{sec:burgers}
La forme générale de l'équation de transport non-linéaire est

$$c(u)\fpart{u}{x} + \fpart{u}{t} = 0.$$

avec comme condition initiale, par exemple, $u(s, 0) = f(s)$.
Cependant, nous considèrons dans ce cours un cas particulier, appelé
équation de \textit{Burgers}, dans lequel $c(u) = u$. 
Les caractéristiques sont obtenues par intégration de 

$$\dif x = u\dif t.$$

Comme l'équation de Burgers est homogène, $u$ est conservée
le long des caractéristiques et on a des caractéristiques de la forme
$x = s + f(s)t$. On constate assez facilement que les pentes des 
caractéristiques dépendent de $s$, elles sont donc variables. Cela
peut poser problème car des caractéristiques peuvent se croiser, la 
solution deviendra alors discontinue. On peut démontrer assez facilement
que le temps critique pour l'apparition de la discontinuité correspond
au minimum de $\frac{-1}{\fdif{f}{s}}$. Cette fonction est minimum
lorsque $\ffdif{f}{s}$.

\subsection{Équation de densité de traffic}
\begin{myrem}
	L'équation de densité de traffic n'a pas été vue lors de 
	l'année 2014-2015. Cependant elle est assez intéressante et permet
	d'avoir un exemple concret d'utilisation d'une EDP (et plus précisement
	d'une équation de transport sous forme conservative).
\end{myrem}

\label{sec:traffic}
Soit
\begin{itemize}
  \item $\rho(x, t)$, le nombre de véhicules par mètre
    en $x$ au temps $t$;
  \item $u(x, t)$, la vitesse des véhicules en $x$ au temps $t$;
  \item $N(x_0, x_1, t)$, le nombre de véhicules entre $x_0$ et $x_1$
    au temps $t$.
\end{itemize}
On peut écrire
\begin{align}
  \label{eq:Nrho}
  N(x_0, x_1, t) & = \int_{x_0}^{x_1} \rho(x, t) \dif x\\
  \nonumber
  \fpart{N}{t} & \stackrel{\eqref{eq:Nrho}}{=}
  \int_{x_0}^{x_1} \fpart{\rho(x, t)}{t} \dif x\\
  \nonumber
  \fpart{N}{t} & = u(x_0, t) \rho(x_0, t) - u(x_1, t) \rho(x_1, t)\\
  \nonumber
  & = - \int_{x_0}^{x_1} \fpart{(u\rho)}{x} \dif x.
\end{align}
D'où
\[ \int_{x_0}^{x_1} \fpart{\rho}{t} + \fpart{(u\rho)}{x} \dif x = 0. \]
Comme c'est vrai pour tout intervalle $[x_0, x_1]$, on a
\begin{equation}
  \label{eq:traffic}
  \fpart{\rho}{t} + \fpart{(u\rho)}{x} = 0.
\end{equation}

\subsubsection{Le modèle de Lighthill-Whitham-Richards}
En utilisant le modèle de Lighthill-Whitham-Richards qui
se résume à dire qu'il y a une vitesse et une densité maximale
et que la vitesse diminue linéairement avec la densité du trafic,
autrement dit
\[ u(\rho) =
u_\mathrm{max} \left(1 - \frac{\rho}{\rho_\mathrm{max}}\right) \]
on peut réécrire l'équation~\eqref{eq:traffic} en fonction de $\rho$
en observant que
$\fpart{(u(\rho)\rho)}{x} = \fpart{(u(\rho)\rho)}{\rho}\fpart{\rho}{x}$.

On obtient
\[ \fpart{\rho}{t} + u_\mathrm{max}
\left(1 - \frac{2\rho}{\rho_\mathrm{max}}\right)\fpart{\rho}{x} = 0 \]
où le facteur devant $\fpart{\rho}{x}$ peut être interprété comme la vitesse
de l'\emph{information} car on sait par la méthode des caractéristiques
qu'il vaut la pente des caractéristiques $\fdif{x}{t}$.

La technique vue à la section~\ref{sec:detcara} en marche
pas car $Q$ est en fonction de $\rho$ et pas que de $x$ et $t$.
Seulement, les caractéristiques sont tout de même des droites
car, comme l'équation est homogène,
$\rho$ ne varie pas le long des caractéristiques et cette
pente est donc constante et vaut
\[ \fdif{x}{t} =
u_\mathrm{max} \left(1-\frac{2\rho(s)}{\rho_\mathrm{max}}\right). \]
On remarque que cette vitesse est \emph{négative} si
$\rho > \frac{\rho_\mathrm{max}}{2}$,
en effet, c'est la vitesse de l'\emph{information} et non
des voitures.

\paragraph{Intersection des caractéristiques}
Le problème c'est que si, dans la conditions initiale,
il y a un traffic dégagé suivit d'un traffic dense,
il y aura un choc des caractéristiques.
Cette endroit de choc sera une discontinuité dans la densité du traffic,
on passera de caractéristiques voulant que le traffic soit dégagé à des
caractéristiques voulant un traffic dense.
Ça peut s'apparenter à des voitures rencontrant un bouchon.

Si il y a une discontinuité dans le traffic en $x_0$ au départ,
comme dans le cas d'un feu rouge,
\[ \rho_-(x_0) > \rho_+(x_0) \]
on a une zone où aucune caractéristique ne vient donner d'information
car la vitesse de l'information en $x_0$ à gauche est plus petite
que celle à droite.
Il faut alors considérerer que toutes les densités de l'intervalles
$[\rho_-(x_0); \rho_+(x_0)]$ se trouvent au point $x_0$.
Ainsi, on peut faire partir toutes les caractéristiques correspondantes
du point $x_0$ et on peut recouvrir cet intervalle.

\section{EDP du second ordre}
Une EPD du deuxième ordre est une équation de la forme
\[ A \ffpart{\phi}{x} + B\fdpart{\phi}{x}{y} + C\ffpart{\phi}{y} = R \]
où $A$, $B$, $C$ et $R$ sont des fonction régulière qui peuvent
dépendre de $x$, $y$, $\phi$ ou même de $\fpart{\phi}{x}$ ou de
$\fpart{\phi}{y}$. On suppose que l'on dispose de $\phi(\Gamma(s))$ et
$\fpart{\phi}{n}(\Gamma(s))$ le long de la courbe paramétrisée $\Gamma
\equiv (x(s), y(s))$, avec $n(s)$ la direction normale à $\Gamma$.

Le problème est bien posé si et seulement si
\[
  \begin{vmatrix}
    A & B & C\\
    \fdif{x}{s} & \fdif{y}{s} & 0\\
    0 & \fdif{x}{s} & \fdif{y}{s}
  \end{vmatrix}
  \neq 0.
\]

On distingue les EDP de second ordre en 3 cas
\begin{center}
  \begin{tabular}{|l|l|}
    \hline
    $B^2 - 4AC > 0$ & EDP hyperbolique\\
    $B^2 - 4AC = 0$ & EDP parabolique\\
    $B^2 - 4AC < 0$ & EDP elliptique\\
    \hline
  \end{tabular}
\end{center}

\subsection{Équation d'onde}
Une équation d'onde est une EDP hyperbolique dont l'équation est
\[ c^2 \lap \phi = \ffpart{\phi}{t}. \]
Soient deux conditions initiales : $\phi(x, y, t=0) = \phi_0(x, y)$ et
$\fpart{\phi}{t}(x, y, t=0) = \phi'(x, y)$.

En 1D, ça devient
\[ c^2\ffpart{\phi}{x} - \ffpart{\phi}{t} = 0. \]
avec les conditions initiales $\phi(x, t=0) = f(x)$ et $\fpart{\phi}{t}(x, t=0) = g(x)$.

La formule d'\textit{Alembert} nous permet alors d'arriver à
\[ \phi(x, t) = \frac{1}{2}(f(x - ct) + f(x + ct)) + \frac{1}{2c}\int_{x-ct}^{x+ct}
g(x')dx'.\]

Dans le cas $g=0$, cette formule nous permet de trouver graphiquement $\phi$ à un temps
$t$ à partir du réseau des caractéristiques et de $f(s)$.

% TO DO : zone de dépendance et zone d'influence (voir figure dans le correctif
% de l'APE3), ajouter la démonstration de la formule d'Alembert
% en annexes?

\subsection{Équation de diffusion}
Une équation de diffusion est une EDP parabolique dont l'équation est
\[ \alpha \lap \phi = \fpart{\phi}{t} \]
où $\alpha$ est le coefficient de \emph{diffusivité}.

En 1D, ça devient
\[ \alpha \ffpart{\phi}{x} = \fpart{\phi}{t}. \]

On remarque qu'on a ici besoin d'une seule condition initiale $\phi_0(x, y)$
puisque $\fpart{\phi}{t}(x, y) = \lap \phi_0(x, y)$.
La deuxième condition est donc donnée par l'EDP elle même.

\begin{myrem}[unités de $\alpha$]
  On remarque que comme $\lap \phi$ a des unités de $[\phi]\per\meter\squared$
  et que $\fpart{\phi}{t}$ a des unités de $[\phi]\per\second$,
  $\alpha$ doit avoir des unités de $\meter\squared\per\second$.
\end{myrem}

L'équation de diffusion étale une fonction de départ en gardant
l'intégrale constante.
Elle perd tous les détails de la fonction de départ et n'est donc
pas réversible, c'est à dire que lorsqu'on connait $\phi(t)$, on
peut connaitre $\phi(t + \Delta t)$ uniquement si $\Delta t \geq 0$.

Si la fonction de départ est un Dirac (voir annexe~\ref{app:dirac})
\[ \phi(x, 0) = Q \delta(x). \]
La solution est une gaussienne
\[ \phi(x, t) = \frac{Q}{\sqrt{\pi4\alpha t}}
\exp\left(-\frac{x^2}{4\alpha t}\right). \]
Si la fonction de départ est une gaussienne
\[ \phi(x, 0) = \phi_0 \exp\left(-\frac{s^2}{b^2}\right), \]
on peut imaginer qu'elle vienne d'un Dirac
\footnote{On n'est pas sûr que ça soit vrai,
comme dit précédemment, une diffusion n'est pas réversible.
Il pourrait y avoir plus de détails qui
ont été effacés mais considérer l'un ou l'autre ne change rien
si on s'intéresse au futur.}.
On a alors nécessairement
\begin{align*}
  b^2 & = 4\alpha t\\
  \phi_0 & = \frac{Q}{\sqrt{\pi4\alpha t}}.
\end{align*}
D'où
\[ \phi(x, t) = \frac{\phi_0b}{\sqrt{b^2 + 4\alpha t}}
\exp\left(-\frac{x^2}{b^2 + 4\alpha t}\right)\]

Cela permet de trouver la solution pour n'importe quelle condition initiale,
que l'on considère comme une somme de Dirac :
$\phi_0(x) = \int \phi_0(x')\delta(x-x') dx'$.
La solution est alors la somme des solutions pour tous les Dirac considérés :
\[\phi(x, t) = 
    \int_{-\infty}^{\infty} \frac{\phi_0(x')}{\sqrt{\pi 4\alpha t}} 
    \exp\left(-\frac{(x-x')^2}{4\alpha t}\right) dx'
\]

\begin{myrem}
L'équation d'onde et l'équation de diffusion sont résolues ici dans
le cas d'un domaine infini, ils sont alors donnés en condition initiale :
$\phi(x, y, t=0)$ est donné sur tout le domaine (et aussi 
$\fpart{\phi}{t}(x, y, t=0)$ pour l'équation d'onde). Si le domaine
est borné, il faut imposer en plus une condition limite sur la frontière
du domaine, l'équation se résout alors par séparation des variables
(sous certaines conditions).
\end{myrem}

\subsection{Équation de Laplace}
Une équation de Laplace est une EDP elliptique dont l'équation est
\[ \lap \phi = 0. \]
En 2D, ça devient
\[ \ffpart{\phi}{x} + \ffpart{\phi}{y} = 0. \]
Elle se résout à l'aide d'une séparation des variables
(voir section~\ref{sec:sepvar}).

% TODO : lien entre équation de diffusion et équation de Laplace
% Voir exemple de diffusion thermique donné en cours.

\subsection{Équation de Poisson}
Une équation de Poisson est une EDP elliptique dont l'équation est
\[ \lap \phi = R. \]
En 2D, ça devient
\[ \ffpart{\phi}{x} + \ffpart{\phi}{y} = R. \]
On remarque que l'équation de Laplace est le cas particulier de
l'équation de Poisson où $R = 0$.

\section{Méthode de séparation de variables}
\label{sec:sepvar}
Nous nous intéressons ici à la méthode de séparation des variables pour les EDP
qui n'est pas la même que celle pour les EDO.

On va supposer qu'il existe deux fontions $X$ et $Y$ telles que
\[ \phi(x, y) = X(x) \cdot Y(y). \]
À partir de cette supposition, il est maintenant possible de résoudre
les EDP plus facilement.

Pour que cette méthode marche, il faut deux choses
\begin{itemize}
  \item L'équation doit être homogène et linéaire;
  \item Le domaine doit se ramener à une domaine rectangulaire
    (ou cubique si on est en 3D)
    avec, si nécessaire, un changement de variable.
\end{itemize}
Pour la 2\ieme{} condition, un changement de variable classique est
celui pour passer d'un cercle à un carré
\[ (x, y) = (r\cos\theta, r\sin\theta). \]
Dans ce cas,
\begin{align*}
  \lap \phi & = \ffpart{\phi}{x} + \ffpart{\phi}{y}\\
  & = \ffpart{\phi}{r} + \frac{1}{r}\fpart{\phi}{r}
  + \frac{1}{r^2}\ffpart{\phi}{\theta}
\end{align*}

Sur les côtés, on pose des conditions limite.
Ces conditions sont soit homogènes, soit non-homogènes.
Elles sont de 3 types:
\begin{itemize}
  \item Conditions de Dirichlet,
    on impose une condition sur $\phi$;
  \item Condition de Neumann,
    on impose un condition sur $\fpart{\phi}{v}$ pour
    une certaine variable $v$;
  \item Condition de Robin,
    on impose une condition sur une combinaison linéaire de
    $\phi$ et $\fpart{\phi}{v}$ pour une certaine variable $v$.
\end{itemize}

On peut aussi appliquer la méthode de séparation des variables
avec un domaine infini, dans ce cas il peut être utile
de se souvenir de la remarque~\ref{rem:inf}, c'est à dire
que que $u$ ne peut pas tendre vers l'infini,
pour éliminer une des exponentielles qu'on obtient.

\subsection{L'équation de Laplace par séparation de variables}
Supposons qu'on soit dans un rectangle.
Il nous faut 4 conditions limites, une sur chaque côté.
Si toutes les conditions limites sont homogènes, la solution est la solution
triviale $\phi = 0$.

On ne sait gérer qu'une seule condition limite non-homogène à la fois.
S'il y en a plusieurs, on les traite séparément en prenant les autres
égales à 0 puis on somme les solutions.
En effet, comme l'équation de Laplace est linéaire, le principe
de superposition s'applique.
Supposons donc par la suite qu'on ait une seule condition limite non-homogène.

\subsubsection{Recherche des solutions}
On peut calculer
\[ \lap \phi = X'' Y + X Y''. \]
On a donc, dans le cas de l'équation de Laplace,
\[ \frac{X''}{X} + \frac{Y''}{Y} = 0 \]
où on a divisé par $XY$ car on les suppose non-nuls comme on
écarte la solution triviale.

En dérivant l'équation par $x$, on a
\[ \fdif{\left(\frac{X''}{X}\right)}{x} + 0 = 0 \]
d'où on tire que $\frac{X''}{X}$ est égal à une constante $\pm k^2$
et donc $\frac{Y''}{Y} = \mp k^2$.
On a ici 3 cas à traiter où on pose $k > 0$
\begin{itemize}
  \item $X'' = k^2X$ qui donne une solution $\phi_1 = X_1Y_1$
    avec $X_1 = Ae^{kx} + Be^{-kx}$ et $Y_1 = C\cos(ky) + D\sin(ky)$;
  \item $X'' = -k^2X$ qui donne une solution $\phi_2 = X_2Y_2$
    avec $X_2 = E\cos(kx) + F\sin(kx)$ et $Y_2 = Ge^{ky} + He^{-ky}$;
  \item $X'' = 0$ qui donne une solution $\phi_3 = X_3Y_3$
    avec $X_3 = Ix + J$ et $Y_3 = Ky + L$.
\end{itemize}

\subsubsection{Application des conditions limites homogènes}
Posons $\phi_\Sigma = \phi_1 + \phi_2 + \phi_3$.
Que doit-on faire maintenant ?
Appliquer les conditions sur $\phi_1$, $\phi_2$ et $\phi_3$ séparément ou
sur $\phi_\Sigma$ ?
Rappelons nous que rien ne nous indiquait que $\phi$ était effectivement
à variable séparable.
Ici, on a trouvé $\phi_1$, $\phi_2$ et $\phi_3$ qui respectent
l'équation de Laplace, $\phi_\Sigma$ la respecte aussi mais n'est plus
à variable séparable.

Comme rien ne nous impose que $\phi$ soit à variable séparable,
dans le cas général, il faut appliquer les conditions limites sur
$\phi_\Sigma$ et non sur $\phi_1$, $\phi_2$ et $\phi_3$ séparément sinon
on se rajoute une contrainte qui n'existe pas et on perd potentiellement
des solutions.
Seulement, dans la plupart des cas, ça ne change rien
de le faire séparément ou non et ça complique énormément les calculs.
En pratique, une bonne manière de faire est donc d'appliquer
les conditions limites séparément et si plus aucune solution
ne subsiste, appliquer les conditions limites sur $\phi_\Sigma$.
En effet, on a déjà potentiellement perdu des solution avec la supposition
que $\phi$ était à variable séparable, pourquoi ne pas continuer
sur notre lancée ?

En appliquant les conditions limites séparément,
pour les conditions limites homogènes,
on arrive à des équations du style
\[ u(x, 0) = 0 = X(x)Y(0). \]
Si $X(x) = 0$ pour tout $x$, on a la solution triviale, comme elle
est inintéressante, supposons que $\exists x$ tel que $X(x) \neq 0$ et donc
on a $Y(0) = 0$.
Une des deux variables a deux conditions homogènes,
ça permet d'éliminer deux solution.
Par exemple, si c'est $X$ qui a deux conditions homogènes,
on arrive nécessairement à $X_1 = X_3 = 0$
donc $\phi_1$ et $\phi_3$ sont triviaux.

On remarque ici que $\phi_3$ est éliminé dans les deux cas.
En effet, il n'est que utile dans le cas où on applique
les conditions limites sur $\phi_\Sigma$.
Il peut être utile par exemple dans un cas de domaine semi infini où
on veut que $\phi$ ne tende pas vers 0.
Les exponentielles sont alors inutiles car elle tendent
soit vers l'infini, soit vers 0
et les cosinus et les sinus ne convergent pas.
Dans ce cas, si on applique les conditions limite séparément,
on ne trouvera pas de solution, il faudra
l'appliquer sur $\phi_\Sigma$ et on ne trouvera pas un $\phi_3$ nul.
On dira alors que $\phi_3$ est la solution en régime.
$\phi_1 + \phi_2$ est alors la solution transitoire.

En continuant dans notre exemple avec les deux
condition homogène en $x$, supposons que les deux
conditions soient $u(0, y) = 0$ et $u(L, y) = 0$.
On a alors
\begin{align*}
  E & = 0\\
  E\cos(kL) + F\sin(kL) & = 0
\end{align*}
D'où, soit $F = 0$, soit $\sin(kL) = 0$, le premier cas nous
donne la solution triviale, ce qui ne nous intéresse pas.
Le deuxième cas nous restreint les possibilités de $k$,
en se souvenant qu'on a posé que $k > 0$, à
\begin{align*}
  k_n & = \frac{n\pi}{L} & n = 1, 2, \ldots
\end{align*}

Complétons maintenant chacune de ces solution avec la solution
associée pour $Y$.
On a à résoudre
\[ Y_n'' = k_n^2Y_n \]
il est important d'utiliser $k_n$ ici et non $k$
car $X''/X$ et $-Y''/Y$ valent la même constante.

À nouveau, rien ne nous oblige que $\phi$ soit à variable
séparable et en l'imposant on risque de perdre des solutions.
Par le principe de superposition,
la solution générale s'écrit donc maintenant,
\[ \phi(x,y) = \sum_{n=0}^{n=\infty} F_n \sin(k_nx)Y_n(y). \]

\subsubsection{Application de la condition limite non-homogène}
L'astuce, ici, c'est de remarquer que les sinus
forment une base orthogonale avec le produit vectoriel
\[ \int_{x = 0}^{x = L} \sin\left(\frac{n\pi}{L} x\right)
\sin\left(\frac{m\pi}{L} x\right) \dif x. \]
En effet, si $n \neq m$, ce produit vectoriel est nul et
si $n = m$, il vaut $\frac{L}{2}$.

Utilisons cela pour calculer $A_m$ avec la condition non-homogène.
Prenons, par exemple, comme condition $u(x, 0) = f(x)$, qui donne
\[ \sum_{n = 1}^{n = \infty} F_n \sin\left(\frac{n\pi}{L} x\right) Y_n(0)
= f(x) \]
et en appliquant le produit vectoriel avec
$\sin\left(\frac{m\pi}{L} x\right)$, on a
\[ F_mY_m(0)\frac{L}{2} =
\int_{x=0}^{x=L} \sin\left(\frac{m\pi}{L}x\right) f(x) \dif x \]
d'où
\[ F_m = \frac{
\int_{0}^{L} \sin\left(\frac{m\pi}{L}x\right) f(x) \dif x}
{Y_m(0)\frac{L}{2}}. \]
\begin{myrem}
  Il est primordial ici de se rendre compte qu'on se base sur un
  exemple où les conditions limites sont $u(0, y) = 0$ et $u(L, y) = 0$.
  On a donc $k_n = \frac{m\pi}{L}$ et comme bornes pour le produit
  vectoriel 0 et $L$.
  Pour d'autres conditions limites, il y a évidemment d'autres bornes.
\end{myrem}

\part{Analyse complexe}
On a une définition semblable pour la limite et la continuité
pour $f(z)$ que pour les fonctions à plusieurs variables
\begin{mydef}[Limite]
  $\lim_{z\to z_0} f(x) = L$ si et seulement si
  \[ \forall \epsilon>0, \exists\delta>0:
  |z-z_0|<\delta \Rightarrow |f(z)-L|<\epsilon. \]
\end{mydef}
\begin{mydef}[Continuité]
  $f$ est continue en $z_0$ si et seulement si $f$ est définie en $z_0$ et
  \[ \lim_{z \to z_0} f(z) = z_0. \]
\end{mydef}

La définition de la dérivée par contre, est fort différente
et c'est là que réside tout l'intérêt de l'analyse complexe.
En effet, pour les fonctions à plusieurs variables, le dénominateur est
équivalent à $|z - z_0|$ alors qu'ici, c'est $z - z_0$.
\begin{mydef}
  \[ f'(z_0) = \lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0}. \]
\end{mydef}

En pratique, les fonctions usuelles sont continues et dérivables si
on défini les coupures correctement et les règles de dérivations
habituelles sont aussi vérifiées.
Par exemple, $f(z) = z^2$ est continue et dérivable pour tout $z$ et
$f'(z) = 2z$.
Par contre la fonction conjugué $f(z) = \bar{z}$ n'est pas dérivable.

On définit d'ailleurs deux nouveaux termes
\begin{mydef}
  Une fonction $f(z)$ holomorphe est une fonction définie et dérivable
  en tout point d'un sous-ensemble ouvert du plan complexe.
\end{mydef}
\begin{mydef}
  Une fonction $f(z)$ entière est une fonction holomorphe sur tout
  le plan complexe.
\end{mydef}

Comme $f(z)$ a une image complexe,
on peut définir $P$ et $Q$ comme suit $f(z) = P(z) + iQ(z)$.
Définissons aussi $x$ et $y$ comme suit $z = x + iy$.
On a alors le théorème suivant.
\begin{mytheo}[Condition de Cauchy-Riemann]
  $f$ est différentiable si et seulement si
  \[ \left\{\begin{aligned}
      \fpart{P}{x} & = \fpart{Q}{y}\\
      \fpart{Q}{x} & = -\fpart{P}{y}.
  \end{aligned}\right. \]
\end{mytheo}

Un corrolaire direct de ce théorème, c'est que $f$ respecte l'équation
de Laplace $\lap f = 0$.
En effet, on obtient sans mal $\lap P = 0$ et $\lap Q = 0$ d'où
$\lap f = \lap P + i\lap Q = 0$.

Cauchy-Riemann nous permet de montrer
une théorème fort intéressant concernant les intégrales baptisé le
théorème de Cauchy.
\begin{mytheo}[Théorème de Cauchy]
  Soit $f(z)$ holomorphe dans un domaine $D$ ``simplement connexe''
  avec $C \subseteq D$.
  On a
  \[ \oint_C f(z) \dif z = 0. \]
\end{mytheo}
De ce dernier, on peut montrer la formule intégrale de Cauchy.
\begin{mytheo}[Formule intégrale de Cauchy]
  \label{thm:int}
  Soit $f(z)$ holomorphe dans un domaine $D$
  avec $C \subseteq D$. % fermé simple
  On a
  \[ f(z) = \frac{1}{2\pi i}\oint_C
  \frac{f(\zeta)}{\zeta-z} \dif \zeta \]
  où on parcourt $C$ \strong{dans le sens trigonométrique}
  \footnote{i.e. le sens anti-horlogique.}.
  En effet, si cette formule était indépendante du sens,
  ça n'aurait pas de sens,
  en changeant le sens de l'intégrale,
  on doit avoir $-f(z)$.
  C'est le sens trigonométrique ici car c'est dans ce sens
  là qu'on mesure les angles orientés et que les angles en trigonométrie
  sont orientés.
  En effet, $\dif \theta$ est positif si on tourne dans le sens
  trigonométrique et négatif sinon.
\end{mytheo}

Cette formule permet déjà de calculer certaines intégrales mais
en fait, ce n'est qu'un \emph{cas particulier} du théorème des Résidus.
En effet, on verra que $z$ est ici un pôle d'ordre 1 de
$g(\zeta)=\frac{f(\zeta)}{\zeta-z}$
et $f(z)$ est alors son résidu en $z$ noté
$\res(g,z)$.
Toutes les intégrales qui peuvent se calculer avec la formule
intégrale de Cauchy peuvent donc être calculée avec le théorème
des Résidus.

\begin{mytheo}[Théorème des Résidus]
  Soit $D$ un domaine simplement connexe avec
  $C \subseteq D$ et $f$ analytique
  sauf en certains pôles $z_1, z_2, \ldots, z_n$.
  On a
  \[ \oint_C f(z) \dif z = 2i\pi\sum_{k=1}^n \res(f, z_k). \]
  À nouveau,
  comme pour le théorème~\ref{thm:int},
  $\oint_C$ doit être dans le sens trigonométrique sinon,
  il y a un signe $-$ dans le membre de droite de l'équation.
\end{mytheo}

Il y a deux nouveaux points à aborder pour comprendre ce théorème
``magique''.
Qu'est-ce qu'un pôle et comment calculer un résidu.

Supposons qu'on ait une singularité $z$.
Elle peut être
\begin{itemize}
  \item soit ``effaçable'', ça veut dire que $f$ reste borné en $z$;
  \item soit un pôle, ça veut dire que $f$ tend vers l'infini en $z$;
  \item soit essentielle, ça veut dire que la limite n'existe pas en $z$.
\end{itemize}

L'idée du théorème des Résidus, c'est que $\oint_C f(z) \dif z$ ne vaut
pas 0 mais $2i\pi$ fois la somme des
$\frac{1}{2i\pi} \oint_{C_k} f(z) \dif z$ qu'on
nomme $\res(f, z_k)$.
Il suffit donc de calculer ces intégrales.

Pour cela, introduisons un développement en série plus générale qui
part de $-\infty$.
\begin{mydef}[Développement en série de Laurent]
  Soit $f$ analytique sur $D \subseteq \mathbb{C}$,
  le développement en série de Laurent de $f$ autour de $a$ est
  \[ f(z) = \sum_{-\infty}^{\infty}b_n(z-z_0)^n. \]
\end{mydef}
L'avantage de ce développement en série, c'est que
$\oint_C (z-z_0)^j$ est nul pour tout $j \neq -1$ et vaut $2\pi i$ si
$j = -1$.

Le développement de Taylor est le cas particulier de ce développement
pour $f$ analytique.
Ici, $f$ n'est pas analytique car il a un pôle.
L'astuce c'est que pour tout pôle, il existe un plus petit entier
$m$ tel que $H = (z-z_k)^mf$ est analytique sur $C_k$.
On dit alors que $z_k$ est un pôle à l'ordre $m$.

On peut alors calculer Taylor pour $H$ en $z_k$,
le diviser par $(z-z_k)^m$ et ça donne le développement en série de Laurent
de $f$.
En intégrant ce développement en série, on a vu plus haut,
qu'il n'y avait que $b_{-1}$ qui restait,
c'est à dire $c_{m-1}$ dans le développement en série de Taylor de
$H$.
On a donc
\begin{align*}
  b_{-1} & = c_{m-1}\\
  & = \frac{1}{(m-1)!}\lim_{z\to z_k} H^{(m-1)}(z)\\
  & = \frac{1}{(m-1)!}\lim_{z\to z_k} \fdif{^{m-1}}{z^{m-1}}
  \left((z-z_k)^m f(z)\right).
\end{align*}

\begin{mydef}[Résidu d'une fonction] 
  Le résidu d'une fonction $f$ avec un pôle de degré $n$ en $a$ est
  $\res_a f = b_{-1}$.
  Il peut également être obtenu par la formule suivante
  \[ \res_a f = \frac{1}{(n-1)!} \lim_{z \to a}
  \fdif{^{n-1}}{z^{n-1}}((z-a)^nf(z)) \]
\end{mydef}

On peut maintenant calculer des intégrales très compliquées à l'aide de
deux lemmes.
\begin{mylem}[Lemme de Jordan I]
  Soit $f$ analytique sur $D \subseteq \mathbb{C}$.
  Si $\lim_{|z|\to\infty} |zf(z)| = 0$, alors
  \[ \lim_{R\to\infty} \oint_{C(0,R,\alpha)}f(z)\dif z = 0. \]
\end{mylem}
\begin{mylem}[Lemme de Jordan III]
  Soit $f$ analytique sur $D \subseteq \mathbb{C}$.
  Si $\lim_{|z|\to 0} |zf(z)| = 0$, alors
  \[ \lim_{R\to 0} \oint_{C(0,R,\alpha)}f(z)\dif z = 0. \]
\end{mylem}

\annexe
\section{Dirac}
\label{app:dirac}
Un Dirac est une fonction avec singularité en 0.
Elle se note $\delta(x)$ et est définie par
\begin{equation}
  \label{eq:dirac1}
  \delta(x) =
  \begin{cases}
    \infty, & x = 0\\
    0, & x \neq 0
  \end{cases}
\end{equation}
et
\begin{equation}
  \label{eq:dirac2}
  \int_{-\infty}^{\infty} \delta(x) \dif x = 1.
\end{equation}

Ça peut paraitre bizarre que l'intégrale soit finie alors que
$\delta(0) = \infty$.
Seulement, rappelez vous qu'une intégrale c'est une aire et que
l'aire d'un Dirac n'est pas entièrement définie par \eqref{eq:dirac1}
car celle ci impose que l'aire vaut $0 \cdot \infty$ qui est un cas
d'indétermination.
\eqref{eq:dirac2} permet de lever cette indétermination.

\end{document}
