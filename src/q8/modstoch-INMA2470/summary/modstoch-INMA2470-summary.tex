\documentclass[en]{../../../eplsummary}

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\var}{\mathbb{V}ar}

\hypertitle{Discrete stochastic models}{8}{3}{4}
{Benoît Legat}
{Philippe Chevalier}

\begin{center}
  \begin{tikzpicture}
    \node[left] at (-2,-1) {Poisson Processes};
    \node[left] at (-2, 1) {Renewal Processes};
    \draw[thick,->] (-2, 1) to (-1,.1);
    \draw[thick,->] (-2, -1) to (-1,-.1);
    \node[right] at (-1,0) {Processus de Markov};
    \draw[thick,->] (.8, .8) to (.8,.2);
    \node[above] at (.8, .8) {Chaines de Markov};
    \draw[thick,->] (2.8, 0) to (3.4,0);
    \node[right] at (3.4, 0) {Files d'attente};
    \draw[thick,->] (2.5, 1) to (3.4,1);
    \node[right] at (3.4, 1) {Processus de décision Markovien};
  \end{tikzpicture}
\end{center}

\section{Probability}
For more information for this chapter, read \cite[\S1.7]{gallager1995discrete} or \cite[\S1.5]{gallager2010discrete}.
%\[ \bar{F}_X(x) = 1 - F_X(x) \]

\begin{myineg}[Markov's inequality]
  Let $Y$ be a positive random variable.
  We have
  \[ \Pr[Y \geq y] \leq \frac{1}{y}\E[Y] \]
  \begin{proof}
    We can see that
    \begin{align*}
      E[Y]
      & = \int_0^\infty x f(x) \dif x\\
      & \geq \int_y^\infty x f(x) \dif x\\
      & \geq y \int_0^y f(x) \dif x\\
      & = y \Pr[Y \geq y].
    \end{align*}
  \end{proof}
\end{myineg}

If the variance is known, there is a useful corollary
\begin{myineg}[Chebyshev's inequality]
  Let $Y$ be a random variable.
  For an $\epsilon > 0$, we have
  \[ \Pr[|Y-\E[Y]| \geq \epsilon] \leq \frac{\sigma_Y^2}{\epsilon^2}. \]
  \begin{proof}
    Let $Z = (Y - \E[Y])^2$, we see that $Z$ is a positive random variable so we can use Markov's inequality with $z = \epsilon^2$:
    \begin{align*}
      \var[Y]
      & = E[Z]\\
      & \geq \epsilon^2 \Pr[Z \geq \epsilon^2]\\
      & = \epsilon^2 \Pr[(Y - \E[Y])^2 \geq \epsilon^2].
    \end{align*}
  \end{proof}
\end{myineg}

It is easy to prove the weak law of large numbers using Chebyshev's inequality.
\begin{mytheo}[Weak Law of Large Numbers]
  Let $X_1, \ldots, X_n$ we idependent and identically distributed random variables
  with finite mean $\bar{X}$ and finite variance $\sigma_X^2$.
  Denote their sum
  \[ S_n = X_1 + \cdots + X_n. \]
  For any $\epsilon > 0$,
  \[ \lim_{n \to \infty} \Pr\Big[ \Big|\frac{S_n}{n} - \bar{X}\Big| \geq \epsilon \Big] = 0. \]

  \begin{proof}
    Let's fix $n$ and consider the random variable $Y = S_n/n$.
    It is easy to see that $\E\{Y\} = \bar{X}$ and $\var\{Y\} = \sigma_X^2/n$~\cite[(1.41)]{gallager2010discrete}.
    Applying the Chebyshev's inequality we get
    \[ \Pr[|Y - \E[Y]| \geq \epsilon] \leq \frac{\sigma_X^2}{n\epsilon^2}. \]
    Since $\sigma_X^2 = 0$, we see directly that
    \[ \lim_{n \to \infty} \Pr[|Y - \E[Y]| \geq \epsilon] = 0. \]
  \end{proof}
\end{mytheo}

\begin{mytheo}[Central limit theorem (CLT)]
  Let $X_1, X_2, \ldots$ be iid random variables with
  finite mean $\bar{X}$ and finite variance $\sigma^2$.
  Then for every real number $z$,
  \[ \lim_{n \to \infty} \Pr\left\{ \frac{S_n-n\bar{X}}{\sigma\sqrt{n}} \sim \Phi(z) \right\} \]
  where $\Phi$ is the normal distribution function
  \[ \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} \exp\Big(-\frac{y^2}{2}\Big) \dif y. \]
\end{mytheo}

\begin{center}
``The strong law requires considerable patience to understand, but it is a basic and essential result in understanding stochastic processes''\hfill\cite[p.~35]{gallager2010discrete}.
\end{center}

\begin{mytheo}[Strong Law of Large Numbers (SLLN)]
  For each integer $n \geq 1$.
  Let $X_1, X_2, \ldots$ be iid random variables with $\E[|X|] < \infty$.
  Then
  \[ \Pr\Big\{ \lim_{n \to \infty} \frac{S_n}{n} = \bar{X} \Big\} = 1. \]
\end{mytheo}

In short, the Weak Law of Large Numbers guarantees a convergence in probability and the Strong Law a convergence in distribution.

\section{Poisson Processes}
\cite[\S2]{gallager1995discrete,gallager2010discrete}

A Poisson Process is represented by an arrival rate $\lambda$.
The time between two consecutive arrivals are represented by the random variable $X_i$ (see \figuref{poisson}).
Their sum, $S_n$ is therefore the time from the start the the $n$th arrival.
$N(t)$ is the number of arrival from the start to the time $t$.
One can check is understanding by verifying that $\forall n,t,$
\( S_n \leq t \Leftrightarrow N(t) \geq n. \)

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}
    \draw[thick,->] (0,0) to (11.5,0);
    \draw (0,.1) to (0,-.1);
    \draw[<->] (0,-1) to (1,-1);
    \node[below] at (.5,-1) {$x_1$};
    \draw (1,.1) to (1,-.1);
    \node[below] at (1,-.1) {$t_1$};
    \draw[<->] (1,-1) to (3.5,-1);
    \node[below] at (2.25,-1) {$x_2$};
    \draw (3.5,.1) to (3.5,-.1);
    \node[below] at (3.5,-.1) {$t_2$};
    \draw[<->] (3.5,-1) to (7,-1);
    \node[below] at (5.25,-1) {$x_3$};
    \draw (7,.1) to (7,-.1);
    \node[below] at (7,-.1) {$t_3$};
    \draw[<->] (7,-1) to (8,-1);
    \node[below] at (7.5,-1) {$x_4$};
    \draw (8,.1) to (8,-.1);
    \node[below] at (8,-.1) {$t_4$};
    \draw[<->] (8,-1) to (11,-1);
    \node[below] at (9.5,-1) {$x_5$};
    \draw (11,.1) to (11,-.1);
    \node[below] at (11,-.1) {$t_5$};

    \draw[<->] (0,-2) to (3.5,-2);
    \node[below] at (1.75,-2) {$s_2$};
    \draw[<->] (0,-2.5) to (8,-2.5);
    \node[below] at (4,-2.5) {$s_4$};

    \draw (5,.1) to (5,-.1);
    \node[above] at (5,.1) {$N(t) = 2$};

    \draw (10,.1) to (10,-.1);
    \node[above] at (10,.1) {$N(t) = 4$};
  \end{tikzpicture}
  \caption{Example of Poisson Process}
  \label{fig:poisson}
\end{figure}

\section{Renewal Processes}
\cite[\S3]{gallager1995discrete,gallager2010discrete}

\section{Chaines de Markov}
\cite[\S4--5]{gallager1995discrete,gallager2010discrete}

\section{Processus de décision Markovien}
\cite{puterman2014markov}

\section{Processus de Markov}
\cite[\S6]{gallager1995discrete,gallager2010discrete}

\section{Files d'attente}

\nocite{*}
\biblio[alpha]

\end{document}
